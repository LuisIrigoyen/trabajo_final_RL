{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisIrigoyen/trabajo_final_RL/blob/EduardoAyora/Proyecto_pra%CC%81ctico_grupo_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 4 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: Eduardo Antonio Ayora Ochoa\n",
        "*   Alumno 2: Edward José Flores Masias\n",
        "*   Alumno 3: Luis Irigoyen Peña\n",
        "*   Alumno 4: Carlos Jesús Fernando Kong Ramos\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/MyDrive/dqn_spaceinvaders_checkpoints\"\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6n7MIefJ21i",
        "outputId": "09d49ced-2a7f-4b43-b3fb-f51b332f466e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Colab: making sure  /content/gdrive/MyDrive/dqn_spaceinvaders_checkpoints  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/MyDrive/dqn_spaceinvaders_checkpoints\n",
            "/content/gdrive/MyDrive/dqn_spaceinvaders_checkpoints\n",
            "Archivos en el directorio: \n",
            "['video', 'logs', 'checkpoint', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', '400k.zip', 'sequential_memory_2.pkl.zip', 'weightsv2.2', 'weightsv2.3', 'sequential_memory_2.3.pkl.zip', 'legacy', 'weightsv4', 'puntuacion 18.3 weigths 5', 'weights_5', 'sequential_memory_5.pkl', 'weights_6', 'wrapper', 'sequential_memory_6.pkl.zip', 'sequential_memory.pkl.zip', 'cut', 'sequential_memory_cut.pkl', 'cutv2', 'sequential_memory_cutv2.pkl']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbVRjvHCJ8UF",
        "outputId": "a5cfce69-f39d-4f56-e357-ae42b1efaa62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (2.0.2)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=a7fc139c037fde3cc6ede75d069b1374e0c7f50d5fa21f52a97c4a1c3942e3f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/b7/570cb90b10f17e85ccb291ba1f04af41ec697745104a2263eb\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.8.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-i_uuw2ov\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-i_uuw2ov\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (2.0.2)\n",
            "Building wheels for collected packages: atari-py\n",
            "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atari-py: filename=atari_py-1.2.2-cp311-cp311-linux_x86_64.whl size=4738555 sha256=7b8dd34c518bd06b27aef50d644169b92cc91f1b8534427503fd233979f1ba55\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t4xr4t3p/wheels/1a/58/b3/3baab9d1509939ecce2dfd9ca349c222b7ee6590f4bd6097a1\n",
            "Successfully built atari-py\n",
            "Installing collected packages: atari-py\n",
            "Successfully installed atari-py-1.2.2\n",
            "Collecting keras-rl2==1.0.5\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.2)\n",
            "Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.1)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "bigframes 2.8.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.8.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "orbax-checkpoint 0.11.16 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.5.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.8 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "74c24e1fbdd7452e86241b2bad4c7432"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.12  #2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "# from keras.optimizers import Adam  # from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwOE6I_KGb2a"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "metadata": {
        "id": "uRl-zPaXr4iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayFHvtRSPH7F"
      },
      "outputs": [],
      "source": [
        "processor = AtariProcessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "## 1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QmA5qbKpKpW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Permute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4GKrfWSGb2b",
        "outputId": "c34d35b7-2323-4105-df0d-2ed963e7aad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " permute (Permute)           (None, 84, 84, 4)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,687,206\n",
            "Trainable params: 1,687,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Red neuronal\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH, 84, 84)))\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "## 2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8JZdL6tXGp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código Eduardo (Después cambiamos el nombre de esta sección)"
      ],
      "metadata": {
        "id": "kHZLDmwlq3ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero hacemos una modificación del preprocesador, ya que en las observaciones (imágenes del juego) se incluye una barra superior que muestra el puntaje actual. Sin embargo, esta región de la imagen no forma parte del estado del juego que el agente necesita para tomar decisiones óptimas, ya que el puntaje refleja el rendimiento pasado, no información sobre la situación actual en el entorno (como la posición de enemigos o del jugador). Además, incluir el puntaje en las observaciones puede inducir sesgos. Originalmente las observaciones se ven de la siguiente manera:"
      ],
      "metadata": {
        "id": "BuZ3-c0LlDZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = AtariProcessor()"
      ],
      "metadata": {
        "id": "Ao4K3o1vrXB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "amf0W7J0O1Ea",
        "outputId": "98e94451-7067-400e-f6e5-09becf7ec205"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "frame = env.reset()\n",
        "processed_frame = processor.process_observation(frame)\n",
        "\n",
        "plt.imshow(processed_frame, cmap='gray')\n",
        "plt.title(\"Processed Frame\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por lo que procedemos a quitar esta región de la imagen agregando al preprocesador la línea: `cropped_observation = observation[34:194, :, :]`"
      ],
      "metadata": {
        "id": "gynlkWTxoFcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PofZgFA1OoJS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "\n",
        "        # 1. Recortar imagen (eliminar HUD, solo zona de juego)\n",
        "        # Gym Atari frames suelen ser 210x160x3, recortamos y=34:194\n",
        "        cropped_observation = observation[34:194, :, :]\n",
        "\n",
        "        # 2. Convertir a PIL, escala de grises, redimensionar\n",
        "        img = Image.fromarray(cropped_observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')  # INPUT_SHAPE usualmente (84, 84)\n",
        "\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "zOvkptkyP1t6",
        "outputId": "aa52da7d-2a4a-4010-ed02-68663d952096"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "frame = env.reset()\n",
        "processed_frame = processor.process_observation(frame)\n",
        "\n",
        "plt.imshow(processed_frame, cmap='gray')\n",
        "plt.title(\"Processed Frame\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMxBpewG7GsH"
      },
      "source": [
        "Usamos un wrapper que termina el episodio al perder una vida, así reducimos la duración media de cada episodio, lo cual permite realizar más iteraciones de entrenamiento en menos tiempo real, donde el agente puede sobrevivir por largo tiempo en las vidas siguientes, incluso sin mejorar significativamente su política."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXGZHQ4_8nVe"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "class LifeTerminatingWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.steps_since_enemy_kill = 0  # Nuevo contador\n",
        "        self.consecutive_shots = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "        # --- Terminar episodio si pierde una vida ---\n",
        "        lives = info.get('ale.lives', 0)\n",
        "        if self.lives > lives > 0:\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        self.last_direction = None\n",
        "        self.direction_streak = 0\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtOZyg8Wo2KR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "checkpoint_dir = drive_root\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También se ha implementado un callback que permite guardar checkpoints del modelo en intervalos regulares de pasos durante el entrenamiento, así como registrar las recompensas acumuladas al final de cada episodio"
      ],
      "metadata": {
        "id": "tshlDjfqdXsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYEJLlV0sQeK"
      },
      "outputs": [],
      "source": [
        "from rl.callbacks import Callback\n",
        "\n",
        "class SaveCheckpointCallback(Callback):\n",
        "    def __init__(self, interval, path_template, reward_log_path):\n",
        "        self.interval = interval\n",
        "        self.path_template = path_template\n",
        "        self.reward_log_path = reward_log_path\n",
        "        self.episode_rewards = []\n",
        "\n",
        "        # Crear carpeta para rewards si es necesario\n",
        "        if self.reward_log_path:\n",
        "            reward_dir = os.path.dirname(self.reward_log_path)\n",
        "            if reward_dir and not os.path.exists(reward_dir):\n",
        "                os.makedirs(reward_dir)\n",
        "                print(f\"📂 Carpeta creada: {reward_dir}\")\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        if step % self.interval == 0:\n",
        "            filename = self.path_template.format(step=step)\n",
        "            print(f\"\\n💾 Guardando pesos en: {filename}\")\n",
        "            self.model.save_weights(filename, overwrite=True)\n",
        "\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        reward = logs.get('episode_reward', 0)\n",
        "        self.episode_rewards.append(reward)\n",
        "        if self.reward_log_path:\n",
        "            np.save(self.reward_log_path, np.array(self.episode_rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-QmXS59pdPS"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=330000, window_length=WINDOW_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el entorno de entrenamiento y se envuelve con LifeTerminatingWrapper"
      ],
      "metadata": {
        "id": "lSMixTgMudHk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlCT0pUP6OXa"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name)\n",
        "env = LifeTerminatingWrapper(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnGIkWsfUSHx"
      },
      "source": [
        "#### Entrenamiento parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este primer entrenamiento, se utiliza una política de exploración basada en ε-greedy con un esquema de disminución lineal del parámetro ε desde 1.0 hasta 0.1, lo cual permite al agente explorar ampliamente al inicio y luego reducir progresivamente la aleatoriedad en sus acciones. Esta disminución ocurre a lo largo de 300,000 pasos (nb_steps_annealing), de un total de 400,000 pasos de entrenamiento (nb_steps). Durante los últimos 100,000 pasos, el valor de ε se mantiene constante en 0.1, permitiendo una fase de ajuste fino con menor aleatoriedad. También es importante que el warmup no sea menor de 40,000 pasos, ya que durante esta fase el agente recopila suficientes experiencias en la memoria antes de comenzar a entrenar, lo cual es crucial para evitar aprender a partir de datos poco variados o no representativos.\n",
        "\n",
        "El agente se configura con un Double DQN `(enable_double_dqn=True)` para mitigar la sobreestimación de valores Q que ocurre en el DQN estándar, mejorando así la estabilidad del entrenamiento. La actualización del modelo objetivo se realiza cada 8500 pasos para mantener un equilibrio entre estabilidad y adaptabilidad: actualizaciones muy frecuentes pueden introducir ruido, mientras que actualizaciones muy espaciadas ralentizan el aprendizaje. Se utiliza clipping del error temporal con `delta_clip=1.0` para limitar la magnitud de las actualizaciones y evitar que errores grandes desestabilicen el entrenamiento. Finalmente, se emplea el optimizador Adam con una tasa de aprendizaje de 0.0001, ya que proporciona una convergencia eficiente, y un valor bajo de lr evita oscilaciones excesivas en las primeras fases del entrenamiento."
      ],
      "metadata": {
        "id": "kFzL6L7jv-DR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRFy2z18UbZ2"
      },
      "outputs": [],
      "source": [
        "# Exploracion 1 a 0.1, # steps -> 400k\n",
        "nb_steps = 400000\n",
        "nb_steps_annealing = 300000\n",
        "nb_steps_warmup=40000\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
        "                              attr='eps',\n",
        "                              value_max=1, #0.22 # 0.24 # 0.35\n",
        "                              value_min=0.1, # 0.08\n",
        "                              value_test=0.0,\n",
        "                              nb_steps=nb_steps_annealing)\n",
        "\n",
        "\n",
        "dqn = DQNAgent(model=model,\n",
        "               nb_actions=nb_actions,\n",
        "               policy=policy,\n",
        "               memory=memory,\n",
        "               processor=processor,\n",
        "               nb_steps_warmup=nb_steps_warmup, #30000\n",
        "               enable_double_dqn=True,\n",
        "               gamma=0.99, # 0.99\n",
        "               target_model_update=8500,\n",
        "               train_interval=4,\n",
        "               delta_clip=1.0) # si el loss no baja, probar bajarlo a 0.5\n",
        "\n",
        "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae']) # antes 0.00025"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define una ruta para guardar los checkpoints del modelo y otra para registrar las recompensas por episodio y procedemos a entrenar el agente"
      ],
      "metadata": {
        "id": "oPGmkkany-3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(checkpoint_dir, 'cut/dqn_SpaceInvaders-v0_weights.h5f')\n",
        "reward_log_path = os.path.join(checkpoint_dir, 'logs/episode_rewards_cut.npy')\n",
        "\n",
        "checkpoint_callback = SaveCheckpointCallback(interval=10000, path_template=checkpoint_path, reward_log_path=reward_log_path)\n",
        "\n",
        "dqn.fit(env,\n",
        "        nb_steps=nb_steps,\n",
        "        visualize=False,\n",
        "        verbose=2,\n",
        "        callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "i1V6iy0Xuw4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos visualizar las recompensas acumuladas por episodio en el tiempo. Y vemos que desde el episodio 800 el agente mejora su rendimiento, obteniendo puntuaciones mayores (Se debe tener en cuenta que debido al wrapper del entorno, el agente está jugando con 1 sola vida, por lo que si quitamos el wrapper y el agente juega con las 3 vidas su rendimiento será mejor)"
      ],
      "metadata": {
        "id": "FsmhCfOIzgnt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "2-B0Ju5PlaXA",
        "outputId": "31196e4c-d911-4d26-beb2-1f3b90305304"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "episode_rewards = np.load('logs/episode_rewards_cut.npy')\n",
        "\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylabel('Reward total')\n",
        "plt.title('Evolución del entrenamiento')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se guarda la `memory` del agente porque contiene trazas diversas acumuladas durante el entrenamiento, valiosas al haberse recolectado con un valor de ε que disminuye desde 1.0 a 0.1. Esta diversidad de transiciones permite retomar el entrenamiento en el futuro con una base rica y variada, lo cual contribuye a una mayor estabilidad en el aprendizaje."
      ],
      "metadata": {
        "id": "tVTpBXpF0RwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVznFIXzlcD5"
      },
      "outputs": [],
      "source": [
        "# Guardar\n",
        "import pickle\n",
        "\n",
        "# Suponiendo que `memory` es tu SequentialMemory\n",
        "with open('sequential_memory_cut.pkl', 'wb') as f:\n",
        "    pickle.dump(memory, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obtener el rendimiento promedio del agente, volvemos a definir el entorno, pero esta vez sin el wrapper que limita a una sola vida. De modo que probamos el agente en el entorno de juego original"
      ],
      "metadata": {
        "id": "nn-2thtw14Pv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2gp5LHiAYef"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probamos por 50 episodios para obtener una buena medida del rendimiento promedio"
      ],
      "metadata": {
        "id": "eCSdwF9b2s8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDHkowdllubK",
        "outputId": "59345d92-c7a4-462b-c7a7-9942b05c1180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 13.000, steps: 677\n",
            "Episode 2: reward: 25.000, steps: 925\n",
            "Episode 3: reward: 12.000, steps: 495\n",
            "Episode 4: reward: 24.000, steps: 1296\n",
            "Episode 5: reward: 17.000, steps: 1028\n",
            "Episode 6: reward: 10.000, steps: 531\n",
            "Episode 7: reward: 15.000, steps: 626\n",
            "Episode 8: reward: 15.000, steps: 914\n",
            "Episode 9: reward: 10.000, steps: 486\n",
            "Episode 10: reward: 14.000, steps: 953\n",
            "Episode 11: reward: 11.000, steps: 602\n",
            "Episode 12: reward: 26.000, steps: 1287\n",
            "Episode 13: reward: 14.000, steps: 956\n",
            "Episode 14: reward: 20.000, steps: 944\n",
            "Episode 15: reward: 12.000, steps: 663\n",
            "Episode 16: reward: 17.000, steps: 1266\n",
            "Episode 17: reward: 26.000, steps: 1752\n",
            "Episode 18: reward: 15.000, steps: 1296\n",
            "Episode 19: reward: 7.000, steps: 413\n",
            "Episode 20: reward: 16.000, steps: 838\n",
            "Episode 21: reward: 13.000, steps: 731\n",
            "Episode 22: reward: 10.000, steps: 596\n",
            "Episode 23: reward: 14.000, steps: 951\n",
            "Episode 24: reward: 11.000, steps: 601\n",
            "Episode 25: reward: 13.000, steps: 671\n",
            "Episode 26: reward: 28.000, steps: 1046\n",
            "Episode 27: reward: 10.000, steps: 563\n",
            "Episode 28: reward: 24.000, steps: 945\n",
            "Episode 29: reward: 20.000, steps: 945\n",
            "Episode 30: reward: 12.000, steps: 1012\n",
            "Episode 31: reward: 17.000, steps: 1088\n",
            "Episode 32: reward: 11.000, steps: 686\n",
            "Episode 33: reward: 22.000, steps: 858\n",
            "Episode 34: reward: 16.000, steps: 1318\n",
            "Episode 35: reward: 23.000, steps: 1166\n",
            "Episode 36: reward: 13.000, steps: 842\n",
            "Episode 37: reward: 10.000, steps: 628\n",
            "Episode 38: reward: 10.000, steps: 543\n",
            "Episode 39: reward: 22.000, steps: 1374\n",
            "Episode 40: reward: 22.000, steps: 879\n",
            "Episode 41: reward: 23.000, steps: 1325\n",
            "Episode 42: reward: 16.000, steps: 894\n",
            "Episode 43: reward: 11.000, steps: 515\n",
            "Episode 44: reward: 10.000, steps: 955\n",
            "Episode 45: reward: 15.000, steps: 710\n",
            "Episode 46: reward: 27.000, steps: 1241\n",
            "Episode 47: reward: 10.000, steps: 614\n",
            "Episode 48: reward: 24.000, steps: 996\n",
            "Episode 49: reward: 23.000, steps: 1068\n",
            "Episode 50: reward: 11.000, steps: 677\n"
          ]
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = checkpoint_dir + '/cut/dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "env = Monitor(env, './video', force=True)\n",
        "history = dqn.test(env, nb_episodes=50, visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El rendimiento promedio es de 16.2, por lo que volveremos a entrenar el agente pero partiendo de estos pesos que ya han logrado una mejora en el rendimiento"
      ],
      "metadata": {
        "id": "CR6GU3kS2_Y1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKO5uy6Ll2MZ",
        "outputId": "9d658fef-c39f-4d2f-9aef-22cacece8eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa promedio: 16.2\n"
          ]
        }
      ],
      "source": [
        "# Access episode rewards\n",
        "episode_rewards = history.history['episode_reward']\n",
        "\n",
        "# Calcular el promedio\n",
        "promedio = np.mean(episode_rewards)\n",
        "\n",
        "print(f\"Recompensa promedio: {promedio}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenamiento parte 2"
      ],
      "metadata": {
        "id": "BNZom_vaHyEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volvemos a ejecutar el wrapper que hace que el agente juegue una sola vida"
      ],
      "metadata": {
        "id": "gxFEhwyLbdRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)\n",
        "env = LifeTerminatingWrapper(env)"
      ],
      "metadata": {
        "id": "saHuuPGS6lrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora realizamos un entrenamiento con una menor cantidad de pasos que antes, esto debido a que ya no haremos el entrenamiento del agente desde cero sino con del agente que ya aprendió una estrategia. Además, se escogió iniciar el entrenamiento con un valor de ε relativamente alto (0.4) para fomentar la exploración y evitar que el agente se estanque en una política subóptima, ya que estamos continuando el entrenamiento previo"
      ],
      "metadata": {
        "id": "kgGvEXJCbn7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploracion 1 a 0.1, # steps -> 400k\n",
        "nb_steps = 200000\n",
        "nb_steps_annealing = 180000\n",
        "nb_steps_warmup=40000\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
        "                              attr='eps',\n",
        "                              value_max=0.4, #0.22 # 0.24 # 0.35\n",
        "                              value_min=0.1, # 0.08\n",
        "                              value_test=0.0,\n",
        "                              nb_steps=nb_steps_annealing)\n",
        "\n",
        "\n",
        "dqn = DQNAgent(model=model,\n",
        "               nb_actions=nb_actions,\n",
        "               policy=policy,\n",
        "               memory=memory,\n",
        "               processor=processor,\n",
        "               nb_steps_warmup=nb_steps_warmup, #30000\n",
        "               enable_double_dqn=True,\n",
        "               gamma=0.99, # 0.99\n",
        "               target_model_update=8500,\n",
        "               train_interval=4,\n",
        "               delta_clip=1.0) # si el loss no baja, probar bajarlo a 0.5\n",
        "\n",
        "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae']) # antes 0.00025"
      ],
      "metadata": {
        "id": "w8kxvvvmHXrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los pesos anteriores"
      ],
      "metadata": {
        "id": "jfnHvFese3WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_filename = checkpoint_dir + '/cut/dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)"
      ],
      "metadata": {
        "id": "1yxP_Xp5KWsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y entrenamos"
      ],
      "metadata": {
        "id": "WsxrzjM-e7z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(checkpoint_dir, 'cutv3/dqn_SpaceInvaders-v0_weights.h5f')\n",
        "reward_log_path = os.path.join(checkpoint_dir, 'logs/episode_rewards_cutv3.npy')\n",
        "\n",
        "checkpoint_callback = SaveCheckpointCallback(interval=10000, path_template=checkpoint_path, reward_log_path=reward_log_path)\n",
        "\n",
        "dqn.fit(env,\n",
        "        nb_steps=nb_steps,\n",
        "        visualize=False,\n",
        "        verbose=2,\n",
        "        callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "ined2vgKe8nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que al final se produce una mejora"
      ],
      "metadata": {
        "id": "suYcKdqRfEoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "episode_rewards = np.load('logs/episode_rewards_cutv3.npy')\n",
        "\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylabel('Reward total')\n",
        "plt.title('Evolución del entrenamiento')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c4HweY_0J43Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "0cbd6e89-2376-4aa5-83ec-42c76361deaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar\n",
        "import pickle\n",
        "\n",
        "# Suponiendo que `memory` es tu SequentialMemory\n",
        "with open('sequential_memory_cutv3.pkl', 'wb') as f:\n",
        "    pickle.dump(memory, f)"
      ],
      "metadata": {
        "id": "kyRS7sbTJ5ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volvemos a establecer el entorno original de Atari para realizar el test"
      ],
      "metadata": {
        "id": "QGMQ7_0WfPDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "1A_ClTrEKEfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = checkpoint_dir + '/cutv3/dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "env = Monitor(env, './video', force=True)\n",
        "history = dqn.test(env, nb_episodes=50, visualize=False)"
      ],
      "metadata": {
        "id": "OcA3Ee1uKKHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50155d46-6f55-4aef-8547-1ad57726887d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 50 episodes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: reward: 22.000, steps: 1257\n",
            "Episode 2: reward: 24.000, steps: 962\n",
            "Episode 3: reward: 28.000, steps: 1093\n",
            "Episode 4: reward: 9.000, steps: 526\n",
            "Episode 5: reward: 17.000, steps: 851\n",
            "Episode 6: reward: 17.000, steps: 742\n",
            "Episode 7: reward: 24.000, steps: 1057\n",
            "Episode 8: reward: 18.000, steps: 1136\n",
            "Episode 9: reward: 14.000, steps: 643\n",
            "Episode 10: reward: 21.000, steps: 887\n",
            "Episode 11: reward: 25.000, steps: 1033\n",
            "Episode 12: reward: 28.000, steps: 1248\n",
            "Episode 13: reward: 19.000, steps: 851\n",
            "Episode 14: reward: 27.000, steps: 1230\n",
            "Episode 15: reward: 13.000, steps: 729\n",
            "Episode 16: reward: 23.000, steps: 1114\n",
            "Episode 17: reward: 12.000, steps: 695\n",
            "Episode 18: reward: 10.000, steps: 558\n",
            "Episode 19: reward: 21.000, steps: 951\n",
            "Episode 20: reward: 31.000, steps: 1811\n",
            "Episode 21: reward: 14.000, steps: 663\n",
            "Episode 22: reward: 13.000, steps: 1135\n",
            "Episode 23: reward: 15.000, steps: 655\n",
            "Episode 24: reward: 26.000, steps: 1256\n",
            "Episode 25: reward: 24.000, steps: 858\n",
            "Episode 26: reward: 16.000, steps: 943\n",
            "Episode 27: reward: 22.000, steps: 890\n",
            "Episode 28: reward: 19.000, steps: 815\n",
            "Episode 29: reward: 10.000, steps: 760\n",
            "Episode 30: reward: 16.000, steps: 968\n",
            "Episode 31: reward: 15.000, steps: 848\n",
            "Episode 32: reward: 13.000, steps: 734\n",
            "Episode 33: reward: 20.000, steps: 869\n",
            "Episode 34: reward: 12.000, steps: 544\n",
            "Episode 35: reward: 22.000, steps: 919\n",
            "Episode 36: reward: 12.000, steps: 699\n",
            "Episode 37: reward: 31.000, steps: 1467\n",
            "Episode 38: reward: 29.000, steps: 1442\n",
            "Episode 39: reward: 10.000, steps: 521\n",
            "Episode 40: reward: 12.000, steps: 642\n",
            "Episode 41: reward: 22.000, steps: 855\n",
            "Episode 42: reward: 24.000, steps: 1012\n",
            "Episode 43: reward: 19.000, steps: 1271\n",
            "Episode 44: reward: 24.000, steps: 911\n",
            "Episode 45: reward: 18.000, steps: 899\n",
            "Episode 46: reward: 28.000, steps: 1503\n",
            "Episode 47: reward: 20.000, steps: 918\n",
            "Episode 48: reward: 21.000, steps: 870\n",
            "Episode 49: reward: 22.000, steps: 1162\n",
            "Episode 50: reward: 15.000, steps: 793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que se obtiene un 19.34 de promedio tras 50 juegos, muy cerca del objetivo de promedio 20"
      ],
      "metadata": {
        "id": "mGMCg7iMfXL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access episode rewards\n",
        "episode_rewards = history.history['episode_reward']\n",
        "\n",
        "# Calcular el promedio\n",
        "promedio = np.mean(episode_rewards)\n",
        "\n",
        "print(f\"Recompensa promedio: {promedio}\")"
      ],
      "metadata": {
        "id": "WjJks1wzLFTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c3b19b-d178-4530-ba07-4b6eeae58e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa promedio: 19.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenamiento parte 3"
      ],
      "metadata": {
        "id": "E41TMUkd2qLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = SequentialMemory(limit=330000, window_length=WINDOW_LENGTH)"
      ],
      "metadata": {
        "id": "qHZDIkm-ven-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el objetivo de realizar un ajuste fino final, se cargan las trazas generadas durante el entrenamiento previo, lo que permite contar con una memoria de experiencias más diversa. Esta diversidad contribuye a un entrenamiento más estable, el cual es clave en esta fase donde se busca refinar la estrategia sin introducir inestabilidad en el aprendizaje.\n"
      ],
      "metadata": {
        "id": "x2z6i9nNudZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar\n",
        "import pickle\n",
        "\n",
        "with open('sequential_memory_cut.pkl', 'rb') as f:\n",
        "    memory = pickle.load(f)"
      ],
      "metadata": {
        "id": "qw53To9h3FkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos el wrapper para que el episodio termine tras una sola vida"
      ],
      "metadata": {
        "id": "oTMhWYzeuVdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)\n",
        "env = LifeTerminatingWrapper(env)"
      ],
      "metadata": {
        "id": "sUq0AYiC6Uhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y realizamos el ajuste fino con un epsilon que va desde 0.1 hasta 0.08 en 50,000 pasos"
      ],
      "metadata": {
        "id": "Sd8T85sey1qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_steps = 50000\n",
        "nb_steps_annealing = 50000\n",
        "nb_steps_warmup=0\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
        "                              attr='eps',\n",
        "                              value_max=0.1, #0.22 # 0.24 # 0.35\n",
        "                              value_min=0.08, # 0.08\n",
        "                              value_test=0.0,\n",
        "                              nb_steps=nb_steps_annealing)\n",
        "\n",
        "\n",
        "dqn = DQNAgent(model=model,\n",
        "               nb_actions=nb_actions,\n",
        "               policy=policy,\n",
        "               memory=memory,\n",
        "               processor=processor,\n",
        "               nb_steps_warmup=nb_steps_warmup, #30000\n",
        "               enable_double_dqn=True,\n",
        "               gamma=0.99, # 0.99\n",
        "               target_model_update=8500,\n",
        "               train_interval=4,\n",
        "               delta_clip=1.0) # si el loss no baja, probar bajarlo a 0.5\n",
        "\n",
        "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae']) # antes 0.00025"
      ],
      "metadata": {
        "id": "F0iL00662lIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los pesos del modelo que ya teniamos y volvemos a entrenar"
      ],
      "metadata": {
        "id": "WHJXxUj31mlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_filename = checkpoint_dir + '/cutv3/dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)"
      ],
      "metadata": {
        "id": "UAjEKxUp4_HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(checkpoint_dir, 'cutv4/dqn_SpaceInvaders-v0_weights.h5f')\n",
        "reward_log_path = os.path.join(checkpoint_dir, 'logs/episode_rewards_cutv4.npy')\n",
        "\n",
        "checkpoint_callback = SaveCheckpointCallback(interval=10000, path_template=checkpoint_path, reward_log_path=reward_log_path)\n",
        "\n",
        "dqn.fit(env,\n",
        "        nb_steps=nb_steps,\n",
        "        visualize=False,\n",
        "        verbose=2,\n",
        "        callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "xEoY3z8rtDQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso la mejora del agente es muy leve, y no se ve una diferencia notable en la gráfica. Sin embargo, al evaluar el desempeño en el test y obtener el promedio, se observa que se ha superado el promedio de 20"
      ],
      "metadata": {
        "id": "_x2AJyGi4LaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "episode_rewards = np.load('logs/episode_rewards_cutv4.npy')\n",
        "\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylabel('Reward total')\n",
        "plt.title('Evolución del entrenamiento')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "A4VAada952zV",
        "outputId": "362308ed-2953-43b1-a15d-3d774ab90ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar\n",
        "import pickle\n",
        "\n",
        "# Suponiendo que `memory` es tu SequentialMemory\n",
        "with open('sequential_memory_cutv4.pkl', 'wb') as f:\n",
        "    pickle.dump(memory, f)"
      ],
      "metadata": {
        "id": "RznMlTMO56-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volvemos a definir el entorno original"
      ],
      "metadata": {
        "id": "kmf_-ME53Fhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "kESpyuoj6DMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "Z30LtwUR-ooH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing part to calculate the mean reward 02\n",
        "weights_filename = checkpoint_dir + '/cutv4/dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "env = Monitor(env, './video', force=True)\n",
        "history = dqn.test(env, nb_episodes=50, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5P6kIxSAhvO",
        "outputId": "1a2f1fe5-472a-4369-c36b-bc7176993142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 35.000, steps: 1809\n",
            "Episode 2: reward: 17.000, steps: 837\n",
            "Episode 3: reward: 21.000, steps: 800\n",
            "Episode 4: reward: 27.000, steps: 1133\n",
            "Episode 5: reward: 22.000, steps: 942\n",
            "Episode 6: reward: 19.000, steps: 1104\n",
            "Episode 7: reward: 22.000, steps: 1542\n",
            "Episode 8: reward: 29.000, steps: 1409\n",
            "Episode 9: reward: 20.000, steps: 992\n",
            "Episode 10: reward: 16.000, steps: 711\n",
            "Episode 11: reward: 34.000, steps: 1463\n",
            "Episode 12: reward: 16.000, steps: 870\n",
            "Episode 13: reward: 19.000, steps: 692\n",
            "Episode 14: reward: 23.000, steps: 1057\n",
            "Episode 15: reward: 21.000, steps: 1054\n",
            "Episode 16: reward: 20.000, steps: 663\n",
            "Episode 17: reward: 12.000, steps: 538\n",
            "Episode 18: reward: 22.000, steps: 917\n",
            "Episode 19: reward: 22.000, steps: 1084\n",
            "Episode 20: reward: 22.000, steps: 1010\n",
            "Episode 21: reward: 10.000, steps: 675\n",
            "Episode 22: reward: 11.000, steps: 643\n",
            "Episode 23: reward: 15.000, steps: 690\n",
            "Episode 24: reward: 20.000, steps: 1347\n",
            "Episode 25: reward: 27.000, steps: 995\n",
            "Episode 26: reward: 10.000, steps: 489\n",
            "Episode 27: reward: 22.000, steps: 900\n",
            "Episode 28: reward: 20.000, steps: 880\n",
            "Episode 29: reward: 27.000, steps: 1108\n",
            "Episode 30: reward: 25.000, steps: 976\n",
            "Episode 31: reward: 20.000, steps: 921\n",
            "Episode 32: reward: 16.000, steps: 704\n",
            "Episode 33: reward: 20.000, steps: 750\n",
            "Episode 34: reward: 13.000, steps: 737\n",
            "Episode 35: reward: 17.000, steps: 1278\n",
            "Episode 36: reward: 34.000, steps: 1926\n",
            "Episode 37: reward: 10.000, steps: 484\n",
            "Episode 38: reward: 19.000, steps: 883\n",
            "Episode 39: reward: 22.000, steps: 926\n",
            "Episode 40: reward: 15.000, steps: 659\n",
            "Episode 41: reward: 27.000, steps: 1110\n",
            "Episode 42: reward: 14.000, steps: 1488\n",
            "Episode 43: reward: 25.000, steps: 1309\n",
            "Episode 44: reward: 21.000, steps: 845\n",
            "Episode 45: reward: 26.000, steps: 1106\n",
            "Episode 46: reward: 26.000, steps: 1234\n",
            "Episode 47: reward: 10.000, steps: 753\n",
            "Episode 48: reward: 24.000, steps: 1158\n",
            "Episode 49: reward: 11.000, steps: 637\n",
            "Episode 50: reward: 29.000, steps: 1568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y finalmente hemos alcanzado el promedio superior a 20"
      ],
      "metadata": {
        "id": "yec4IyFi3VTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access episode rewards\n",
        "episode_rewards = history.history['episode_reward']\n",
        "\n",
        "# Calcular el promedio\n",
        "promedio = np.mean(episode_rewards)\n",
        "\n",
        "print(f\"Recompensa promedio: {promedio}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpQ0GzCjAriG",
        "outputId": "f0301258-2c21-4973-ddfa-a73e694340a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa promedio: 20.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código Edward (Después cambiamos el nombre de esta sección)"
      ],
      "metadata": {
        "id": "R2cKxphGu4_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "height, width, channels = env.observation_space.shape\n",
        "obs = env.reset()\n",
        "print(height, width, channels)\n",
        "print(\"Forma de la observación:\", obs.shape)  # Debe ser (height, width, channels)\n",
        "env.unwrapped.get_action_meanings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLlIeiMmkYVd",
        "outputId": "26b900f2-c76a-45e0-db71-32be68f16e7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210 160 3\n",
            "Forma de la observación: (210, 160, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n"
      ],
      "metadata": {
        "id": "dyNWdTHIkl4B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.callbacks import ModelIntervalCheckpoint,Callback\n",
        "\n",
        "class EpisodeRewardRecorder(Callback):\n",
        "    def __init__(self):\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        reward = logs.get('episode_reward')\n",
        "        self.episode_rewards.append(reward)\n",
        "\n",
        "# Guardar el modelo cada 50,000 pasos\n",
        "checkpoint_callback = ModelIntervalCheckpoint(\n",
        "    filepath='checkpoints/dqn_v18_weights_{step}.h5f',\n",
        "    interval=50000,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "import os\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "dqn = build_agent(model, nb_actions)\n",
        "dqn.compile(Adam(learning_rate=0.00025, epsilon=0.001))"
      ],
      "metadata": {
        "id": "VkWvLkETklpW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ejfm 1. Implementación de la red neuronal"
      ],
      "metadata": {
        "id": "KofmPox1p0RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(height, width, channels, actions):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Permute((2, 3, 1), input_shape=(4, 84, 84)))  # WINDOW_LENGTH, H, W\n",
        "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu')) #, input_shape=(4, 84, 84), data_format='channels_first')) #input_shape=(4,84,84,1)))\n",
        "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(nb_actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "wPtwikIEoRM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(84, 84, 1, nb_actions)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ZxIlK6FfoRJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ejfm 2. Implementación de la solución DQN"
      ],
      "metadata": {
        "id": "4J42yhfFqDQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ],
      "metadata": {
        "id": "u6l5XWEWoRF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(),\n",
        "        attr='eps',\n",
        "        value_max=1.0,\n",
        "        value_min=0.1,\n",
        "        value_test=0.05,\n",
        "        nb_steps=400000)                    # Pasos de exploración\n",
        "\n",
        "    memory = SequentialMemory(\n",
        "        limit=400000,                       # Replay Buffer (Memoria de experiencias pasadas en total (s,a,r,s'))\n",
        "        window_length=4)\n",
        "\n",
        "    dqn = DQNAgent(model=model,\n",
        "                   nb_actions=nb_actions,\n",
        "                   policy=policy,\n",
        "                   memory=memory,\n",
        "                   processor=AtariProcessor(),\n",
        "                   nb_steps_warmup=100000,    # Llena en buffer de pasos previos (s,a,r,s') sin evaluar aún.\n",
        "                   gamma=0.99,\n",
        "                   target_model_update=10000, # cada 10,000 steps actualiza la red objetivo\n",
        "                   train_interval=4,          # Solo entrena cada n pasos de interacción con el entorno\n",
        "                   delta_clip=1.0,            # Valor utilizado para recortar los errores TD (diferencia temporal) durante el entrenamiento para evitar fluctuaciones externas\n",
        "                   enable_double_dqn=True,\n",
        "                   enable_dueling_network=True,\n",
        "                   dueling_type='avg',\n",
        "                   batch_size=32,\n",
        "                   )\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "dn3AeQRVoRAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.callbacks import ModelIntervalCheckpoint,Callback\n",
        "\n",
        "class EpisodeRewardRecorder(Callback):\n",
        "    def __init__(self):\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        reward = logs.get('episode_reward')\n",
        "        self.episode_rewards.append(reward)\n",
        "\n",
        "# Guardar el modelo cada 50,000 pasos\n",
        "checkpoint_callback = ModelIntervalCheckpoint(\n",
        "    filepath='checkpoints/dqn_v16_weights_{step}.h5f',\n",
        "    interval=50000,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "DjJYgCX6oQ84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)"
      ],
      "metadata": {
        "id": "XJLPUmhCoQoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "dqn = build_agent(model, nb_actions)\n",
        "dqn.compile(Adam(learning_rate=0.00025, epsilon=0.001))"
      ],
      "metadata": {
        "id": "1E5bJ1UOqe_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ejfm Ejecución"
      ],
      "metadata": {
        "id": "lZ2pObtVteBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"⏱ Tiempo de inicio: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "reward_logger = EpisodeRewardRecorder()\n",
        "historial = dqn.fit(env, nb_steps=1250000, visualize=False, verbose=2,callbacks=[checkpoint_callback,reward_logger])\n",
        "\n",
        "print(f\"⏱ Tiempo de fin: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "# Guardando modelo\n",
        "rewards = reward_logger.episode_rewards\n",
        "dqn.save_weights('dqn_v16_weights.h5f')\n"
      ],
      "metadata": {
        "id": "BE-RadF9qfFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ejfm terminada la ejecución se procede con el test de validación para 5 eventos."
      ],
      "metadata": {
        "id": "bFTGnCuwq5og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weights_filename = 'dqn_v16_weights.h5f'.format(env_name)\n",
        "\n",
        "#Número pruebas de validación\n",
        "test = 5\n",
        "\n",
        "print(f\"Versión del fichero final: {weights_filename}\")\n",
        "\n",
        "medias = []\n",
        "episode_rewards = []\n",
        "\n",
        "for i in range(1, test + 1):\n",
        "\n",
        "    dqn.load_weights(weights_filename)\n",
        "    # Ejecuta 10 episodios sin explorar (ε=0)\n",
        "    hist = dqn.test(env, nb_episodes=10, visualize=False)\n",
        "\n",
        "    # Calcula la media\n",
        "    media = np.mean(hist.history[\"episode_reward\"])\n",
        "    medias.append(media)\n",
        "\n",
        "    # Emite resultado\n",
        "    print(f\"Ejecución {i}: media = {media:.2f} \\n\")\n",
        "\n",
        "    # Guardando resultados\n",
        "    episode_reward = hist.history[\"episode_reward\"]\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "n = sum(m >= 20 for m in medias)\n",
        "print(f\"{n} de {test} pruebas realizadas de 10 episodios cada una.\")\n",
        "\n",
        "# Graficar\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, episode_reward in enumerate(episode_rewards):\n",
        "    plt.plot(range(1, len(episode_reward) + 1), episode_reward, marker='o', label=f'Ejecución {i + 1}')\n",
        "plt.xlabel(\"Episodio\")\n",
        "plt.ylabel(\"Recompensa obtenida\")\n",
        "plt.title(\"Recompensa por episodio en prueba\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uf0D4ZwQqfIg",
        "outputId": "c3af5342-de86-4513-e198-fc234e3107b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión del fichero final: dqn_v16_weights.h5f\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 34.000, steps: 1385\n",
            "Episode 2: reward: 18.000, steps: 695\n",
            "Episode 3: reward: 19.000, steps: 638\n",
            "Episode 4: reward: 18.000, steps: 665\n",
            "Episode 5: reward: 28.000, steps: 1121\n",
            "Episode 6: reward: 29.000, steps: 1239\n",
            "Episode 7: reward: 17.000, steps: 567\n",
            "Episode 8: reward: 24.000, steps: 754\n",
            "Episode 9: reward: 32.000, steps: 1337\n",
            "Episode 10: reward: 22.000, steps: 802\n",
            "Ejecución 1: media = 24.10 \n",
            "\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 31.000, steps: 1244\n",
            "Episode 2: reward: 25.000, steps: 817\n",
            "Episode 3: reward: 14.000, steps: 516\n",
            "Episode 4: reward: 26.000, steps: 979\n",
            "Episode 5: reward: 29.000, steps: 1030\n",
            "Episode 6: reward: 26.000, steps: 848\n",
            "Episode 7: reward: 29.000, steps: 1311\n",
            "Episode 8: reward: 26.000, steps: 795\n",
            "Episode 9: reward: 30.000, steps: 1210\n",
            "Episode 10: reward: 25.000, steps: 948\n",
            "Ejecución 2: media = 26.10 \n",
            "\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 23.000, steps: 893\n",
            "Episode 2: reward: 31.000, steps: 1078\n",
            "Episode 3: reward: 29.000, steps: 1102\n",
            "Episode 4: reward: 29.000, steps: 984\n",
            "Episode 5: reward: 25.000, steps: 876\n",
            "Episode 6: reward: 29.000, steps: 945\n",
            "Episode 7: reward: 40.000, steps: 1374\n",
            "Episode 8: reward: 35.000, steps: 1603\n",
            "Episode 9: reward: 27.000, steps: 961\n",
            "Episode 10: reward: 38.000, steps: 1753\n",
            "Ejecución 3: media = 30.60 \n",
            "\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 28.000, steps: 1140\n",
            "Episode 2: reward: 26.000, steps: 948\n",
            "Episode 3: reward: 19.000, steps: 700\n",
            "Episode 4: reward: 30.000, steps: 980\n",
            "Episode 5: reward: 26.000, steps: 820\n",
            "Episode 6: reward: 14.000, steps: 492\n",
            "Episode 7: reward: 40.000, steps: 1341\n",
            "Episode 8: reward: 33.000, steps: 1050\n",
            "Episode 9: reward: 29.000, steps: 955\n",
            "Episode 10: reward: 32.000, steps: 1081\n",
            "Ejecución 4: media = 27.70 \n",
            "\n",
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 27.000, steps: 1046\n",
            "Episode 2: reward: 34.000, steps: 1547\n",
            "Episode 3: reward: 25.000, steps: 896\n",
            "Episode 4: reward: 25.000, steps: 872\n",
            "Episode 5: reward: 23.000, steps: 743\n",
            "Episode 6: reward: 34.000, steps: 1752\n",
            "Episode 7: reward: 24.000, steps: 824\n",
            "Episode 8: reward: 27.000, steps: 764\n",
            "Episode 9: reward: 32.000, steps: 1388\n",
            "Episode 10: reward: 29.000, steps: 1162\n",
            "Ejecución 5: media = 28.00 \n",
            "\n",
            "5 de 5 pruebas realizadas de 10 episodios cada una.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretación de la gráfica**"
      ],
      "metadata": {
        "id": "TpgEQxEBpTJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La gráfica representa los resultados de 5 pruebas independientes (tests) ejecutadas sobre un agente en el entorno SpaceInvaders-v0, durante 10 episodios cada una. En el eje X se representa el número de episodios (del 1 al 10), y el eje Y se representa la recompensa obtenida. La gráfica obtenida indica además fluctuación episodio a episodio en cada test, por ejemplo, en la línea roja (4.° test), hay una recompensa muy baja en el episodio 6 de 14 puntos, y una recompensa altísima en el episodio 7 de 40 puntos, esto refleja el carácter estocástico del entorno y posiblemente también inestabilidad en la política aprendida a pesar que se cumplió el objetivo de tener una media sobre los 20 puntos. La mayoría de las recompensas por episodio oscilan entre 15 y 35 puntos, con algunos picos de hasta 40 puntos."
      ],
      "metadata": {
        "id": "SQiuas4rpbDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del mismo modo, para la implementación del DQN se tuvo como referencia inicial los parámetros encontrados en el artículo titulado Human-level control through deep reinforcement learning, del autor Mnih, V. et al (2015), quién identificó unos parámetros base para obtener recompensas aceptables, sin embargo, al realizar la ejecución sobre 1 millón de pasos se identificó que no había resultados satisfactorios aún, por lo que se decidió comenzar a realizar algunas pruebas moviendo los parámetros iniciales y realizando las ejecuciones.  "
      ],
      "metadata": {
        "id": "DpMUsyyUwHnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, se identificó que determinados parámetros permitián modificar significativamente los resultados, con los cual, se realizaron más pruebas y se obtuvo una combinación que permitió cumplir el objetivo con una poca cantidad de pasos, (1250000). Se indica poca cantidad debido a que la mayor cantidad de la literatura encontrada recomienda 2000000 de pasos como mínimo para su ejecución."
      ],
      "metadata": {
        "id": "7vqCRfvzxZ_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los parámetros modificados que permiten mejores resultados son los siguientes:\n",
        "\n",
        "*   Menor pasos de exploración: de 1MM a 400,000\n",
        "*   Menor cantidad de pasos previos: 100000\n",
        "*   Habilitación de enable_double_dqn=True\n",
        "*   Uso de Adam en vez de RMSProp\n",
        "*   Epsilon más pequeño (epsilon=0.001)"
      ],
      "metadata": {
        "id": "2Ykk0Li3yETF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, también se pudo identificar que una red densa con varias capas ocultas no necesariamente da mejores resultados, pero si aumenta significativamente el tiempo de ejecución"
      ],
      "metadata": {
        "id": "MbCRatlP164z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referencias:**\n",
        "\n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533. https://doi.org/10.1038/nature14236\n",
        "\n",
        "Yao, G., Zhang, N., Duan, Z., & Tian, C. (2024). Improved SARSA and DQN algorithms for reinforcement learning. Theoretical Computer Science, 115025. https://doi.org/10.1016/j.tcs.2024.115025\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CPVYVnfr2Q1S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1Nz2T0jqfRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9inqxUP2Hqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROLj2gHP2Ho6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drUufbk72Hjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código Luis (Después cambiamos el nombre de esta sección)"
      ],
      "metadata": {
        "id": "P_LoDa_fvHlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Arquitectura Dueling Network ~Dueling Deep Q-Network:"
      ],
      "metadata": {
        "id": "lIClkG0tvIPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definimos una red neuronal también para la arquitectura Dueling DQN:"
      ],
      "metadata": {
        "id": "J616bDZ8Wt-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhwO3z2oPVq3"
      },
      "outputs": [],
      "source": [
        "def build_model(height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Capa 1: Conv2D con BatchNorm y ReLU\n",
        "    model.add(Convolution2D(32, (8, 8), strides=(4, 4), padding='same', input_shape=(4,84,84,1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # Capa 2: Conv2D con BatchNorm y ReLU\n",
        "    model.add(Convolution2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # Capa 3: Conv2D con BatchNorm y ReLU\n",
        "    model.add(Convolution2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # Aplanar + Capas Dense\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Capa Fully Connected con Dropout\n",
        "    model.add(Dense(512))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    # model.add(Dropout(0.3))  # Opcional para evitar overfitting\n",
        "\n",
        "    # Capa de salida\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K93-fNbEXJku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eBwiHKuZ6J_",
        "outputId": "02d7fc3e-eb0b-4ffb-b592-fb2994514c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 4, 21, 21, 32)     2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4, 21, 21, 32)     128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4, 21, 21, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 11, 11, 64)     32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 11, 11, 64)     256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 11, 11, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 11, 11, 64)     36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 11, 11, 64)     256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4, 11, 11, 64)     0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 30976)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               15860224  \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 15,937,830\n",
            "Trainable params: 15,936,486\n",
            "Non-trainable params: 1,344\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(84, 84, 1, nb_actions)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar la implementación se creó una red neuronal convolucional para tareas de visión por computadora. La arquitectura se eligió considerando buenas prácticas validadas en literatura como el paper original de DeepMind (Mnih et al., 2015), adaptándola con batch normalization y capacidad para extenderse hacia variantes como Double DQN o Dueling DQN.\n",
        "\n",
        "Así mismo, a continuación justificamos la estructura:\n",
        "- Entrada de 4 frames apilados: Lo que permite capturar la dinámica temporal del entorno.\n",
        "- 3 Capas convolucionales profundas: Que nos permite extraer patrones visuales progresivamente desde bordes simples hasta características más complejas.\n",
        "- Batch Normalization: Que sirve para mejorar la estabilidad y acelera la convergencia, siendo especialmente útil cuando se entrenan en sesiones de tiempo limitado (como en el caso de este modelo, que fue ejecutado empleando Colab).\n",
        "- Capa densa de 512 neuronas: Que actuó como codificador del estado de la toma de decisiones.\n",
        "- Capa de salida lineal: Que produjo los valores Q para cada acción posible, sin necesidad de usar el softmax.\n",
        "\n",
        "**Antecedentes técnicos y de uso**:\n",
        "Debido a que en mi caso no contaba con recursos de cómputo locales con GPU integrado, opté por desarrollar y entrenar el modelo usando Google Colab, aprovechando:\n",
        "1.   Acceso gratuito a GPU.\n",
        "1.   Facilidad de colaboración en línea con el equipo de trabajo.\n",
        "1.   Soporte integrado para librerias requeridas.\n",
        "1.   Cabe resaltar, que por la disponibilidad de Colab, se almacenaron los pesos paulatinamente en una ruta de Drive para no perder el entrenamiento."
      ],
      "metadata": {
        "id": "8bpkfX1JZd-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta conffiguración se construye un agente DQN que incorpora una policy de exploración epsilon greedy con disminución lineal, utilizando la clase \"LinearAnnealedPolicy\". Con el valor de epsilon empezando en 1.0, permitiendo una máxima exploración en las primeras etapas, y decreciendo gradualmente hasta 0.01 a lo largo de los steps.\n",
        "\n",
        "Durante las pruebas, se fijó en 0.005, garantizando una policy casi completamente explotativa. Este esquema le permitió al agente aprender progresivamente a tomar decisiones más deterministas basadas en la experiencia acumulada, sin eliminar completamente la capacidad de explorar."
      ],
      "metadata": {
        "id": "NcuKFNNAniu4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EDlLcIIPUsf"
      },
      "outputs": [],
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FbbInVqR0sg"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=0.01, value_test=0.005, nb_steps=1000000)\n",
        "    memory = SequentialMemory(limit=500000, window_length=4)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  enable_dueling_network=True, dueling_type='avg',\n",
        "                   nb_actions=nb_actions, nb_steps_warmup=1000,\n",
        "                   batch_size=32,\n",
        "                   processor=AtariProcessor()\n",
        "                  )\n",
        "    return dqn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así mismo, se definió una memoria secuencial (sequentialmemory) con una capacidad de 500,000 transiciones y una ventana de 4 pasos. Así mismo, el entrenamiento inicia tras un corto periodo de \"calentamiento\" o warm-up de 1000 pasos, en el cual el agente acumula experiencias antes de comenzar a actualizar los pesos de la red.\n",
        "\n",
        "Además, se activó la arquitectura del modelo Dueling DQN empleando el comando: \"Enable_dueling_network = TRUE\", con un tipo de agregación \"avg\", el cual separa la estimación del valor del estado (V) y la ventaja de cada acción (A), ayudando al agente a distinguir de manera más clara entre la calidad del estado y la calidad de las acciones disponibles. Esto permite mejorar el aprendizaje en escenarios donde muchas acciones tienen efectos similares.\n",
        "\n",
        "Finalmente, optamos por compilar el modelo con el optimizador ADAM y una tasa de aprendizaje de 0.0001, un valor comúnmente empleado por el balance entre velocidad de convergencia y estabilidad. También, se configuraron callbacks para guardar los pesos del modelo cada 10,000 pasos. Lo que permite retomar el entrenamiento o analizar versiones intermedias del agente. Aunque posteriormente, por temas de memoria en el Drive se empezaron a crear cada 25,000 pasos."
      ],
      "metadata": {
        "id": "ymH3lfyYoTog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VBridsNeoQFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnuIYPrTUE1X"
      },
      "outputs": [],
      "source": [
        "from rl.callbacks import ModelIntervalCheckpoint\n",
        "\n",
        "# Guardar el modelo cada 50,000 pasos\n",
        "checkpoint_callback = ModelIntervalCheckpoint(\n",
        "    filepath='checkpoints/dqn_weights_{step}.h5f',\n",
        "    interval=10000,  # cada 10,000 pasos\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL513wUlUE1X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-jxyOrdR0cg"
      },
      "outputs": [],
      "source": [
        "dqn = build_agent(model, nb_actions)\n",
        "dqn.compile(Adam(learning_rate=1e-4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La implementación del agente DQN presentada se justifica por su adecuación a entornos complejos como los videojuegos tipo Atari, donde las observaciones son visuales y secuenciales. Para ello, se emplea un procesador especializado (AtariProcessor) y una memoria de experiencia con window_length=4, lo que permite al agente capturar la dinámica temporal del entorno. El uso de la policy epsilon-greedy con decaimiento lineal favorece una exploración intensiva en las primeras etapas del entrenamiento y una explotación eficiente en las fases finales, distribuyendo el aprendizaje de forma progresiva y estable.\n",
        "\n",
        "Además, se activa la arquitectura Dueling DQN, que separa la estimación del valor del estado y la ventaja de cada acción, permitiendo al agente aprender de forma más robusta incluso en estados donde algunas acciones tienen poco efecto. Esta mejora arquitectónica, combinada con la memoria de repetición y un tamaño de lote adecuado, contribuye a reducir la varianza y mejorar la estabilidad del aprendizaje.\n",
        "\n",
        "Finalmente, el modelo se entrena con el optimizador Adam y se configura un sistema de guardado periódico que permite preservar versiones del agente a lo largo del entrenamiento, facilitando la recuperación y el análisis posterior del desempeño."
      ],
      "metadata": {
        "id": "PXY7DyFucMzb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK6SMHjHeRX2"
      },
      "source": [
        "#### Entrenamiento parte 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR2u9_7LR0WI",
        "outputId": "f5b2e047-6e50-431c-baf7-3fa0fcbcc849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏱ Tiempo de inicio: 18:21:10\n",
            "Training for 300000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    513/300000: episode: 1, duration: 5.703s, episode steps: 513, steps per second:  90, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\curso\\anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   1189/300000: episode: 2, duration: 51.488s, episode steps: 676, steps per second:  13, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.199296, mean_q: 0.938525, mean_eps: 0.998916\n",
            "   1706/300000: episode: 3, duration: 120.204s, episode steps: 517, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.027185, mean_q: 0.955640, mean_eps: 0.998567\n",
            "   2324/300000: episode: 4, duration: 141.814s, episode steps: 618, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015326, mean_q: 0.961571, mean_eps: 0.998006\n",
            "   3138/300000: episode: 5, duration: 186.313s, episode steps: 814, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.012298, mean_q: 0.852844, mean_eps: 0.997297\n",
            "   3500/300000: episode: 6, duration: 82.863s, episode steps: 362, steps per second:   4, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010441, mean_q: 0.794642, mean_eps: 0.996715\n",
            "   4131/300000: episode: 7, duration: 144.222s, episode steps: 631, steps per second:   4, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.010650, mean_q: 0.847168, mean_eps: 0.996223\n",
            "   4652/300000: episode: 8, duration: 119.340s, episode steps: 521, steps per second:   4, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.010031, mean_q: 0.833450, mean_eps: 0.995653\n",
            "   5380/300000: episode: 9, duration: 166.550s, episode steps: 728, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.008512, mean_q: 0.801495, mean_eps: 0.995035\n",
            "   6311/300000: episode: 10, duration: 212.658s, episode steps: 931, steps per second:   4, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.009074, mean_q: 0.757011, mean_eps: 0.994213\n",
            "   6980/300000: episode: 11, duration: 153.080s, episode steps: 669, steps per second:   4, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.009585, mean_q: 0.714388, mean_eps: 0.993421\n",
            "   7402/300000: episode: 12, duration: 96.462s, episode steps: 422, steps per second:   4, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007699, mean_q: 0.693806, mean_eps: 0.992881\n",
            "   7765/300000: episode: 13, duration: 83.049s, episode steps: 363, steps per second:   4, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.008751, mean_q: 0.739306, mean_eps: 0.992493\n",
            "   8962/300000: episode: 14, duration: 274.595s, episode steps: 1197, steps per second:   4, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007791, mean_q: 0.715231, mean_eps: 0.991721\n",
            "Step 10000: saving model to checkpoints/dqn_weights_10000.h5f\n",
            "  10129/300000: episode: 15, duration: 270.568s, episode steps: 1167, steps per second:   4, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.008586, mean_q: 0.662821, mean_eps: 0.990550\n",
            "  10799/300000: episode: 16, duration: 166.663s, episode steps: 670, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011566, mean_q: 0.750392, mean_eps: 0.989641\n",
            "  11200/300000: episode: 17, duration: 105.402s, episode steps: 401, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.009164, mean_q: 0.733362, mean_eps: 0.989111\n",
            "  11893/300000: episode: 18, duration: 180.393s, episode steps: 693, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.008845, mean_q: 0.765789, mean_eps: 0.988569\n",
            "  12573/300000: episode: 19, duration: 176.531s, episode steps: 680, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.008912, mean_q: 0.768963, mean_eps: 0.987890\n",
            "  13103/300000: episode: 20, duration: 134.514s, episode steps: 530, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.008193, mean_q: 0.762021, mean_eps: 0.987291\n",
            "  13743/300000: episode: 21, duration: 145.694s, episode steps: 640, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.007649, mean_q: 0.753008, mean_eps: 0.986712\n",
            "  14400/300000: episode: 22, duration: 149.834s, episode steps: 657, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.007371, mean_q: 0.767869, mean_eps: 0.986070\n",
            "  15073/300000: episode: 23, duration: 153.127s, episode steps: 673, steps per second:   4, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.007519, mean_q: 0.748557, mean_eps: 0.985411\n",
            "  15754/300000: episode: 24, duration: 154.794s, episode steps: 681, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.006628, mean_q: 0.736947, mean_eps: 0.984741\n",
            "  16931/300000: episode: 25, duration: 267.286s, episode steps: 1177, steps per second:   4, episode reward:  9.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.006886, mean_q: 0.732480, mean_eps: 0.983821\n",
            "  18176/300000: episode: 26, duration: 283.296s, episode steps: 1245, steps per second:   4, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.007468, mean_q: 0.709744, mean_eps: 0.982623\n",
            "  18801/300000: episode: 27, duration: 142.069s, episode steps: 625, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.007618, mean_q: 0.709009, mean_eps: 0.981697\n",
            "  19598/300000: episode: 28, duration: 181.696s, episode steps: 797, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006916, mean_q: 0.694779, mean_eps: 0.980993\n",
            "  19985/300000: episode: 29, duration: 88.168s, episode steps: 387, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.006813, mean_q: 0.673920, mean_eps: 0.980407\n",
            "Step 20000: saving model to checkpoints/dqn_weights_20000.h5f\n",
            "  21111/300000: episode: 30, duration: 257.225s, episode steps: 1126, steps per second:   4, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010249, mean_q: 0.821125, mean_eps: 0.979658\n",
            "  21840/300000: episode: 31, duration: 166.158s, episode steps: 729, steps per second:   4, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.008697, mean_q: 0.819776, mean_eps: 0.978740\n",
            "  22328/300000: episode: 32, duration: 111.442s, episode steps: 488, steps per second:   4, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.008308, mean_q: 0.808001, mean_eps: 0.978137\n",
            "  23137/300000: episode: 33, duration: 183.217s, episode steps: 809, steps per second:   4, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.007939, mean_q: 0.797738, mean_eps: 0.977495\n",
            "  24279/300000: episode: 34, duration: 257.379s, episode steps: 1142, steps per second:   4, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.007669, mean_q: 0.791415, mean_eps: 0.976530\n",
            "  25106/300000: episode: 35, duration: 186.729s, episode steps: 827, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.007848, mean_q: 0.790377, mean_eps: 0.975555\n",
            "  25757/300000: episode: 36, duration: 147.096s, episode steps: 651, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.007439, mean_q: 0.797625, mean_eps: 0.974823\n",
            "  26271/300000: episode: 37, duration: 116.185s, episode steps: 514, steps per second:   4, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.007437, mean_q: 0.789041, mean_eps: 0.974247\n",
            "  26955/300000: episode: 38, duration: 154.163s, episode steps: 684, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.007602, mean_q: 0.801001, mean_eps: 0.973654\n",
            "  27780/300000: episode: 39, duration: 186.801s, episode steps: 825, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007182, mean_q: 0.794714, mean_eps: 0.972907\n",
            "  28596/300000: episode: 40, duration: 184.151s, episode steps: 816, steps per second:   4, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006956, mean_q: 0.783611, mean_eps: 0.972094\n",
            "  29275/300000: episode: 41, duration: 153.453s, episode steps: 679, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.006440, mean_q: 0.797663, mean_eps: 0.971354\n",
            "  29681/300000: episode: 42, duration: 91.797s, episode steps: 406, steps per second:   4, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.006112, mean_q: 0.785129, mean_eps: 0.970817\n",
            "Step 30000: saving model to checkpoints/dqn_weights_30000.h5f\n",
            "  30350/300000: episode: 43, duration: 151.697s, episode steps: 669, steps per second:   4, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.008760, mean_q: 0.839578, mean_eps: 0.970285\n",
            "  30981/300000: episode: 44, duration: 142.056s, episode steps: 631, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.009168, mean_q: 0.908041, mean_eps: 0.969642\n",
            "  31385/300000: episode: 45, duration: 91.293s, episode steps: 404, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.008591, mean_q: 0.896279, mean_eps: 0.969129\n",
            "  31770/300000: episode: 46, duration: 86.895s, episode steps: 385, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.008710, mean_q: 0.909663, mean_eps: 0.968739\n",
            "  32197/300000: episode: 47, duration: 96.447s, episode steps: 427, steps per second:   4, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.008291, mean_q: 0.916325, mean_eps: 0.968337\n",
            "  32783/300000: episode: 48, duration: 132.371s, episode steps: 586, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.008237, mean_q: 0.908347, mean_eps: 0.967835\n",
            "  33481/300000: episode: 49, duration: 157.546s, episode steps: 698, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.007931, mean_q: 0.935019, mean_eps: 0.967200\n",
            "  34599/300000: episode: 50, duration: 253.727s, episode steps: 1118, steps per second:   4, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008040, mean_q: 0.924329, mean_eps: 0.966301\n",
            "  35105/300000: episode: 51, duration: 114.120s, episode steps: 506, steps per second:   4, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008234, mean_q: 0.918342, mean_eps: 0.965497\n",
            "  35790/300000: episode: 52, duration: 154.123s, episode steps: 685, steps per second:   4, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.007822, mean_q: 0.921513, mean_eps: 0.964907\n",
            "  36952/300000: episode: 53, duration: 261.813s, episode steps: 1162, steps per second:   4, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.007938, mean_q: 0.915890, mean_eps: 0.963993\n",
            "  38077/300000: episode: 54, duration: 253.200s, episode steps: 1125, steps per second:   4, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.007135, mean_q: 0.913929, mean_eps: 0.962861\n",
            "  38861/300000: episode: 55, duration: 176.628s, episode steps: 784, steps per second:   4, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007685, mean_q: 0.908552, mean_eps: 0.961916\n",
            "  39363/300000: episode: 56, duration: 113.666s, episode steps: 502, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.007463, mean_q: 0.904116, mean_eps: 0.961280\n",
            "  39855/300000: episode: 57, duration: 111.093s, episode steps: 492, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.007116, mean_q: 0.908187, mean_eps: 0.960788\n",
            "Step 40000: saving model to checkpoints/dqn_weights_40000.h5f\n",
            "  40328/300000: episode: 58, duration: 107.251s, episode steps: 473, steps per second:   4, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.009506, mean_q: 0.984332, mean_eps: 0.960310\n",
            "  40857/300000: episode: 59, duration: 119.558s, episode steps: 529, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.009620, mean_q: 1.016761, mean_eps: 0.959814\n",
            "  41218/300000: episode: 60, duration: 81.497s, episode steps: 361, steps per second:   4, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.009198, mean_q: 1.027224, mean_eps: 0.959373\n",
            "  41829/300000: episode: 61, duration: 138.325s, episode steps: 611, steps per second:   4, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.008870, mean_q: 1.019080, mean_eps: 0.958892\n",
            "  42612/300000: episode: 62, duration: 176.299s, episode steps: 783, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.008557, mean_q: 1.035984, mean_eps: 0.958202\n",
            "  43137/300000: episode: 63, duration: 118.266s, episode steps: 525, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.008229, mean_q: 1.013755, mean_eps: 0.957555\n",
            "  43696/300000: episode: 64, duration: 127.135s, episode steps: 559, steps per second:   4, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.008323, mean_q: 1.047641, mean_eps: 0.957018\n",
            "  44300/300000: episode: 65, duration: 140.205s, episode steps: 604, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.007915, mean_q: 1.037329, mean_eps: 0.956442\n",
            "  45151/300000: episode: 66, duration: 192.182s, episode steps: 851, steps per second:   4, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.007771, mean_q: 1.025259, mean_eps: 0.955722\n",
            "  45500/300000: episode: 67, duration: 79.006s, episode steps: 349, steps per second:   4, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007771, mean_q: 1.012975, mean_eps: 0.955128\n",
            "  46714/300000: episode: 68, duration: 273.787s, episode steps: 1214, steps per second:   4, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008016, mean_q: 1.034028, mean_eps: 0.954355\n",
            "  47381/300000: episode: 69, duration: 150.548s, episode steps: 667, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.007708, mean_q: 1.028591, mean_eps: 0.953423\n",
            "  48391/300000: episode: 70, duration: 227.768s, episode steps: 1010, steps per second:   4, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.008055, mean_q: 1.042035, mean_eps: 0.952593\n",
            "  48779/300000: episode: 71, duration: 87.792s, episode steps: 388, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007311, mean_q: 1.012482, mean_eps: 0.951901\n",
            "Step 50000: saving model to checkpoints/dqn_weights_50000.h5f\n",
            "  50146/300000: episode: 72, duration: 308.384s, episode steps: 1367, steps per second:   4, episode reward: 15.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.007719, mean_q: 1.038019, mean_eps: 0.951033\n",
            "  50964/300000: episode: 73, duration: 184.584s, episode steps: 818, steps per second:   4, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.010462, mean_q: 1.207531, mean_eps: 0.949951\n",
            "  51488/300000: episode: 74, duration: 118.155s, episode steps: 524, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009286, mean_q: 1.214471, mean_eps: 0.949287\n",
            "  52537/300000: episode: 75, duration: 236.533s, episode steps: 1049, steps per second:   4, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.008986, mean_q: 1.203691, mean_eps: 0.948508\n",
            "  53210/300000: episode: 76, duration: 152.160s, episode steps: 673, steps per second:   4, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.009118, mean_q: 1.201260, mean_eps: 0.947656\n",
            "  53833/300000: episode: 77, duration: 140.866s, episode steps: 623, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.008680, mean_q: 1.180957, mean_eps: 0.947014\n",
            "  54884/300000: episode: 78, duration: 237.174s, episode steps: 1051, steps per second:   4, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.008836, mean_q: 1.195814, mean_eps: 0.946186\n",
            "  55567/300000: episode: 79, duration: 154.144s, episode steps: 683, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.008377, mean_q: 1.185224, mean_eps: 0.945327\n",
            "  56473/300000: episode: 80, duration: 204.811s, episode steps: 906, steps per second:   4, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008764, mean_q: 1.189235, mean_eps: 0.944541\n",
            "  57368/300000: episode: 81, duration: 214.958s, episode steps: 895, steps per second:   4, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.008536, mean_q: 1.176192, mean_eps: 0.943649\n",
            "  57923/300000: episode: 82, duration: 135.619s, episode steps: 555, steps per second:   4, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.007927, mean_q: 1.172882, mean_eps: 0.942931\n",
            "  58574/300000: episode: 83, duration: 165.637s, episode steps: 651, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.008276, mean_q: 1.175971, mean_eps: 0.942334\n",
            "  59187/300000: episode: 84, duration: 149.771s, episode steps: 613, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008021, mean_q: 1.176183, mean_eps: 0.941709\n",
            "Step 60000: saving model to checkpoints/dqn_weights_60000.h5f\n",
            "  60298/300000: episode: 85, duration: 255.490s, episode steps: 1111, steps per second:   4, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.008775, mean_q: 1.216901, mean_eps: 0.940855\n",
            "  60929/300000: episode: 86, duration: 142.958s, episode steps: 631, steps per second:   4, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.010522, mean_q: 1.306515, mean_eps: 0.939993\n",
            "  61547/300000: episode: 87, duration: 139.781s, episode steps: 618, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010080, mean_q: 1.323250, mean_eps: 0.939375\n",
            "  62276/300000: episode: 88, duration: 164.459s, episode steps: 729, steps per second:   4, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.009574, mean_q: 1.316605, mean_eps: 0.938708\n",
            "  63089/300000: episode: 89, duration: 183.066s, episode steps: 813, steps per second:   4, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.009031, mean_q: 1.298693, mean_eps: 0.937945\n",
            "  63541/300000: episode: 90, duration: 101.625s, episode steps: 452, steps per second:   4, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.008885, mean_q: 1.316367, mean_eps: 0.937319\n",
            "  64664/300000: episode: 91, duration: 253.099s, episode steps: 1123, steps per second:   4, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.009251, mean_q: 1.318411, mean_eps: 0.936539\n",
            "  65247/300000: episode: 92, duration: 131.863s, episode steps: 583, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.008577, mean_q: 1.319278, mean_eps: 0.935695\n",
            "  66117/300000: episode: 93, duration: 196.443s, episode steps: 870, steps per second:   4, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.008852, mean_q: 1.317590, mean_eps: 0.934975\n",
            "  66976/300000: episode: 94, duration: 194.058s, episode steps: 859, steps per second:   4, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.009048, mean_q: 1.317463, mean_eps: 0.934119\n",
            "  67805/300000: episode: 95, duration: 187.266s, episode steps: 829, steps per second:   4, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.008474, mean_q: 1.308948, mean_eps: 0.933284\n",
            "  68357/300000: episode: 96, duration: 124.765s, episode steps: 552, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.008737, mean_q: 1.307317, mean_eps: 0.932600\n",
            "  69278/300000: episode: 97, duration: 207.758s, episode steps: 921, steps per second:   4, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.008845, mean_q: 1.309685, mean_eps: 0.931871\n",
            "  69711/300000: episode: 98, duration: 97.761s, episode steps: 433, steps per second:   4, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.008963, mean_q: 1.293679, mean_eps: 0.931201\n",
            "Step 70000: saving model to checkpoints/dqn_weights_70000.h5f\n",
            "  70215/300000: episode: 99, duration: 114.086s, episode steps: 504, steps per second:   4, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.009639, mean_q: 1.358563, mean_eps: 0.930737\n",
            "  70841/300000: episode: 100, duration: 141.298s, episode steps: 626, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010927, mean_q: 1.431754, mean_eps: 0.930178\n",
            "  71333/300000: episode: 101, duration: 111.166s, episode steps: 492, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010299, mean_q: 1.445152, mean_eps: 0.929624\n",
            "  72159/300000: episode: 102, duration: 186.747s, episode steps: 826, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.010064, mean_q: 1.444151, mean_eps: 0.928972\n",
            "  72904/300000: episode: 103, duration: 168.117s, episode steps: 745, steps per second:   4, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.009937, mean_q: 1.448751, mean_eps: 0.928194\n",
            "  73412/300000: episode: 104, duration: 114.509s, episode steps: 508, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.009791, mean_q: 1.459941, mean_eps: 0.927574\n",
            "  73955/300000: episode: 105, duration: 122.479s, episode steps: 543, steps per second:   4, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.009609, mean_q: 1.425718, mean_eps: 0.927054\n",
            "  74379/300000: episode: 106, duration: 95.973s, episode steps: 424, steps per second:   4, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.009927, mean_q: 1.453211, mean_eps: 0.926575\n",
            "  75322/300000: episode: 107, duration: 212.796s, episode steps: 943, steps per second:   4, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.009058, mean_q: 1.453135, mean_eps: 0.925898\n",
            "  76234/300000: episode: 108, duration: 206.727s, episode steps: 912, steps per second:   4, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.009867, mean_q: 1.440210, mean_eps: 0.924980\n",
            "  76744/300000: episode: 109, duration: 115.342s, episode steps: 510, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.009415, mean_q: 1.443438, mean_eps: 0.924276\n",
            "  77501/300000: episode: 110, duration: 171.325s, episode steps: 757, steps per second:   4, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.009109, mean_q: 1.462547, mean_eps: 0.923649\n",
            "  77926/300000: episode: 111, duration: 96.388s, episode steps: 425, steps per second:   4, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.009432, mean_q: 1.451476, mean_eps: 0.923064\n",
            "  78589/300000: episode: 112, duration: 150.339s, episode steps: 663, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009001, mean_q: 1.457990, mean_eps: 0.922526\n",
            "  79278/300000: episode: 113, duration: 156.335s, episode steps: 689, steps per second:   4, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.009297, mean_q: 1.445979, mean_eps: 0.921856\n",
            "Step 80000: saving model to checkpoints/dqn_weights_80000.h5f\n",
            "  80162/300000: episode: 114, duration: 200.502s, episode steps: 884, steps per second:   4, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.009552, mean_q: 1.472504, mean_eps: 0.921078\n",
            "  80634/300000: episode: 115, duration: 107.478s, episode steps: 472, steps per second:   4, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.011418, mean_q: 1.574773, mean_eps: 0.920406\n",
            "  81034/300000: episode: 116, duration: 90.743s, episode steps: 400, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010810, mean_q: 1.556383, mean_eps: 0.919975\n",
            "  82101/300000: episode: 117, duration: 242.278s, episode steps: 1067, steps per second:   4, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010847, mean_q: 1.562278, mean_eps: 0.919249\n",
            "  82492/300000: episode: 118, duration: 88.858s, episode steps: 391, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.010700, mean_q: 1.569720, mean_eps: 0.918527\n",
            "  83137/300000: episode: 119, duration: 146.856s, episode steps: 645, steps per second:   4, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.010619, mean_q: 1.574322, mean_eps: 0.918014\n",
            "  83516/300000: episode: 120, duration: 86.179s, episode steps: 379, steps per second:   4, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010580, mean_q: 1.567114, mean_eps: 0.917507\n",
            "  84197/300000: episode: 121, duration: 154.761s, episode steps: 681, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.010413, mean_q: 1.576529, mean_eps: 0.916983\n",
            "  85278/300000: episode: 122, duration: 245.277s, episode steps: 1081, steps per second:   4, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.009956, mean_q: 1.580902, mean_eps: 0.916110\n",
            "  85675/300000: episode: 123, duration: 90.331s, episode steps: 397, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.009728, mean_q: 1.548338, mean_eps: 0.915379\n",
            "  86358/300000: episode: 124, duration: 155.030s, episode steps: 683, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.009254, mean_q: 1.559954, mean_eps: 0.914844\n",
            "  86783/300000: episode: 125, duration: 96.909s, episode steps: 425, steps per second:   4, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.010157, mean_q: 1.584264, mean_eps: 0.914296\n",
            "  87449/300000: episode: 126, duration: 151.398s, episode steps: 666, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010115, mean_q: 1.582907, mean_eps: 0.913756\n",
            "  87849/300000: episode: 127, duration: 91.001s, episode steps: 400, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.009477, mean_q: 1.566322, mean_eps: 0.913228\n",
            "  88312/300000: episode: 128, duration: 105.473s, episode steps: 463, steps per second:   4, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009588, mean_q: 1.579799, mean_eps: 0.912801\n",
            "  88954/300000: episode: 129, duration: 145.565s, episode steps: 642, steps per second:   4, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.009273, mean_q: 1.578634, mean_eps: 0.912254\n",
            "  89643/300000: episode: 130, duration: 156.474s, episode steps: 689, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.008967, mean_q: 1.576318, mean_eps: 0.911595\n",
            "Step 90000: saving model to checkpoints/dqn_weights_90000.h5f\n",
            "  90335/300000: episode: 131, duration: 157.005s, episode steps: 692, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.010713, mean_q: 1.616906, mean_eps: 0.910911\n",
            "  91014/300000: episode: 132, duration: 154.229s, episode steps: 679, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.011627, mean_q: 1.644978, mean_eps: 0.910233\n",
            "  91584/300000: episode: 133, duration: 129.406s, episode steps: 570, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010411, mean_q: 1.632293, mean_eps: 0.909614\n",
            "  92106/300000: episode: 134, duration: 118.741s, episode steps: 522, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011464, mean_q: 1.628281, mean_eps: 0.909074\n",
            "  92830/300000: episode: 135, duration: 164.091s, episode steps: 724, steps per second:   4, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.010477, mean_q: 1.647788, mean_eps: 0.908457\n",
            "  93532/300000: episode: 136, duration: 158.923s, episode steps: 702, steps per second:   4, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010168, mean_q: 1.624597, mean_eps: 0.907751\n",
            "  94330/300000: episode: 137, duration: 180.590s, episode steps: 798, steps per second:   4, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010756, mean_q: 1.648310, mean_eps: 0.907009\n",
            "  94975/300000: episode: 138, duration: 146.126s, episode steps: 645, steps per second:   4, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010685, mean_q: 1.633640, mean_eps: 0.906295\n",
            "  95351/300000: episode: 139, duration: 85.261s, episode steps: 376, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010238, mean_q: 1.641390, mean_eps: 0.905789\n",
            "  96017/300000: episode: 140, duration: 151.069s, episode steps: 666, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.011006, mean_q: 1.640940, mean_eps: 0.905273\n",
            "  96605/300000: episode: 141, duration: 133.765s, episode steps: 588, steps per second:   4, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010454, mean_q: 1.634781, mean_eps: 0.904653\n",
            "  97090/300000: episode: 142, duration: 110.271s, episode steps: 485, steps per second:   4, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.009603, mean_q: 1.641718, mean_eps: 0.904121\n",
            "  97636/300000: episode: 143, duration: 124.075s, episode steps: 546, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.009620, mean_q: 1.617067, mean_eps: 0.903611\n",
            "  98358/300000: episode: 144, duration: 163.719s, episode steps: 722, steps per second:   4, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.010192, mean_q: 1.640764, mean_eps: 0.902983\n",
            "  99034/300000: episode: 145, duration: 153.668s, episode steps: 676, steps per second:   4, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.010007, mean_q: 1.624236, mean_eps: 0.902291\n",
            "  99526/300000: episode: 146, duration: 111.734s, episode steps: 492, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.009227, mean_q: 1.617737, mean_eps: 0.901713\n",
            "Step 100000: saving model to checkpoints/dqn_weights_100000.h5f\n",
            " 100487/300000: episode: 147, duration: 220.094s, episode steps: 961, steps per second:   4, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011425, mean_q: 1.686135, mean_eps: 0.900994\n",
            " 101039/300000: episode: 148, duration: 125.238s, episode steps: 552, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.011164, mean_q: 1.731324, mean_eps: 0.900245\n",
            " 101817/300000: episode: 149, duration: 176.589s, episode steps: 778, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.011596, mean_q: 1.721890, mean_eps: 0.899587\n",
            " 102210/300000: episode: 150, duration: 89.252s, episode steps: 393, steps per second:   4, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.011435, mean_q: 1.723204, mean_eps: 0.899007\n",
            " 103036/300000: episode: 151, duration: 187.443s, episode steps: 826, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.011375, mean_q: 1.724230, mean_eps: 0.898404\n",
            " 103659/300000: episode: 152, duration: 141.375s, episode steps: 623, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.010739, mean_q: 1.739519, mean_eps: 0.897686\n",
            " 104193/300000: episode: 153, duration: 121.857s, episode steps: 534, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011234, mean_q: 1.739261, mean_eps: 0.897114\n",
            " 104923/300000: episode: 154, duration: 166.180s, episode steps: 730, steps per second:   4, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010864, mean_q: 1.726351, mean_eps: 0.896488\n",
            " 105746/300000: episode: 155, duration: 186.697s, episode steps: 823, steps per second:   4, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.010412, mean_q: 1.728944, mean_eps: 0.895719\n",
            " 106380/300000: episode: 156, duration: 144.449s, episode steps: 634, steps per second:   4, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.010783, mean_q: 1.718333, mean_eps: 0.894998\n",
            " 106777/300000: episode: 157, duration: 90.387s, episode steps: 397, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.010981, mean_q: 1.718441, mean_eps: 0.894488\n",
            " 107260/300000: episode: 158, duration: 109.754s, episode steps: 483, steps per second:   4, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010337, mean_q: 1.716060, mean_eps: 0.894052\n",
            " 108250/300000: episode: 159, duration: 225.082s, episode steps: 990, steps per second:   4, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.010791, mean_q: 1.735586, mean_eps: 0.893323\n",
            " 108947/300000: episode: 160, duration: 158.527s, episode steps: 697, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.010434, mean_q: 1.717081, mean_eps: 0.892488\n",
            "Step 110000: saving model to checkpoints/dqn_weights_110000.h5f\n",
            " 110430/300000: episode: 161, duration: 337.137s, episode steps: 1483, steps per second:   4, episode reward: 18.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011171, mean_q: 1.749171, mean_eps: 0.891409\n",
            " 110802/300000: episode: 162, duration: 84.407s, episode steps: 372, steps per second:   4, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011639, mean_q: 1.783375, mean_eps: 0.890491\n",
            " 111352/300000: episode: 163, duration: 124.793s, episode steps: 550, steps per second:   4, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012361, mean_q: 1.789638, mean_eps: 0.890034\n",
            " 111972/300000: episode: 164, duration: 140.578s, episode steps: 620, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011408, mean_q: 1.794749, mean_eps: 0.889455\n",
            " 112613/300000: episode: 165, duration: 145.767s, episode steps: 641, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011415, mean_q: 1.796691, mean_eps: 0.888831\n",
            " 113264/300000: episode: 166, duration: 148.313s, episode steps: 651, steps per second:   4, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.011175, mean_q: 1.795518, mean_eps: 0.888191\n",
            " 114183/300000: episode: 167, duration: 209.051s, episode steps: 919, steps per second:   4, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.010987, mean_q: 1.782591, mean_eps: 0.887414\n",
            " 114984/300000: episode: 168, duration: 182.150s, episode steps: 801, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.010881, mean_q: 1.793861, mean_eps: 0.886563\n",
            " 115552/300000: episode: 169, duration: 129.186s, episode steps: 568, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011211, mean_q: 1.791352, mean_eps: 0.885885\n",
            " 116387/300000: episode: 170, duration: 189.977s, episode steps: 835, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.010948, mean_q: 1.799380, mean_eps: 0.885191\n",
            " 117182/300000: episode: 171, duration: 180.352s, episode steps: 795, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011085, mean_q: 1.786214, mean_eps: 0.884384\n",
            " 118141/300000: episode: 172, duration: 217.909s, episode steps: 959, steps per second:   4, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.010822, mean_q: 1.785374, mean_eps: 0.883516\n",
            " 118928/300000: episode: 173, duration: 178.690s, episode steps: 787, steps per second:   4, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010528, mean_q: 1.783568, mean_eps: 0.882651\n",
            " 119334/300000: episode: 174, duration: 92.782s, episode steps: 406, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.010363, mean_q: 1.793375, mean_eps: 0.882061\n",
            "Step 120000: saving model to checkpoints/dqn_weights_120000.h5f\n",
            " 120042/300000: episode: 175, duration: 161.223s, episode steps: 708, steps per second:   4, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.010949, mean_q: 1.789502, mean_eps: 0.881509\n",
            " 120580/300000: episode: 176, duration: 122.479s, episode steps: 538, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013467, mean_q: 1.860158, mean_eps: 0.880893\n",
            " 121064/300000: episode: 177, duration: 109.882s, episode steps: 484, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011885, mean_q: 1.881030, mean_eps: 0.880387\n",
            " 121844/300000: episode: 178, duration: 177.068s, episode steps: 780, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.012244, mean_q: 1.886643, mean_eps: 0.879761\n",
            " 122462/300000: episode: 179, duration: 140.078s, episode steps: 618, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.012855, mean_q: 1.882186, mean_eps: 0.879069\n",
            " 123164/300000: episode: 180, duration: 159.800s, episode steps: 702, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.011862, mean_q: 1.880571, mean_eps: 0.878416\n",
            " 123765/300000: episode: 181, duration: 136.786s, episode steps: 601, steps per second:   4, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.011853, mean_q: 1.869298, mean_eps: 0.877771\n",
            " 124550/300000: episode: 182, duration: 178.842s, episode steps: 785, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011898, mean_q: 1.864241, mean_eps: 0.877085\n",
            " 125731/300000: episode: 183, duration: 268.131s, episode steps: 1181, steps per second:   4, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011904, mean_q: 1.868001, mean_eps: 0.876111\n",
            " 126352/300000: episode: 184, duration: 141.308s, episode steps: 621, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011133, mean_q: 1.860594, mean_eps: 0.875219\n",
            " 126842/300000: episode: 185, duration: 111.406s, episode steps: 490, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.011191, mean_q: 1.855931, mean_eps: 0.874669\n",
            " 127481/300000: episode: 186, duration: 145.272s, episode steps: 639, steps per second:   4, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.011211, mean_q: 1.876986, mean_eps: 0.874111\n",
            " 128115/300000: episode: 187, duration: 144.414s, episode steps: 634, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.011141, mean_q: 1.879939, mean_eps: 0.873480\n",
            " 128786/300000: episode: 188, duration: 152.669s, episode steps: 671, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.010814, mean_q: 1.869587, mean_eps: 0.872834\n",
            " 129623/300000: episode: 189, duration: 190.812s, episode steps: 837, steps per second:   4, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011412, mean_q: 1.868045, mean_eps: 0.872088\n",
            "Step 130000: saving model to checkpoints/dqn_weights_130000.h5f\n",
            " 130112/300000: episode: 190, duration: 112.203s, episode steps: 489, steps per second:   4, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.011090, mean_q: 1.868236, mean_eps: 0.871432\n",
            " 130730/300000: episode: 191, duration: 140.405s, episode steps: 618, steps per second:   4, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013956, mean_q: 1.897518, mean_eps: 0.870884\n",
            " 131675/300000: episode: 192, duration: 215.458s, episode steps: 945, steps per second:   4, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.013041, mean_q: 1.898455, mean_eps: 0.870110\n",
            " 133135/300000: episode: 193, duration: 332.660s, episode steps: 1460, steps per second:   4, episode reward: 17.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012262, mean_q: 1.892125, mean_eps: 0.868920\n",
            " 133845/300000: episode: 194, duration: 161.824s, episode steps: 710, steps per second:   4, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.012072, mean_q: 1.880063, mean_eps: 0.867845\n",
            " 134378/300000: episode: 195, duration: 121.894s, episode steps: 533, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.012158, mean_q: 1.892942, mean_eps: 0.867230\n",
            " 135026/300000: episode: 196, duration: 148.174s, episode steps: 648, steps per second:   4, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.011612, mean_q: 1.883993, mean_eps: 0.866646\n",
            " 135520/300000: episode: 197, duration: 113.126s, episode steps: 494, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.012122, mean_q: 1.874921, mean_eps: 0.866080\n",
            " 136307/300000: episode: 198, duration: 180.081s, episode steps: 787, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012289, mean_q: 1.873885, mean_eps: 0.865446\n",
            " 137146/300000: episode: 199, duration: 191.732s, episode steps: 839, steps per second:   4, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.012130, mean_q: 1.879966, mean_eps: 0.864641\n",
            " 137816/300000: episode: 200, duration: 154.551s, episode steps: 670, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011925, mean_q: 1.877993, mean_eps: 0.863894\n",
            " 138240/300000: episode: 201, duration: 96.980s, episode steps: 424, steps per second:   4, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011347, mean_q: 1.882937, mean_eps: 0.863353\n",
            " 139221/300000: episode: 202, duration: 225.036s, episode steps: 981, steps per second:   4, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011372, mean_q: 1.892230, mean_eps: 0.862657\n",
            " 139867/300000: episode: 203, duration: 148.062s, episode steps: 646, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011193, mean_q: 1.877722, mean_eps: 0.861852\n",
            "Step 140000: saving model to checkpoints/dqn_weights_140000.h5f\n",
            " 140676/300000: episode: 204, duration: 185.294s, episode steps: 809, steps per second:   4, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013271, mean_q: 1.977254, mean_eps: 0.861132\n",
            " 141459/300000: episode: 205, duration: 178.710s, episode steps: 783, steps per second:   4, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012952, mean_q: 1.999972, mean_eps: 0.860344\n",
            " 142001/300000: episode: 206, duration: 123.897s, episode steps: 542, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012380, mean_q: 2.008803, mean_eps: 0.859688\n",
            " 142758/300000: episode: 207, duration: 172.709s, episode steps: 757, steps per second:   4, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.012760, mean_q: 2.001748, mean_eps: 0.859045\n",
            " 143269/300000: episode: 208, duration: 116.836s, episode steps: 511, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012395, mean_q: 1.989111, mean_eps: 0.858417\n",
            " 143986/300000: episode: 209, duration: 163.566s, episode steps: 717, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.012383, mean_q: 2.003727, mean_eps: 0.857809\n",
            " 144599/300000: episode: 210, duration: 140.274s, episode steps: 613, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.011955, mean_q: 2.002235, mean_eps: 0.857151\n",
            " 145752/300000: episode: 211, duration: 263.170s, episode steps: 1153, steps per second:   4, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.012337, mean_q: 2.002877, mean_eps: 0.856277\n",
            " 146387/300000: episode: 212, duration: 144.869s, episode steps: 635, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.011678, mean_q: 1.995818, mean_eps: 0.855392\n",
            " 146931/300000: episode: 213, duration: 124.359s, episode steps: 544, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012262, mean_q: 2.010583, mean_eps: 0.854808\n",
            " 147491/300000: episode: 214, duration: 130.237s, episode steps: 560, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.012362, mean_q: 2.004457, mean_eps: 0.854262\n",
            " 147979/300000: episode: 215, duration: 114.832s, episode steps: 488, steps per second:   4, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.011903, mean_q: 1.992400, mean_eps: 0.853743\n",
            " 148537/300000: episode: 216, duration: 137.964s, episode steps: 558, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012453, mean_q: 2.014259, mean_eps: 0.853225\n",
            " 149087/300000: episode: 217, duration: 135.807s, episode steps: 550, steps per second:   4, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.011521, mean_q: 1.988560, mean_eps: 0.852677\n",
            " 149707/300000: episode: 218, duration: 142.782s, episode steps: 620, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.011813, mean_q: 1.991176, mean_eps: 0.852097\n",
            "Step 150000: saving model to checkpoints/dqn_weights_150000.h5f\n",
            " 150799/300000: episode: 219, duration: 252.022s, episode steps: 1092, steps per second:   4, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013524, mean_q: 2.141634, mean_eps: 0.851250\n",
            " 151576/300000: episode: 220, duration: 178.440s, episode steps: 777, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014242, mean_q: 2.197016, mean_eps: 0.850325\n",
            " 152156/300000: episode: 221, duration: 133.302s, episode steps: 580, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014683, mean_q: 2.186646, mean_eps: 0.849653\n",
            " 152727/300000: episode: 222, duration: 130.397s, episode steps: 571, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.013190, mean_q: 2.190987, mean_eps: 0.849083\n",
            " 153513/300000: episode: 223, duration: 179.722s, episode steps: 786, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013649, mean_q: 2.193911, mean_eps: 0.848412\n",
            " 153892/300000: episode: 224, duration: 86.822s, episode steps: 379, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013791, mean_q: 2.184727, mean_eps: 0.847835\n",
            " 155104/300000: episode: 225, duration: 277.381s, episode steps: 1212, steps per second:   4, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012886, mean_q: 2.193487, mean_eps: 0.847047\n",
            " 155750/300000: episode: 226, duration: 148.170s, episode steps: 646, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.013379, mean_q: 2.173508, mean_eps: 0.846128\n",
            " 156287/300000: episode: 227, duration: 123.580s, episode steps: 537, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.013365, mean_q: 2.177986, mean_eps: 0.845542\n",
            " 157097/300000: episode: 228, duration: 185.134s, episode steps: 810, steps per second:   4, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.012819, mean_q: 2.195852, mean_eps: 0.844875\n",
            " 157798/300000: episode: 229, duration: 160.477s, episode steps: 701, steps per second:   4, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.013320, mean_q: 2.180862, mean_eps: 0.844127\n",
            " 158940/300000: episode: 230, duration: 261.403s, episode steps: 1142, steps per second:   4, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013266, mean_q: 2.191556, mean_eps: 0.843215\n",
            " 159477/300000: episode: 231, duration: 122.958s, episode steps: 537, steps per second:   4, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.012655, mean_q: 2.175429, mean_eps: 0.842384\n",
            "Step 160000: saving model to checkpoints/dqn_weights_160000.h5f\n",
            " 160138/300000: episode: 232, duration: 160.852s, episode steps: 661, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013514, mean_q: 2.197962, mean_eps: 0.841791\n",
            " 161147/300000: episode: 233, duration: 231.818s, episode steps: 1009, steps per second:   4, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.014190, mean_q: 2.244241, mean_eps: 0.840964\n",
            " 161556/300000: episode: 234, duration: 94.007s, episode steps: 409, steps per second:   4, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013824, mean_q: 2.222212, mean_eps: 0.840263\n",
            " 162543/300000: episode: 235, duration: 227.976s, episode steps: 987, steps per second:   4, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014148, mean_q: 2.245799, mean_eps: 0.839571\n",
            " 163160/300000: episode: 236, duration: 142.265s, episode steps: 617, steps per second:   4, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.013658, mean_q: 2.242073, mean_eps: 0.838778\n",
            " 163948/300000: episode: 237, duration: 180.802s, episode steps: 788, steps per second:   4, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013183, mean_q: 2.239539, mean_eps: 0.838082\n",
            " 164698/300000: episode: 238, duration: 171.568s, episode steps: 750, steps per second:   4, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.013743, mean_q: 2.233332, mean_eps: 0.837321\n",
            " 165351/300000: episode: 239, duration: 154.728s, episode steps: 653, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.013344, mean_q: 2.249346, mean_eps: 0.836626\n",
            " 165718/300000: episode: 240, duration: 87.656s, episode steps: 367, steps per second:   4, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.014249, mean_q: 2.238182, mean_eps: 0.836121\n",
            " 166275/300000: episode: 241, duration: 127.872s, episode steps: 557, steps per second:   4, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013998, mean_q: 2.237327, mean_eps: 0.835664\n",
            " 166942/300000: episode: 242, duration: 153.466s, episode steps: 667, steps per second:   4, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013964, mean_q: 2.249140, mean_eps: 0.835058\n",
            " 167922/300000: episode: 243, duration: 225.414s, episode steps: 980, steps per second:   4, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013110, mean_q: 2.252967, mean_eps: 0.834243\n",
            " 168543/300000: episode: 244, duration: 142.448s, episode steps: 621, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013118, mean_q: 2.218317, mean_eps: 0.833450\n",
            " 169160/300000: episode: 245, duration: 141.471s, episode steps: 617, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012946, mean_q: 2.242216, mean_eps: 0.832838\n",
            " 169951/300000: episode: 246, duration: 181.109s, episode steps: 791, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.013352, mean_q: 2.254343, mean_eps: 0.832141\n",
            "Step 170000: saving model to checkpoints/dqn_weights_170000.h5f\n",
            " 170972/300000: episode: 247, duration: 234.946s, episode steps: 1021, steps per second:   4, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014808, mean_q: 2.314064, mean_eps: 0.831244\n",
            " 171349/300000: episode: 248, duration: 86.803s, episode steps: 377, steps per second:   4, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.014179, mean_q: 2.315376, mean_eps: 0.830552\n",
            " 171910/300000: episode: 249, duration: 129.251s, episode steps: 561, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014221, mean_q: 2.325250, mean_eps: 0.830087\n",
            " 172723/300000: episode: 250, duration: 187.072s, episode steps: 813, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.014545, mean_q: 2.320636, mean_eps: 0.829407\n",
            " 173672/300000: episode: 251, duration: 221.877s, episode steps: 949, steps per second:   4, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014315, mean_q: 2.329631, mean_eps: 0.828535\n",
            " 174156/300000: episode: 252, duration: 115.019s, episode steps: 484, steps per second:   4, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.014655, mean_q: 2.315771, mean_eps: 0.827826\n",
            " 174762/300000: episode: 253, duration: 139.363s, episode steps: 606, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.013927, mean_q: 2.318660, mean_eps: 0.827286\n",
            " 175731/300000: episode: 254, duration: 223.245s, episode steps: 969, steps per second:   4, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.014194, mean_q: 2.328333, mean_eps: 0.826506\n",
            " 176294/300000: episode: 255, duration: 129.585s, episode steps: 563, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013465, mean_q: 2.326165, mean_eps: 0.825748\n",
            " 177174/300000: episode: 256, duration: 202.222s, episode steps: 880, steps per second:   4, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.013048, mean_q: 2.314639, mean_eps: 0.825034\n",
            " 177733/300000: episode: 257, duration: 128.470s, episode steps: 559, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012923, mean_q: 2.305654, mean_eps: 0.824322\n",
            " 178358/300000: episode: 258, duration: 143.260s, episode steps: 625, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.013099, mean_q: 2.330800, mean_eps: 0.823735\n",
            " 179195/300000: episode: 259, duration: 192.251s, episode steps: 837, steps per second:   4, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.014126, mean_q: 2.325247, mean_eps: 0.823012\n",
            " 179738/300000: episode: 260, duration: 125.323s, episode steps: 543, steps per second:   4, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.014372, mean_q: 2.329937, mean_eps: 0.822329\n",
            "Step 180000: saving model to checkpoints/dqn_weights_180000.h5f\n",
            " 180133/300000: episode: 261, duration: 91.258s, episode steps: 395, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.013734, mean_q: 2.342324, mean_eps: 0.821864\n",
            " 180932/300000: episode: 262, duration: 184.146s, episode steps: 799, steps per second:   4, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.015549, mean_q: 2.407301, mean_eps: 0.821273\n",
            " 181764/300000: episode: 263, duration: 191.799s, episode steps: 832, steps per second:   4, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.014917, mean_q: 2.417709, mean_eps: 0.820466\n",
            " 182720/300000: episode: 264, duration: 220.121s, episode steps: 956, steps per second:   4, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.015228, mean_q: 2.400822, mean_eps: 0.819581\n",
            " 183592/300000: episode: 265, duration: 200.600s, episode steps: 872, steps per second:   4, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.014866, mean_q: 2.413950, mean_eps: 0.818676\n",
            " 184109/300000: episode: 266, duration: 116.388s, episode steps: 517, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.013773, mean_q: 2.409620, mean_eps: 0.817989\n",
            " 184645/300000: episode: 267, duration: 119.798s, episode steps: 536, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.014888, mean_q: 2.412467, mean_eps: 0.817467\n",
            " 185225/300000: episode: 268, duration: 133.307s, episode steps: 580, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014182, mean_q: 2.405731, mean_eps: 0.816915\n",
            " 186580/300000: episode: 269, duration: 311.185s, episode steps: 1355, steps per second:   4, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014295, mean_q: 2.410031, mean_eps: 0.815957\n",
            " 187262/300000: episode: 270, duration: 157.175s, episode steps: 682, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013749, mean_q: 2.408210, mean_eps: 0.814949\n",
            " 188434/300000: episode: 271, duration: 268.406s, episode steps: 1172, steps per second:   4, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.014470, mean_q: 2.404480, mean_eps: 0.814031\n",
            " 189234/300000: episode: 272, duration: 183.709s, episode steps: 800, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.013967, mean_q: 2.399782, mean_eps: 0.813055\n",
            " 189798/300000: episode: 273, duration: 129.650s, episode steps: 564, steps per second:   4, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.013707, mean_q: 2.384874, mean_eps: 0.812380\n",
            "Step 190000: saving model to checkpoints/dqn_weights_190000.h5f\n",
            " 191142/300000: episode: 274, duration: 309.793s, episode steps: 1344, steps per second:   4, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.015054, mean_q: 2.494435, mean_eps: 0.811435\n",
            " 191889/300000: episode: 275, duration: 172.070s, episode steps: 747, steps per second:   4, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.016750, mean_q: 2.503088, mean_eps: 0.810400\n",
            " 192741/300000: episode: 276, duration: 196.145s, episode steps: 852, steps per second:   4, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.015506, mean_q: 2.504401, mean_eps: 0.809609\n",
            " 193369/300000: episode: 277, duration: 144.656s, episode steps: 628, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015451, mean_q: 2.497174, mean_eps: 0.808876\n",
            " 194007/300000: episode: 278, duration: 147.078s, episode steps: 638, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.014787, mean_q: 2.507380, mean_eps: 0.808249\n",
            " 194523/300000: episode: 279, duration: 119.197s, episode steps: 516, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014177, mean_q: 2.495403, mean_eps: 0.807678\n",
            " 195313/300000: episode: 280, duration: 182.006s, episode steps: 790, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.015516, mean_q: 2.509485, mean_eps: 0.807032\n",
            " 195850/300000: episode: 281, duration: 124.026s, episode steps: 537, steps per second:   4, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.014378, mean_q: 2.506323, mean_eps: 0.806375\n",
            " 196384/300000: episode: 282, duration: 123.738s, episode steps: 534, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.015498, mean_q: 2.517754, mean_eps: 0.805845\n",
            " 196802/300000: episode: 283, duration: 97.040s, episode steps: 418, steps per second:   4, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.015252, mean_q: 2.501587, mean_eps: 0.805373\n",
            " 197755/300000: episode: 284, duration: 219.977s, episode steps: 953, steps per second:   4, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.014796, mean_q: 2.504608, mean_eps: 0.804695\n",
            " 198237/300000: episode: 285, duration: 111.567s, episode steps: 482, steps per second:   4, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015070, mean_q: 2.501880, mean_eps: 0.803984\n",
            " 199012/300000: episode: 286, duration: 179.226s, episode steps: 775, steps per second:   4, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.014379, mean_q: 2.502402, mean_eps: 0.803362\n",
            " 199842/300000: episode: 287, duration: 191.447s, episode steps: 830, steps per second:   4, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014534, mean_q: 2.493333, mean_eps: 0.802568\n",
            "Step 200000: saving model to checkpoints/dqn_weights_200000.h5f\n",
            " 200452/300000: episode: 288, duration: 141.283s, episode steps: 610, steps per second:   4, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.016698, mean_q: 2.615094, mean_eps: 0.801855\n",
            " 201490/300000: episode: 289, duration: 239.554s, episode steps: 1038, steps per second:   4, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.016748, mean_q: 2.639022, mean_eps: 0.801039\n",
            " 202190/300000: episode: 290, duration: 165.898s, episode steps: 700, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.016517, mean_q: 2.634180, mean_eps: 0.800179\n",
            " 202852/300000: episode: 291, duration: 152.885s, episode steps: 662, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.016928, mean_q: 2.641545, mean_eps: 0.799505\n",
            " 203331/300000: episode: 292, duration: 110.938s, episode steps: 479, steps per second:   4, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.015902, mean_q: 2.635555, mean_eps: 0.798940\n",
            " 203727/300000: episode: 293, duration: 91.251s, episode steps: 396, steps per second:   4, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.016652, mean_q: 2.652700, mean_eps: 0.798507\n",
            " 204169/300000: episode: 294, duration: 102.179s, episode steps: 442, steps per second:   4, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.016818, mean_q: 2.636882, mean_eps: 0.798092\n",
            " 204952/300000: episode: 295, duration: 180.403s, episode steps: 783, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.016120, mean_q: 2.644939, mean_eps: 0.797486\n",
            " 205995/300000: episode: 296, duration: 240.694s, episode steps: 1043, steps per second:   4, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.015858, mean_q: 2.639799, mean_eps: 0.796582\n",
            " 206653/300000: episode: 297, duration: 152.178s, episode steps: 658, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015744, mean_q: 2.637933, mean_eps: 0.795740\n",
            " 207590/300000: episode: 298, duration: 219.071s, episode steps: 937, steps per second:   4, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.016048, mean_q: 2.639834, mean_eps: 0.794950\n",
            " 207974/300000: episode: 299, duration: 89.122s, episode steps: 384, steps per second:   4, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.015281, mean_q: 2.621811, mean_eps: 0.794296\n",
            " 208555/300000: episode: 300, duration: 134.599s, episode steps: 581, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.015523, mean_q: 2.645735, mean_eps: 0.793819\n",
            " 209772/300000: episode: 301, duration: 280.823s, episode steps: 1217, steps per second:   4, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.016338, mean_q: 2.638584, mean_eps: 0.792929\n",
            "Step 210000: saving model to checkpoints/dqn_weights_210000.h5f\n",
            " 210447/300000: episode: 302, duration: 157.331s, episode steps: 675, steps per second:   4, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.016487, mean_q: 2.666797, mean_eps: 0.791992\n",
            " 210931/300000: episode: 303, duration: 112.265s, episode steps: 484, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.015671, mean_q: 2.685517, mean_eps: 0.791418\n",
            " 211659/300000: episode: 304, duration: 168.617s, episode steps: 728, steps per second:   4, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.017179, mean_q: 2.677884, mean_eps: 0.790818\n",
            " 212266/300000: episode: 305, duration: 140.673s, episode steps: 607, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.015507, mean_q: 2.674130, mean_eps: 0.790158\n",
            " 212656/300000: episode: 306, duration: 90.361s, episode steps: 390, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.016635, mean_q: 2.682162, mean_eps: 0.789664\n",
            " 213399/300000: episode: 307, duration: 171.859s, episode steps: 743, steps per second:   4, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.016584, mean_q: 2.689848, mean_eps: 0.789103\n",
            " 214335/300000: episode: 308, duration: 216.699s, episode steps: 936, steps per second:   4, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016922, mean_q: 2.686251, mean_eps: 0.788272\n",
            " 214914/300000: episode: 309, duration: 134.159s, episode steps: 579, steps per second:   4, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.017530, mean_q: 2.707034, mean_eps: 0.787522\n",
            " 215951/300000: episode: 310, duration: 275.092s, episode steps: 1037, steps per second:   4, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.016671, mean_q: 2.685764, mean_eps: 0.786722\n",
            " 217247/300000: episode: 311, duration: 311.998s, episode steps: 1296, steps per second:   4, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015812, mean_q: 2.683653, mean_eps: 0.785567\n",
            " 217746/300000: episode: 312, duration: 119.567s, episode steps: 499, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.016218, mean_q: 2.694397, mean_eps: 0.784679\n",
            " 218121/300000: episode: 313, duration: 89.461s, episode steps: 375, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.015786, mean_q: 2.688492, mean_eps: 0.784246\n",
            " 218783/300000: episode: 314, duration: 158.538s, episode steps: 662, steps per second:   4, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.015509, mean_q: 2.693887, mean_eps: 0.783733\n",
            " 219412/300000: episode: 315, duration: 147.711s, episode steps: 629, steps per second:   4, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.015801, mean_q: 2.692594, mean_eps: 0.783094\n",
            " 219945/300000: episode: 316, duration: 124.941s, episode steps: 533, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.015629, mean_q: 2.677624, mean_eps: 0.782519\n",
            "Step 220000: saving model to checkpoints/dqn_weights_220000.h5f\n",
            " 220783/300000: episode: 317, duration: 197.270s, episode steps: 838, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.017108, mean_q: 2.730346, mean_eps: 0.781840\n",
            " 221505/300000: episode: 318, duration: 168.300s, episode steps: 722, steps per second:   4, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.018022, mean_q: 2.751281, mean_eps: 0.781068\n",
            " 222690/300000: episode: 319, duration: 275.636s, episode steps: 1185, steps per second:   4, episode reward: 10.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017810, mean_q: 2.723997, mean_eps: 0.780124\n",
            " 223172/300000: episode: 320, duration: 113.036s, episode steps: 482, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.016381, mean_q: 2.739643, mean_eps: 0.779299\n",
            " 224305/300000: episode: 321, duration: 275.562s, episode steps: 1133, steps per second:   4, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.017349, mean_q: 2.730327, mean_eps: 0.778499\n",
            " 224699/300000: episode: 322, duration: 108.143s, episode steps: 394, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.017043, mean_q: 2.747969, mean_eps: 0.777744\n",
            " 225405/300000: episode: 323, duration: 177.027s, episode steps: 706, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.016671, mean_q: 2.724601, mean_eps: 0.777199\n",
            " 225814/300000: episode: 324, duration: 95.465s, episode steps: 409, steps per second:   4, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.016780, mean_q: 2.736379, mean_eps: 0.776647\n",
            " 226350/300000: episode: 325, duration: 124.412s, episode steps: 536, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.016006, mean_q: 2.727713, mean_eps: 0.776179\n",
            " 226761/300000: episode: 326, duration: 96.182s, episode steps: 411, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.018162, mean_q: 2.748572, mean_eps: 0.775711\n",
            " 227414/300000: episode: 327, duration: 151.976s, episode steps: 653, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.016506, mean_q: 2.741734, mean_eps: 0.775184\n",
            " 228106/300000: episode: 328, duration: 161.235s, episode steps: 692, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017311, mean_q: 2.717208, mean_eps: 0.774518\n",
            " 228453/300000: episode: 329, duration: 80.840s, episode steps: 347, steps per second:   4, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.015367, mean_q: 2.731618, mean_eps: 0.774004\n",
            " 229266/300000: episode: 330, duration: 189.257s, episode steps: 813, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.016407, mean_q: 2.736336, mean_eps: 0.773430\n",
            " 229848/300000: episode: 331, duration: 135.705s, episode steps: 582, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.016662, mean_q: 2.737910, mean_eps: 0.772739\n",
            "Step 230000: saving model to checkpoints/dqn_weights_230000.h5f\n",
            " 230777/300000: episode: 332, duration: 217.386s, episode steps: 929, steps per second:   4, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.018011, mean_q: 2.786814, mean_eps: 0.771991\n",
            " 231444/300000: episode: 333, duration: 155.262s, episode steps: 667, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.019657, mean_q: 2.803595, mean_eps: 0.771201\n",
            " 231875/300000: episode: 334, duration: 100.576s, episode steps: 431, steps per second:   4, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.018365, mean_q: 2.810526, mean_eps: 0.770658\n",
            " 232343/300000: episode: 335, duration: 109.847s, episode steps: 468, steps per second:   4, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.018064, mean_q: 2.791365, mean_eps: 0.770213\n",
            " 233100/300000: episode: 336, duration: 176.289s, episode steps: 757, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.017535, mean_q: 2.792805, mean_eps: 0.769606\n",
            " 233790/300000: episode: 337, duration: 160.616s, episode steps: 690, steps per second:   4, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017931, mean_q: 2.809360, mean_eps: 0.768890\n",
            " 234163/300000: episode: 338, duration: 87.197s, episode steps: 373, steps per second:   4, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.017801, mean_q: 2.813373, mean_eps: 0.768364\n",
            " 234718/300000: episode: 339, duration: 129.689s, episode steps: 555, steps per second:   4, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.017902, mean_q: 2.801095, mean_eps: 0.767904\n",
            " 235608/300000: episode: 340, duration: 207.807s, episode steps: 890, steps per second:   4, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.017744, mean_q: 2.798468, mean_eps: 0.767189\n",
            " 235990/300000: episode: 341, duration: 89.188s, episode steps: 382, steps per second:   4, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.018136, mean_q: 2.791827, mean_eps: 0.766559\n",
            " 236591/300000: episode: 342, duration: 140.111s, episode steps: 601, steps per second:   4, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.018615, mean_q: 2.801101, mean_eps: 0.766073\n",
            " 237190/300000: episode: 343, duration: 139.648s, episode steps: 599, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.018244, mean_q: 2.807063, mean_eps: 0.765479\n",
            " 238006/300000: episode: 344, duration: 190.298s, episode steps: 816, steps per second:   4, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.018189, mean_q: 2.805633, mean_eps: 0.764778\n",
            " 238812/300000: episode: 345, duration: 188.084s, episode steps: 806, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.016874, mean_q: 2.809440, mean_eps: 0.763976\n",
            " 239511/300000: episode: 346, duration: 163.141s, episode steps: 699, steps per second:   4, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.017040, mean_q: 2.798010, mean_eps: 0.763231\n",
            "Step 240000: saving model to checkpoints/dqn_weights_240000.h5f\n",
            " 240021/300000: episode: 347, duration: 119.225s, episode steps: 510, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.017751, mean_q: 2.794603, mean_eps: 0.762632\n",
            " 240586/300000: episode: 348, duration: 131.679s, episode steps: 565, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.017244, mean_q: 2.760568, mean_eps: 0.762100\n",
            " 241306/300000: episode: 349, duration: 167.689s, episode steps: 720, steps per second:   4, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.017538, mean_q: 2.745104, mean_eps: 0.761464\n",
            " 241997/300000: episode: 350, duration: 160.942s, episode steps: 691, steps per second:   4, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.017493, mean_q: 2.740236, mean_eps: 0.760766\n",
            " 242406/300000: episode: 351, duration: 95.403s, episode steps: 409, steps per second:   4, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.016721, mean_q: 2.753106, mean_eps: 0.760221\n",
            " 243522/300000: episode: 352, duration: 260.086s, episode steps: 1116, steps per second:   4, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.018298, mean_q: 2.751631, mean_eps: 0.759466\n",
            " 244194/300000: episode: 353, duration: 156.596s, episode steps: 672, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.018170, mean_q: 2.767671, mean_eps: 0.758581\n",
            " 244842/300000: episode: 354, duration: 151.185s, episode steps: 648, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.017324, mean_q: 2.746591, mean_eps: 0.757928\n",
            " 245231/300000: episode: 355, duration: 90.868s, episode steps: 389, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.016052, mean_q: 2.740513, mean_eps: 0.757414\n",
            " 245728/300000: episode: 356, duration: 115.929s, episode steps: 497, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.016696, mean_q: 2.758489, mean_eps: 0.756976\n",
            "done, took 56416.629 seconds\n",
            "⏱ Tiempo de fin: 10:01:26\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"⏱ Tiempo de inicio: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "dqn.fit(env, nb_steps=300000, visualize=False, verbose=2,callbacks=[checkpoint_callback])\n",
        "\n",
        "print(f\"⏱ Tiempo de fin: {datetime.now().strftime('%H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae8n7eplpE_u",
        "outputId": "1bd0bbb3-113b-4394-f0fc-f817f0136e4f"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[WARNING] dqn_04_weights.h5f.index already exists - overwrite? [y/n] y\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TIP] Next time specify overwrite=True!\n"
          ]
        }
      ],
      "source": [
        "dqn.save_weights('dqn_04_weights.h5f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSVcMgOsR0Nv",
        "outputId": "5208ce9f-fe6e-41eb-af05-f797d4db9370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 13.000, steps: 697\n",
            "Episode 2: reward: 15.000, steps: 912\n",
            "Episode 3: reward: 11.000, steps: 669\n",
            "Episode 4: reward: 13.000, steps: 671\n",
            "Episode 5: reward: 4.000, steps: 429\n",
            "Episode 6: reward: 15.000, steps: 940\n",
            "Episode 7: reward: 22.000, steps: 920\n",
            "Episode 8: reward: 18.000, steps: 861\n",
            "Episode 9: reward: 12.000, steps: 907\n",
            "Episode 10: reward: 17.000, steps: 1029\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x27afbcfd850>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_filename = 'dqn_04_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenamiento parte 2"
      ],
      "metadata": {
        "id": "aVZc0X-Igxos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente bloque se implementa la continuación del entrenamiento del agente DQN a partir de un modelo previamente entrenado, lo que permite acumular más pasos sin reiniciar desde cero. Se cargan los pesos guardados del checkpoint correspondiente a los 75,000 pasos de entrenamiento y se establece cesta marca como el nuevo punto de partida. Para de esta manera continuar el entrenamiento hasta completar un total de 200,000 pasos planificados en esta fase.\n",
        "\n",
        "Esta estrategia fue definida debido a las limitaciones identificadas empleando el entorno gratuito de Colab para hacer un entrenamiento incremental y controlado. Ya que requeríamos detener y reanudar los procesos por la limitante de 12 horas de ambiente gratis."
      ],
      "metadata": {
        "id": "LQuFHFApq_Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 + 100000+75000+100000 = 800000\n",
        "\n",
        "dqn.load_weights(\"dqn_weights_75000.h5f\")\n",
        "# 🔢 Paso de partida 3\n",
        "starting_step = 75000 #ultimo weight actualizado\n",
        "remaining_steps = 200000 - starting_step\n",
        "\n",
        "\n",
        "# 🧩 Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# 🚀 Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htRNej0vy8_Q",
        "outputId": "2b88d6aa-12ef-4d1a-d3b9-03ffb424f0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 125000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    634/125000: episode: 1, duration: 4.627s, episode steps: 634, steps per second: 137, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1406/125000: episode: 2, duration: 5.157s, episode steps: 772, steps per second: 150, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1807/125000: episode: 3, duration: 1.898s, episode steps: 401, steps per second: 211, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2589/125000: episode: 4, duration: 3.808s, episode steps: 782, steps per second: 205, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3305/125000: episode: 5, duration: 5.100s, episode steps: 716, steps per second: 140, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3784/125000: episode: 6, duration: 2.415s, episode steps: 479, steps per second: 198, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4298/125000: episode: 7, duration: 2.923s, episode steps: 514, steps per second: 176, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4667/125000: episode: 8, duration: 2.109s, episode steps: 369, steps per second: 175, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5574/125000: episode: 9, duration: 5.990s, episode steps: 907, steps per second: 151, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   6177/125000: episode: 10, duration: 3.165s, episode steps: 603, steps per second: 191, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   6684/125000: episode: 11, duration: 2.724s, episode steps: 507, steps per second: 186, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   7197/125000: episode: 12, duration: 2.659s, episode steps: 513, steps per second: 193, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8081/125000: episode: 13, duration: 5.898s, episode steps: 884, steps per second: 150, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8605/125000: episode: 14, duration: 2.764s, episode steps: 524, steps per second: 190, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   9363/125000: episode: 15, duration: 4.168s, episode steps: 758, steps per second: 182, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  10247/125000: episode: 16, duration: 6.038s, episode steps: 884, steps per second: 146, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  11119/125000: episode: 17, duration: 4.669s, episode steps: 872, steps per second: 187, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  11642/125000: episode: 18, duration: 2.564s, episode steps: 523, steps per second: 204, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  12037/125000: episode: 19, duration: 2.095s, episode steps: 395, steps per second: 189, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  12704/125000: episode: 20, duration: 4.974s, episode steps: 667, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  13694/125000: episode: 21, duration: 5.113s, episode steps: 990, steps per second: 194, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  14360/125000: episode: 22, duration: 3.719s, episode steps: 666, steps per second: 179, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  14805/125000: episode: 23, duration: 3.403s, episode steps: 445, steps per second: 131, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  15864/125000: episode: 24, duration: 7.916s, episode steps: 1059, steps per second: 134, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  16365/125000: episode: 25, duration: 2.612s, episode steps: 501, steps per second: 192, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  16888/125000: episode: 26, duration: 3.116s, episode steps: 523, steps per second: 168, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  17667/125000: episode: 27, duration: 5.018s, episode steps: 779, steps per second: 155, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  18695/125000: episode: 28, duration: 5.049s, episode steps: 1028, steps per second: 204, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  19242/125000: episode: 29, duration: 2.801s, episode steps: 547, steps per second: 195, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  19735/125000: episode: 30, duration: 3.994s, episode steps: 493, steps per second: 123, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20135/125000: episode: 31, duration: 2.067s, episode steps: 400, steps per second: 194, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20796/125000: episode: 32, duration: 3.323s, episode steps: 661, steps per second: 199, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  21343/125000: episode: 33, duration: 2.694s, episode steps: 547, steps per second: 203, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  22167/125000: episode: 34, duration: 5.674s, episode steps: 824, steps per second: 145, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23243/125000: episode: 35, duration: 5.464s, episode steps: 1076, steps per second: 197, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23644/125000: episode: 36, duration: 1.976s, episode steps: 401, steps per second: 203, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24133/125000: episode: 37, duration: 2.597s, episode steps: 489, steps per second: 188, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24641/125000: episode: 38, duration: 4.115s, episode steps: 508, steps per second: 123, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  25134/125000: episode: 39, duration: 3.046s, episode steps: 493, steps per second: 162, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  25622/125000: episode: 40, duration: 2.386s, episode steps: 488, steps per second: 205, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26286/125000: episode: 41, duration: 3.416s, episode steps: 664, steps per second: 194, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26999/125000: episode: 42, duration: 5.210s, episode steps: 713, steps per second: 137, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  27646/125000: episode: 43, duration: 3.364s, episode steps: 647, steps per second: 192, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  28299/125000: episode: 44, duration: 3.311s, episode steps: 653, steps per second: 197, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  28711/125000: episode: 45, duration: 1.969s, episode steps: 412, steps per second: 209, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  29208/125000: episode: 46, duration: 3.506s, episode steps: 497, steps per second: 142, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30251/125000: episode: 47, duration: 5.798s, episode steps: 1043, steps per second: 180, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30629/125000: episode: 48, duration: 1.758s, episode steps: 378, steps per second: 215, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31068/125000: episode: 49, duration: 2.198s, episode steps: 439, steps per second: 200, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31789/125000: episode: 50, duration: 4.698s, episode steps: 721, steps per second: 153, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  32222/125000: episode: 51, duration: 2.572s, episode steps: 433, steps per second: 168, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  32802/125000: episode: 52, duration: 2.846s, episode steps: 580, steps per second: 204, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  33315/125000: episode: 53, duration: 2.393s, episode steps: 513, steps per second: 214, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34026/125000: episode: 54, duration: 4.018s, episode steps: 711, steps per second: 177, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34747/125000: episode: 55, duration: 4.688s, episode steps: 721, steps per second: 154, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  35234/125000: episode: 56, duration: 2.512s, episode steps: 487, steps per second: 194, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36016/125000: episode: 57, duration: 4.012s, episode steps: 782, steps per second: 195, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36654/125000: episode: 58, duration: 4.246s, episode steps: 638, steps per second: 150, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  37171/125000: episode: 59, duration: 3.193s, episode steps: 517, steps per second: 162, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  37716/125000: episode: 60, duration: 2.633s, episode steps: 545, steps per second: 207, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  38454/125000: episode: 61, duration: 3.578s, episode steps: 738, steps per second: 206, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  39708/125000: episode: 62, duration: 7.918s, episode steps: 1254, steps per second: 158, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40080/125000: episode: 63, duration: 1.894s, episode steps: 372, steps per second: 196, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40474/125000: episode: 64, duration: 1.931s, episode steps: 394, steps per second: 204, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40824/125000: episode: 65, duration: 1.790s, episode steps: 350, steps per second: 196, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41347/125000: episode: 66, duration: 2.928s, episode steps: 523, steps per second: 179, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41851/125000: episode: 67, duration: 3.871s, episode steps: 504, steps per second: 130, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  42985/125000: episode: 68, duration: 5.588s, episode steps: 1134, steps per second: 203, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44189/125000: episode: 69, duration: 7.555s, episode steps: 1204, steps per second: 159, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44966/125000: episode: 70, duration: 4.052s, episode steps: 777, steps per second: 192, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  45711/125000: episode: 71, duration: 4.348s, episode steps: 745, steps per second: 171, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  46397/125000: episode: 72, duration: 6.319s, episode steps: 686, steps per second: 109, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  47448/125000: episode: 73, duration: 5.592s, episode steps: 1051, steps per second: 188, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48091/125000: episode: 74, duration: 3.368s, episode steps: 643, steps per second: 191, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48450/125000: episode: 75, duration: 2.274s, episode steps: 359, steps per second: 158, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48970/125000: episode: 76, duration: 3.856s, episode steps: 520, steps per second: 135, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  49364/125000: episode: 77, duration: 1.980s, episode steps: 394, steps per second: 199, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  49844/125000: episode: 78, duration: 2.594s, episode steps: 480, steps per second: 185, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  50326/125000: episode: 79, duration: 81.127s, episode steps: 482, steps per second:   6, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.011988, mae: 0.480977, mean_q: 0.591011, mean_eps: 0.774266\n",
            "  50836/125000: episode: 80, duration: 122.197s, episode steps: 510, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011484, mae: 0.479145, mean_q: 0.585658, mean_eps: 0.772388\n",
            "  51476/125000: episode: 81, duration: 156.931s, episode steps: 640, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010772, mae: 0.475709, mean_q: 0.580060, mean_eps: 0.769800\n",
            "  51971/125000: episode: 82, duration: 119.482s, episode steps: 495, steps per second:   4, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.009741, mae: 0.476042, mean_q: 0.580918, mean_eps: 0.767246\n",
            "  52571/125000: episode: 83, duration: 143.439s, episode steps: 600, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.009943, mae: 0.471340, mean_q: 0.574531, mean_eps: 0.764783\n",
            "  53172/125000: episode: 84, duration: 142.251s, episode steps: 601, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009456, mae: 0.472407, mean_q: 0.578133, mean_eps: 0.762081\n",
            "  53522/125000: episode: 85, duration: 84.129s, episode steps: 350, steps per second:   4, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.009044, mae: 0.472888, mean_q: 0.577837, mean_eps: 0.759941\n",
            "  54416/125000: episode: 86, duration: 210.955s, episode steps: 894, steps per second:   4, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.009258, mae: 0.472055, mean_q: 0.577535, mean_eps: 0.757142\n",
            "  55067/125000: episode: 87, duration: 153.907s, episode steps: 651, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008847, mae: 0.470176, mean_q: 0.576097, mean_eps: 0.753665\n",
            "  56299/125000: episode: 88, duration: 298.046s, episode steps: 1232, steps per second:   4, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007948, mae: 0.467888, mean_q: 0.573750, mean_eps: 0.749429\n",
            "  57138/125000: episode: 89, duration: 203.244s, episode steps: 839, steps per second:   4, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.007476, mae: 0.471152, mean_q: 0.579060, mean_eps: 0.744769\n",
            "  57750/125000: episode: 90, duration: 148.132s, episode steps: 612, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.007409, mae: 0.469741, mean_q: 0.578037, mean_eps: 0.741504\n",
            "  58268/125000: episode: 91, duration: 121.757s, episode steps: 518, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.006943, mae: 0.467585, mean_q: 0.574831, mean_eps: 0.738962\n",
            "  58731/125000: episode: 92, duration: 110.882s, episode steps: 463, steps per second:   4, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.006642, mae: 0.465818, mean_q: 0.572870, mean_eps: 0.736754\n",
            "  59118/125000: episode: 93, duration: 92.218s, episode steps: 387, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.006418, mae: 0.463832, mean_q: 0.571791, mean_eps: 0.734842\n",
            "  59959/125000: episode: 94, duration: 200.285s, episode steps: 841, steps per second:   4, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006708, mae: 0.462724, mean_q: 0.570988, mean_eps: 0.732079\n",
            "  60483/125000: episode: 95, duration: 123.408s, episode steps: 524, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.008201, mae: 0.490624, mean_q: 0.604158, mean_eps: 0.729008\n",
            "  61471/125000: episode: 96, duration: 235.448s, episode steps: 988, steps per second:   4, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.007944, mae: 0.493279, mean_q: 0.607539, mean_eps: 0.725606\n",
            "  61988/125000: episode: 97, duration: 123.289s, episode steps: 517, steps per second:   4, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.007317, mae: 0.496141, mean_q: 0.611961, mean_eps: 0.722219\n",
            "  62323/125000: episode: 98, duration: 79.244s, episode steps: 335, steps per second:   4, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.007284, mae: 0.495897, mean_q: 0.612368, mean_eps: 0.720302\n",
            "  62968/125000: episode: 99, duration: 153.939s, episode steps: 645, steps per second:   4, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.006773, mae: 0.491007, mean_q: 0.606372, mean_eps: 0.718098\n",
            "  63567/125000: episode: 100, duration: 141.227s, episode steps: 599, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.006545, mae: 0.490937, mean_q: 0.606002, mean_eps: 0.715299\n",
            "  64224/125000: episode: 101, duration: 155.688s, episode steps: 657, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.006263, mae: 0.491224, mean_q: 0.605351, mean_eps: 0.712472\n",
            "  64737/125000: episode: 102, duration: 124.993s, episode steps: 513, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.006309, mae: 0.495133, mean_q: 0.611349, mean_eps: 0.709840\n",
            "  65304/125000: episode: 103, duration: 133.926s, episode steps: 567, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006387, mae: 0.489166, mean_q: 0.603210, mean_eps: 0.707410\n",
            "  66120/125000: episode: 104, duration: 196.143s, episode steps: 816, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.005739, mae: 0.489083, mean_q: 0.604608, mean_eps: 0.704298\n",
            "  66680/125000: episode: 105, duration: 134.394s, episode steps: 560, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.006092, mae: 0.490236, mean_q: 0.605086, mean_eps: 0.701202\n",
            "  67203/125000: episode: 106, duration: 125.859s, episode steps: 523, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.006080, mae: 0.494678, mean_q: 0.611210, mean_eps: 0.698766\n",
            "  67864/125000: episode: 107, duration: 161.214s, episode steps: 661, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.006133, mae: 0.487727, mean_q: 0.602894, mean_eps: 0.696101\n",
            "  68607/125000: episode: 108, duration: 184.274s, episode steps: 743, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.005735, mae: 0.489630, mean_q: 0.605837, mean_eps: 0.692943\n",
            "  69514/125000: episode: 109, duration: 223.947s, episode steps: 907, steps per second:   4, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.005835, mae: 0.487494, mean_q: 0.602982, mean_eps: 0.689230\n",
            "  70025/125000: episode: 110, duration: 125.750s, episode steps: 511, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.005731, mae: 0.488064, mean_q: 0.604163, mean_eps: 0.686039\n",
            "  71091/125000: episode: 111, duration: 264.181s, episode steps: 1066, steps per second:   4, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.007558, mae: 0.521328, mean_q: 0.643369, mean_eps: 0.682491\n",
            "  71837/125000: episode: 112, duration: 182.904s, episode steps: 746, steps per second:   4, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.006518, mae: 0.519342, mean_q: 0.640099, mean_eps: 0.678414\n",
            "  72270/125000: episode: 113, duration: 103.632s, episode steps: 433, steps per second:   4, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.007048, mae: 0.524887, mean_q: 0.646578, mean_eps: 0.675762\n",
            "  72933/125000: episode: 114, duration: 160.426s, episode steps: 663, steps per second:   4, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.006731, mae: 0.519790, mean_q: 0.642088, mean_eps: 0.673296\n",
            "  73323/125000: episode: 115, duration: 93.591s, episode steps: 390, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.006362, mae: 0.519267, mean_q: 0.640569, mean_eps: 0.670926\n",
            "  74267/125000: episode: 116, duration: 228.242s, episode steps: 944, steps per second:   4, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.006425, mae: 0.518701, mean_q: 0.639885, mean_eps: 0.667925\n",
            "  74819/125000: episode: 117, duration: 136.507s, episode steps: 552, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.006325, mae: 0.519381, mean_q: 0.641006, mean_eps: 0.664559\n",
            "  75209/125000: episode: 118, duration: 95.982s, episode steps: 390, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.005928, mae: 0.514596, mean_q: 0.636071, mean_eps: 0.662439\n",
            "  76073/125000: episode: 119, duration: 212.981s, episode steps: 864, steps per second:   4, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.006001, mae: 0.522086, mean_q: 0.644906, mean_eps: 0.659618\n",
            "  76424/125000: episode: 120, duration: 84.661s, episode steps: 351, steps per second:   4, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.006246, mae: 0.521851, mean_q: 0.644763, mean_eps: 0.656884\n",
            "  77160/125000: episode: 121, duration: 182.030s, episode steps: 736, steps per second:   4, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.005710, mae: 0.519249, mean_q: 0.641293, mean_eps: 0.654438\n",
            "  77796/125000: episode: 122, duration: 156.454s, episode steps: 636, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.005322, mae: 0.521345, mean_q: 0.645274, mean_eps: 0.651351\n",
            "  78172/125000: episode: 123, duration: 89.706s, episode steps: 376, steps per second:   4, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.005320, mae: 0.522984, mean_q: 0.646439, mean_eps: 0.649074\n",
            "  79336/125000: episode: 124, duration: 282.285s, episode steps: 1164, steps per second:   4, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.005783, mae: 0.525156, mean_q: 0.648266, mean_eps: 0.645609\n",
            "  80317/125000: episode: 125, duration: 238.947s, episode steps: 981, steps per second:   4, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.006021, mae: 0.527036, mean_q: 0.650446, mean_eps: 0.640783\n",
            "  81148/125000: episode: 126, duration: 203.600s, episode steps: 831, steps per second:   4, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.007346, mae: 0.551695, mean_q: 0.680611, mean_eps: 0.636706\n",
            "  82026/125000: episode: 127, duration: 212.264s, episode steps: 878, steps per second:   4, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006772, mae: 0.546298, mean_q: 0.673571, mean_eps: 0.632861\n",
            "  83040/125000: episode: 128, duration: 241.188s, episode steps: 1014, steps per second:   4, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.006473, mae: 0.548580, mean_q: 0.675958, mean_eps: 0.628604\n",
            "  83465/125000: episode: 129, duration: 96.814s, episode steps: 425, steps per second:   4, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.006174, mae: 0.549155, mean_q: 0.676607, mean_eps: 0.625366\n",
            "  84209/125000: episode: 130, duration: 168.770s, episode steps: 744, steps per second:   4, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.006215, mae: 0.545550, mean_q: 0.672346, mean_eps: 0.622736\n",
            "  84667/125000: episode: 131, duration: 106.266s, episode steps: 458, steps per second:   4, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.006375, mae: 0.550588, mean_q: 0.678593, mean_eps: 0.620031\n",
            "  85249/125000: episode: 132, duration: 134.883s, episode steps: 582, steps per second:   4, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.006289, mae: 0.546209, mean_q: 0.673647, mean_eps: 0.617691\n",
            "  85792/125000: episode: 133, duration: 124.167s, episode steps: 543, steps per second:   4, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.006537, mae: 0.548321, mean_q: 0.675473, mean_eps: 0.615160\n",
            "  86306/125000: episode: 134, duration: 118.418s, episode steps: 514, steps per second:   4, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.006075, mae: 0.546155, mean_q: 0.675178, mean_eps: 0.612782\n",
            "  87428/125000: episode: 135, duration: 254.715s, episode steps: 1122, steps per second:   4, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.006290, mae: 0.550166, mean_q: 0.679400, mean_eps: 0.609101\n",
            "  88261/125000: episode: 136, duration: 188.514s, episode steps: 833, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.005719, mae: 0.546940, mean_q: 0.674266, mean_eps: 0.604702\n",
            "  89138/125000: episode: 137, duration: 200.432s, episode steps: 877, steps per second:   4, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.005840, mae: 0.542837, mean_q: 0.669812, mean_eps: 0.600854\n",
            "  89632/125000: episode: 138, duration: 112.760s, episode steps: 494, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.005842, mae: 0.550935, mean_q: 0.678914, mean_eps: 0.597770\n",
            "  90214/125000: episode: 139, duration: 133.362s, episode steps: 582, steps per second:   4, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.006667, mae: 0.555621, mean_q: 0.685314, mean_eps: 0.595349\n",
            "  90711/125000: episode: 140, duration: 113.897s, episode steps: 497, steps per second:   4, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.008217, mae: 0.576812, mean_q: 0.711340, mean_eps: 0.592921\n",
            "  91185/125000: episode: 141, duration: 106.637s, episode steps: 474, steps per second:   4, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.007941, mae: 0.573514, mean_q: 0.707156, mean_eps: 0.590736\n",
            "  91965/125000: episode: 142, duration: 178.063s, episode steps: 780, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.007871, mae: 0.574353, mean_q: 0.708143, mean_eps: 0.587915\n",
            "  92618/125000: episode: 143, duration: 148.427s, episode steps: 653, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007007, mae: 0.564857, mean_q: 0.696021, mean_eps: 0.584691\n",
            "  93123/125000: episode: 144, duration: 114.228s, episode steps: 505, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.007135, mae: 0.576522, mean_q: 0.710722, mean_eps: 0.582085\n",
            "  93651/125000: episode: 145, duration: 120.606s, episode steps: 528, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.007223, mae: 0.577290, mean_q: 0.711869, mean_eps: 0.579761\n",
            "  94159/125000: episode: 146, duration: 115.026s, episode steps: 508, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.007019, mae: 0.578353, mean_q: 0.713439, mean_eps: 0.577430\n",
            "  94885/125000: episode: 147, duration: 164.014s, episode steps: 726, steps per second:   4, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.006614, mae: 0.570351, mean_q: 0.702536, mean_eps: 0.574653\n",
            "  95507/125000: episode: 148, duration: 142.551s, episode steps: 622, steps per second:   4, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.006228, mae: 0.566955, mean_q: 0.698606, mean_eps: 0.571620\n",
            "  96576/125000: episode: 149, duration: 243.821s, episode steps: 1069, steps per second:   4, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.006156, mae: 0.568608, mean_q: 0.701282, mean_eps: 0.567816\n",
            "  96941/125000: episode: 150, duration: 84.555s, episode steps: 365, steps per second:   4, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.006312, mae: 0.568959, mean_q: 0.701591, mean_eps: 0.564589\n",
            "  97580/125000: episode: 151, duration: 145.220s, episode steps: 639, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.006507, mae: 0.570505, mean_q: 0.702743, mean_eps: 0.562330\n",
            "  98050/125000: episode: 152, duration: 107.816s, episode steps: 470, steps per second:   4, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.006360, mae: 0.571420, mean_q: 0.704288, mean_eps: 0.559835\n",
            "  98749/125000: episode: 153, duration: 158.461s, episode steps: 699, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.005988, mae: 0.566286, mean_q: 0.698506, mean_eps: 0.557204\n",
            "  99304/125000: episode: 154, duration: 126.042s, episode steps: 555, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006123, mae: 0.569917, mean_q: 0.702977, mean_eps: 0.554383\n",
            "  99921/125000: episode: 155, duration: 139.916s, episode steps: 617, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.005923, mae: 0.566077, mean_q: 0.698083, mean_eps: 0.551746\n",
            " 100478/125000: episode: 156, duration: 129.140s, episode steps: 557, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.007984, mae: 0.582972, mean_q: 0.717914, mean_eps: 0.549104\n",
            " 101053/125000: episode: 157, duration: 131.487s, episode steps: 575, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.008028, mae: 0.588658, mean_q: 0.725016, mean_eps: 0.546557\n",
            " 102127/125000: episode: 158, duration: 246.751s, episode steps: 1074, steps per second:   4, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.008063, mae: 0.585983, mean_q: 0.721780, mean_eps: 0.542847\n",
            " 102769/125000: episode: 159, duration: 150.171s, episode steps: 642, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.007298, mae: 0.584425, mean_q: 0.719058, mean_eps: 0.538986\n",
            " 103115/125000: episode: 160, duration: 78.789s, episode steps: 346, steps per second:   4, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.006655, mae: 0.590728, mean_q: 0.725533, mean_eps: 0.536763\n",
            " 103700/125000: episode: 161, duration: 134.600s, episode steps: 585, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.007228, mae: 0.586391, mean_q: 0.719702, mean_eps: 0.534668\n",
            " 104092/125000: episode: 162, duration: 92.029s, episode steps: 392, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.006664, mae: 0.581991, mean_q: 0.716289, mean_eps: 0.532470\n",
            " 104924/125000: episode: 163, duration: 191.780s, episode steps: 832, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.006431, mae: 0.583789, mean_q: 0.718357, mean_eps: 0.529716\n",
            " 105549/125000: episode: 164, duration: 145.511s, episode steps: 625, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.006437, mae: 0.582361, mean_q: 0.715844, mean_eps: 0.526438\n",
            " 106127/125000: episode: 165, duration: 132.234s, episode steps: 578, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.006433, mae: 0.580848, mean_q: 0.714414, mean_eps: 0.523731\n",
            " 106815/125000: episode: 166, duration: 156.981s, episode steps: 688, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.006430, mae: 0.587616, mean_q: 0.723060, mean_eps: 0.520883\n",
            " 107363/125000: episode: 167, duration: 126.233s, episode steps: 548, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.006592, mae: 0.583908, mean_q: 0.718302, mean_eps: 0.518102\n",
            " 108051/125000: episode: 168, duration: 154.999s, episode steps: 688, steps per second:   4, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.006620, mae: 0.587186, mean_q: 0.722711, mean_eps: 0.515321\n",
            " 108560/125000: episode: 169, duration: 115.243s, episode steps: 509, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.006379, mae: 0.584494, mean_q: 0.719770, mean_eps: 0.512628\n",
            " 109057/125000: episode: 170, duration: 113.001s, episode steps: 497, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.006422, mae: 0.584553, mean_q: 0.719652, mean_eps: 0.510364\n",
            " 109687/125000: episode: 171, duration: 143.009s, episode steps: 630, steps per second:   4, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.006351, mae: 0.587421, mean_q: 0.723644, mean_eps: 0.507828\n",
            " 110317/125000: episode: 172, duration: 142.594s, episode steps: 630, steps per second:   4, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.007091, mae: 0.600028, mean_q: 0.738072, mean_eps: 0.504993\n",
            " 110894/125000: episode: 173, duration: 131.333s, episode steps: 577, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.008223, mae: 0.607929, mean_q: 0.747032, mean_eps: 0.502278\n",
            " 111261/125000: episode: 174, duration: 83.408s, episode steps: 367, steps per second:   4, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.008376, mae: 0.605272, mean_q: 0.742902, mean_eps: 0.500154\n",
            " 111947/125000: episode: 175, duration: 155.969s, episode steps: 686, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.007758, mae: 0.604332, mean_q: 0.742622, mean_eps: 0.497784\n",
            " 112437/125000: episode: 176, duration: 108.914s, episode steps: 490, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.007623, mae: 0.609725, mean_q: 0.749552, mean_eps: 0.495138\n",
            " 113605/125000: episode: 177, duration: 265.072s, episode steps: 1168, steps per second:   4, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.007527, mae: 0.610238, mean_q: 0.750962, mean_eps: 0.491408\n",
            " 114004/125000: episode: 178, duration: 90.044s, episode steps: 399, steps per second:   4, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.007595, mae: 0.616648, mean_q: 0.757804, mean_eps: 0.487882\n",
            " 114534/125000: episode: 179, duration: 118.615s, episode steps: 530, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.007122, mae: 0.612388, mean_q: 0.753439, mean_eps: 0.485792\n",
            " 115169/125000: episode: 180, duration: 143.102s, episode steps: 635, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.006972, mae: 0.611215, mean_q: 0.750895, mean_eps: 0.483170\n",
            " 115847/125000: episode: 181, duration: 153.624s, episode steps: 678, steps per second:   4, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.006727, mae: 0.608314, mean_q: 0.748370, mean_eps: 0.480216\n",
            " 116331/125000: episode: 182, duration: 110.585s, episode steps: 484, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.007091, mae: 0.604318, mean_q: 0.743606, mean_eps: 0.477602\n",
            " 117167/125000: episode: 183, duration: 189.501s, episode steps: 836, steps per second:   4, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.007006, mae: 0.608525, mean_q: 0.748469, mean_eps: 0.474632\n",
            " 117670/125000: episode: 184, duration: 113.381s, episode steps: 503, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.006875, mae: 0.607395, mean_q: 0.746698, mean_eps: 0.471619\n",
            " 118352/125000: episode: 185, duration: 153.573s, episode steps: 682, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.006831, mae: 0.608621, mean_q: 0.749161, mean_eps: 0.468953\n",
            " 119301/125000: episode: 186, duration: 214.396s, episode steps: 949, steps per second:   4, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.006338, mae: 0.610669, mean_q: 0.751742, mean_eps: 0.465283\n",
            " 119812/125000: episode: 187, duration: 116.115s, episode steps: 511, steps per second:   4, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.881 [0.000, 5.000],  loss: 0.007054, mae: 0.607993, mean_q: 0.747585, mean_eps: 0.461998\n",
            " 120657/125000: episode: 188, duration: 190.993s, episode steps: 845, steps per second:   4, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.008684, mae: 0.632111, mean_q: 0.777301, mean_eps: 0.458947\n",
            " 121198/125000: episode: 189, duration: 121.723s, episode steps: 541, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008513, mae: 0.645262, mean_q: 0.792039, mean_eps: 0.455828\n",
            " 121728/125000: episode: 190, duration: 119.497s, episode steps: 530, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.008614, mae: 0.641960, mean_q: 0.788651, mean_eps: 0.453419\n",
            " 122500/125000: episode: 191, duration: 172.655s, episode steps: 772, steps per second:   4, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.008344, mae: 0.633270, mean_q: 0.777704, mean_eps: 0.450489\n",
            " 123242/125000: episode: 192, duration: 168.691s, episode steps: 742, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.007810, mae: 0.635274, mean_q: 0.780082, mean_eps: 0.447083\n",
            " 123965/125000: episode: 193, duration: 163.738s, episode steps: 723, steps per second:   4, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.007422, mae: 0.634824, mean_q: 0.781268, mean_eps: 0.443787\n",
            " 124380/125000: episode: 194, duration: 94.127s, episode steps: 415, steps per second:   4, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.007180, mae: 0.635163, mean_q: 0.781220, mean_eps: 0.441226\n",
            "done, took 17828.386 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a92eb93da90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "weights_filename = 'dqn_weights_125000.h5f'.format(env)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-P1haTn_Goo",
        "outputId": "0b71a628-8cc2-4cab-defe-bff7ab8d5f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 9.000, steps: 520\n",
            "Episode 2: reward: 22.000, steps: 980\n",
            "Episode 3: reward: 10.000, steps: 601\n",
            "Episode 4: reward: 22.000, steps: 953\n",
            "Episode 5: reward: 21.000, steps: 1026\n",
            "Episode 6: reward: 10.000, steps: 638\n",
            "Episode 7: reward: 6.000, steps: 477\n",
            "Episode 8: reward: 9.000, steps: 522\n",
            "Episode 9: reward: 8.000, steps: 688\n",
            "Episode 10: reward: 7.000, steps: 598\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a92e8fe7990>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenamiento parte 3"
      ],
      "metadata": {
        "id": "1WNhfeASg9VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se estableció un warm-up de 50,000 pasos en el que el agente recolecta experiencias sin actualizar la red, garantizando una base diversa y representativa en la memoria. Luego la red objetivo se actualiza cada 10,000 pasos proporcionando un balance entre estabilidad y capacidad de adaptación.\n",
        "\n",
        "Seguimos empleando el optimizador ADAM y una tasa de aprendizaje del 0.00025, para equilibrar la velocidad de convergencia y estabilidad en este tipo de arquitecturas.\n",
        "\n",
        "Así mismo, se continpuan guardando los pesos del modelo cada 25,000 pasos y se registran métricas como perdida y recompensa cada 10,000 pasos en un archivo JSON."
      ],
      "metadata": {
        "id": "9OnM2peRlx0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AGENTE DQN\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
        "               nb_steps_warmup=50000, enable_double_dqn=True,\n",
        "               enable_dueling_network=True, dueling_type='avg',\n",
        "               target_model_update=10000, policy=policy,\n",
        "               processor=AtariProcessor())\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "dqn.compile(Adam(lr=0.00025), metrics=['mae'])\n",
        "\n",
        "# CHECKPOINTS Y LOGGING\n",
        "checkpoint_path = drive_root + '/dqn_weights_{step}.h5f'\n",
        "weights_filename = drive_root + '/dqn_final_weights.h5f'\n",
        "\n",
        "callbacks = [\n",
        "    ModelIntervalCheckpoint(checkpoint_path, interval=25000),\n",
        "    FileLogger(drive_root + '/dqn_log.json', interval=10000)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsZNx4gAH8Da",
        "outputId": "324b5c3e-ded5-4ec3-9f1d-c18a3b65bc18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTRENAMIENTO\n",
        "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=2, callbacks=callbacks)\n",
        "dqn.save_weights(weights_filename, overwrite=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcf7QfIxH71S",
        "outputId": "f31a76fa-99a7-4c28-bedb-1dc72a83cf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1000000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    612/1000000: episode: 1, duration: 2.414s, episode steps: 612, steps per second: 254, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1256/1000000: episode: 2, duration: 2.400s, episode steps: 644, steps per second: 268, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1687/1000000: episode: 3, duration: 1.392s, episode steps: 431, steps per second: 310, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2465/1000000: episode: 4, duration: 2.388s, episode steps: 778, steps per second: 326, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3211/1000000: episode: 5, duration: 2.198s, episode steps: 746, steps per second: 339, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3785/1000000: episode: 6, duration: 1.744s, episode steps: 574, steps per second: 329, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4300/1000000: episode: 7, duration: 2.109s, episode steps: 515, steps per second: 244, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4817/1000000: episode: 8, duration: 2.048s, episode steps: 517, steps per second: 252, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5178/1000000: episode: 9, duration: 1.190s, episode steps: 361, steps per second: 303, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5932/1000000: episode: 10, duration: 2.329s, episode steps: 754, steps per second: 324, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   6726/1000000: episode: 11, duration: 2.470s, episode steps: 794, steps per second: 321, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   7444/1000000: episode: 12, duration: 2.241s, episode steps: 718, steps per second: 320, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   7856/1000000: episode: 13, duration: 1.475s, episode steps: 412, steps per second: 279, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8516/1000000: episode: 14, duration: 2.440s, episode steps: 660, steps per second: 270, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   9303/1000000: episode: 15, duration: 2.787s, episode steps: 787, steps per second: 282, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   9862/1000000: episode: 16, duration: 1.852s, episode steps: 559, steps per second: 302, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  10862/1000000: episode: 17, duration: 3.257s, episode steps: 1000, steps per second: 307, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  11541/1000000: episode: 18, duration: 2.446s, episode steps: 679, steps per second: 278, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  12209/1000000: episode: 19, duration: 2.121s, episode steps: 668, steps per second: 315, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  13521/1000000: episode: 20, duration: 4.330s, episode steps: 1312, steps per second: 303, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  14205/1000000: episode: 21, duration: 2.206s, episode steps: 684, steps per second: 310, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  15150/1000000: episode: 22, duration: 3.222s, episode steps: 945, steps per second: 293, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  15695/1000000: episode: 23, duration: 1.914s, episode steps: 545, steps per second: 285, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  17135/1000000: episode: 24, duration: 4.182s, episode steps: 1440, steps per second: 344, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  17764/1000000: episode: 25, duration: 1.985s, episode steps: 629, steps per second: 317, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  18363/1000000: episode: 26, duration: 1.914s, episode steps: 599, steps per second: 313, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  19686/1000000: episode: 27, duration: 4.628s, episode steps: 1323, steps per second: 286, episode reward: 20.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20164/1000000: episode: 28, duration: 1.483s, episode steps: 478, steps per second: 322, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20780/1000000: episode: 29, duration: 1.897s, episode steps: 616, steps per second: 325, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  21762/1000000: episode: 30, duration: 3.149s, episode steps: 982, steps per second: 312, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  22585/1000000: episode: 31, duration: 3.287s, episode steps: 823, steps per second: 250, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23268/1000000: episode: 32, duration: 2.105s, episode steps: 683, steps per second: 324, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23661/1000000: episode: 33, duration: 1.225s, episode steps: 393, steps per second: 321, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24625/1000000: episode: 34, duration: 3.144s, episode steps: 964, steps per second: 307, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  25473/1000000: episode: 35, duration: 2.889s, episode steps: 848, steps per second: 294, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26188/1000000: episode: 36, duration: 2.915s, episode steps: 715, steps per second: 245, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26980/1000000: episode: 37, duration: 2.544s, episode steps: 792, steps per second: 311, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  27368/1000000: episode: 38, duration: 1.167s, episode steps: 388, steps per second: 333, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  28237/1000000: episode: 39, duration: 2.662s, episode steps: 869, steps per second: 326, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  29273/1000000: episode: 40, duration: 3.353s, episode steps: 1036, steps per second: 309, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30606/1000000: episode: 41, duration: 4.801s, episode steps: 1333, steps per second: 278, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30971/1000000: episode: 42, duration: 1.074s, episode steps: 365, steps per second: 340, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31655/1000000: episode: 43, duration: 1.993s, episode steps: 684, steps per second: 343, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  32091/1000000: episode: 44, duration: 1.429s, episode steps: 436, steps per second: 305, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  33065/1000000: episode: 45, duration: 3.271s, episode steps: 974, steps per second: 298, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  33777/1000000: episode: 46, duration: 2.750s, episode steps: 712, steps per second: 259, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34267/1000000: episode: 47, duration: 1.456s, episode steps: 490, steps per second: 337, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34652/1000000: episode: 48, duration: 1.190s, episode steps: 385, steps per second: 323, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  35014/1000000: episode: 49, duration: 1.078s, episode steps: 362, steps per second: 336, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  35650/1000000: episode: 50, duration: 1.906s, episode steps: 636, steps per second: 334, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36207/1000000: episode: 51, duration: 1.732s, episode steps: 557, steps per second: 322, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36876/1000000: episode: 52, duration: 2.387s, episode steps: 669, steps per second: 280, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  38137/1000000: episode: 53, duration: 4.036s, episode steps: 1261, steps per second: 312, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  38642/1000000: episode: 54, duration: 1.463s, episode steps: 505, steps per second: 345, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  38975/1000000: episode: 55, duration: 1.053s, episode steps: 333, steps per second: 316, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  39699/1000000: episode: 56, duration: 2.258s, episode steps: 724, steps per second: 321, episode reward:  3.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40110/1000000: episode: 57, duration: 1.292s, episode steps: 411, steps per second: 318, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40728/1000000: episode: 58, duration: 2.351s, episode steps: 618, steps per second: 263, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41118/1000000: episode: 59, duration: 1.458s, episode steps: 390, steps per second: 267, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41734/1000000: episode: 60, duration: 1.969s, episode steps: 616, steps per second: 313, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  42660/1000000: episode: 61, duration: 2.699s, episode steps: 926, steps per second: 343, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  43232/1000000: episode: 62, duration: 1.918s, episode steps: 572, steps per second: 298, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  43966/1000000: episode: 63, duration: 2.286s, episode steps: 734, steps per second: 321, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44634/1000000: episode: 64, duration: 2.752s, episode steps: 668, steps per second: 243, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  45188/1000000: episode: 65, duration: 1.510s, episode steps: 554, steps per second: 367, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  45926/1000000: episode: 66, duration: 2.411s, episode steps: 738, steps per second: 306, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  46893/1000000: episode: 67, duration: 3.040s, episode steps: 967, steps per second: 318, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48199/1000000: episode: 68, duration: 4.525s, episode steps: 1306, steps per second: 289, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  49343/1000000: episode: 69, duration: 3.459s, episode steps: 1144, steps per second: 331, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  50144/1000000: episode: 70, duration: 23.850s, episode steps: 801, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.006639, mae: 0.019060, mean_q: 0.026977, mean_eps: 0.954935\n",
            "  50564/1000000: episode: 71, duration: 60.915s, episode steps: 420, steps per second:   7, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.007052, mae: 0.020001, mean_q: 0.024775, mean_eps: 0.954682\n",
            "  51062/1000000: episode: 72, duration: 71.727s, episode steps: 498, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.008048, mae: 0.022247, mean_q: 0.026515, mean_eps: 0.954269\n",
            "  51856/1000000: episode: 73, duration: 115.243s, episode steps: 794, steps per second:   7, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.007479, mae: 0.020157, mean_q: 0.025653, mean_eps: 0.953687\n",
            "  52479/1000000: episode: 74, duration: 91.459s, episode steps: 623, steps per second:   7, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.006594, mae: 0.018736, mean_q: 0.023555, mean_eps: 0.953050\n",
            "  53118/1000000: episode: 75, duration: 92.965s, episode steps: 639, steps per second:   7, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006485, mae: 0.020336, mean_q: 0.026323, mean_eps: 0.952482\n",
            "  53753/1000000: episode: 76, duration: 92.442s, episode steps: 635, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.006478, mae: 0.021046, mean_q: 0.027840, mean_eps: 0.951909\n",
            "  54243/1000000: episode: 77, duration: 71.125s, episode steps: 490, steps per second:   7, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.006990, mae: 0.020156, mean_q: 0.027538, mean_eps: 0.951402\n",
            "  54739/1000000: episode: 78, duration: 72.060s, episode steps: 496, steps per second:   7, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.006883, mae: 0.021547, mean_q: 0.028077, mean_eps: 0.950959\n",
            "  55250/1000000: episode: 79, duration: 75.657s, episode steps: 511, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.006689, mae: 0.020065, mean_q: 0.026031, mean_eps: 0.950505\n",
            "  56005/1000000: episode: 80, duration: 112.345s, episode steps: 755, steps per second:   7, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006780, mae: 0.020606, mean_q: 0.028489, mean_eps: 0.949936\n",
            "  56848/1000000: episode: 81, duration: 124.519s, episode steps: 843, steps per second:   7, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.007079, mae: 0.022047, mean_q: 0.030907, mean_eps: 0.949217\n",
            "  57211/1000000: episode: 82, duration: 53.465s, episode steps: 363, steps per second:   7, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.006506, mae: 0.019642, mean_q: 0.026822, mean_eps: 0.948674\n",
            "  57834/1000000: episode: 83, duration: 90.588s, episode steps: 623, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.006322, mae: 0.020297, mean_q: 0.029194, mean_eps: 0.948230\n",
            "  58391/1000000: episode: 84, duration: 81.361s, episode steps: 557, steps per second:   7, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.006641, mae: 0.020366, mean_q: 0.030510, mean_eps: 0.947699\n",
            "  58793/1000000: episode: 85, duration: 58.915s, episode steps: 402, steps per second:   7, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.006773, mae: 0.020862, mean_q: 0.029947, mean_eps: 0.947268\n",
            "  59409/1000000: episode: 86, duration: 90.501s, episode steps: 616, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.006653, mae: 0.021708, mean_q: 0.033140, mean_eps: 0.946810\n",
            "  60309/1000000: episode: 87, duration: 131.109s, episode steps: 900, steps per second:   7, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.006222, mae: 0.027576, mean_q: 0.042896, mean_eps: 0.946127\n",
            "  60767/1000000: episode: 88, duration: 66.892s, episode steps: 458, steps per second:   7, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.006284, mae: 0.038877, mean_q: 0.060540, mean_eps: 0.945516\n",
            "  61264/1000000: episode: 89, duration: 74.546s, episode steps: 497, steps per second:   7, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.006937, mae: 0.042009, mean_q: 0.065439, mean_eps: 0.945086\n",
            "  61852/1000000: episode: 90, duration: 86.369s, episode steps: 588, steps per second:   7, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.006351, mae: 0.041518, mean_q: 0.065594, mean_eps: 0.944598\n",
            "  62502/1000000: episode: 91, duration: 95.504s, episode steps: 650, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.006551, mae: 0.040797, mean_q: 0.063553, mean_eps: 0.944041\n",
            "  63492/1000000: episode: 92, duration: 145.806s, episode steps: 990, steps per second:   7, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.006138, mae: 0.040053, mean_q: 0.063741, mean_eps: 0.943303\n",
            "  64007/1000000: episode: 93, duration: 75.330s, episode steps: 515, steps per second:   7, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.006104, mae: 0.041112, mean_q: 0.067374, mean_eps: 0.942626\n",
            "  64947/1000000: episode: 94, duration: 138.677s, episode steps: 940, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.006139, mae: 0.039995, mean_q: 0.064846, mean_eps: 0.941971\n",
            "  65638/1000000: episode: 95, duration: 100.592s, episode steps: 691, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.005673, mae: 0.039373, mean_q: 0.065506, mean_eps: 0.941237\n",
            "  66327/1000000: episode: 96, duration: 100.996s, episode steps: 689, steps per second:   7, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.006035, mae: 0.040406, mean_q: 0.069085, mean_eps: 0.940616\n",
            "  67171/1000000: episode: 97, duration: 123.351s, episode steps: 844, steps per second:   7, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.005550, mae: 0.039878, mean_q: 0.067603, mean_eps: 0.939926\n",
            "  67769/1000000: episode: 98, duration: 88.053s, episode steps: 598, steps per second:   7, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.005319, mae: 0.037680, mean_q: 0.065568, mean_eps: 0.939277\n",
            "  68179/1000000: episode: 99, duration: 60.730s, episode steps: 410, steps per second:   7, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.005195, mae: 0.039365, mean_q: 0.068110, mean_eps: 0.938824\n",
            "  68682/1000000: episode: 100, duration: 73.985s, episode steps: 503, steps per second:   7, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.004504, mae: 0.035659, mean_q: 0.062575, mean_eps: 0.938413\n",
            "  69380/1000000: episode: 101, duration: 100.981s, episode steps: 698, steps per second:   7, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.005244, mae: 0.038012, mean_q: 0.066491, mean_eps: 0.937873\n",
            "  69799/1000000: episode: 102, duration: 61.010s, episode steps: 419, steps per second:   7, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.005509, mae: 0.039192, mean_q: 0.070827, mean_eps: 0.937370\n",
            "  70678/1000000: episode: 103, duration: 131.345s, episode steps: 879, steps per second:   7, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.006065, mae: 0.058191, mean_q: 0.094623, mean_eps: 0.936786\n",
            "  71193/1000000: episode: 104, duration: 76.375s, episode steps: 515, steps per second:   7, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.005810, mae: 0.062003, mean_q: 0.096820, mean_eps: 0.936159\n",
            "  71984/1000000: episode: 105, duration: 115.287s, episode steps: 791, steps per second:   7, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.005557, mae: 0.060770, mean_q: 0.094277, mean_eps: 0.935571\n",
            "  72404/1000000: episode: 106, duration: 62.417s, episode steps: 420, steps per second:   7, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.005427, mae: 0.061323, mean_q: 0.096277, mean_eps: 0.935026\n",
            "  73223/1000000: episode: 107, duration: 120.766s, episode steps: 819, steps per second:   7, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.005623, mae: 0.060358, mean_q: 0.094659, mean_eps: 0.934468\n",
            "  73729/1000000: episode: 108, duration: 74.373s, episode steps: 506, steps per second:   7, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.005679, mae: 0.059697, mean_q: 0.094392, mean_eps: 0.933872\n",
            "  74588/1000000: episode: 109, duration: 125.539s, episode steps: 859, steps per second:   7, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.005357, mae: 0.059946, mean_q: 0.095586, mean_eps: 0.933258\n",
            "  75549/1000000: episode: 110, duration: 137.876s, episode steps: 961, steps per second:   7, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.005038, mae: 0.059313, mean_q: 0.093749, mean_eps: 0.932439\n",
            "  75974/1000000: episode: 111, duration: 61.355s, episode steps: 425, steps per second:   7, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.004338, mae: 0.055165, mean_q: 0.086929, mean_eps: 0.931815\n",
            "  76428/1000000: episode: 112, duration: 65.975s, episode steps: 454, steps per second:   7, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.004474, mae: 0.057413, mean_q: 0.090818, mean_eps: 0.931420\n",
            "  77070/1000000: episode: 113, duration: 93.858s, episode steps: 642, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.005388, mae: 0.059014, mean_q: 0.092456, mean_eps: 0.930926\n",
            "  77606/1000000: episode: 114, duration: 77.774s, episode steps: 536, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.004418, mae: 0.055273, mean_q: 0.087371, mean_eps: 0.930396\n",
            "  78294/1000000: episode: 115, duration: 101.139s, episode steps: 688, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.004809, mae: 0.058185, mean_q: 0.094960, mean_eps: 0.929845\n",
            "  78909/1000000: episode: 116, duration: 89.773s, episode steps: 615, steps per second:   7, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.004176, mae: 0.056344, mean_q: 0.090357, mean_eps: 0.929259\n",
            "  79423/1000000: episode: 117, duration: 75.861s, episode steps: 514, steps per second:   7, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.004689, mae: 0.057445, mean_q: 0.094201, mean_eps: 0.928751\n",
            "  80048/1000000: episode: 118, duration: 90.291s, episode steps: 625, steps per second:   7, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.004556, mae: 0.057171, mean_q: 0.092520, mean_eps: 0.928238\n",
            "  81353/1000000: episode: 119, duration: 190.772s, episode steps: 1305, steps per second:   7, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.005695, mae: 0.079166, mean_q: 0.121938, mean_eps: 0.927370\n",
            "  81846/1000000: episode: 120, duration: 71.952s, episode steps: 493, steps per second:   7, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.004995, mae: 0.077430, mean_q: 0.118960, mean_eps: 0.926561\n",
            "  82562/1000000: episode: 121, duration: 104.665s, episode steps: 716, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.004635, mae: 0.075468, mean_q: 0.114960, mean_eps: 0.926017\n",
            "  83341/1000000: episode: 122, duration: 114.134s, episode steps: 779, steps per second:   7, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.004774, mae: 0.076864, mean_q: 0.117109, mean_eps: 0.925344\n",
            "  83987/1000000: episode: 123, duration: 94.850s, episode steps: 646, steps per second:   7, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.004544, mae: 0.076469, mean_q: 0.116957, mean_eps: 0.924703\n",
            "  84972/1000000: episode: 124, duration: 145.188s, episode steps: 985, steps per second:   7, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.004655, mae: 0.076211, mean_q: 0.116510, mean_eps: 0.923969\n",
            "  85351/1000000: episode: 125, duration: 56.320s, episode steps: 379, steps per second:   7, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.004310, mae: 0.075407, mean_q: 0.116177, mean_eps: 0.923355\n",
            "  86023/1000000: episode: 126, duration: 99.282s, episode steps: 672, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.004394, mae: 0.075185, mean_q: 0.115420, mean_eps: 0.922882\n",
            "  86540/1000000: episode: 127, duration: 76.498s, episode steps: 517, steps per second:   7, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.004701, mae: 0.075829, mean_q: 0.116722, mean_eps: 0.922347\n",
            "  87493/1000000: episode: 128, duration: 141.361s, episode steps: 953, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.004179, mae: 0.074765, mean_q: 0.116480, mean_eps: 0.921686\n",
            "  88196/1000000: episode: 129, duration: 104.845s, episode steps: 703, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.004347, mae: 0.073752, mean_q: 0.114014, mean_eps: 0.920940\n",
            "  88924/1000000: episode: 130, duration: 108.277s, episode steps: 728, steps per second:   7, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.004378, mae: 0.074551, mean_q: 0.115818, mean_eps: 0.920296\n",
            "  89625/1000000: episode: 131, duration: 104.034s, episode steps: 701, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.003808, mae: 0.072579, mean_q: 0.112630, mean_eps: 0.919653\n",
            "  90029/1000000: episode: 132, duration: 60.135s, episode steps: 404, steps per second:   7, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.004195, mae: 0.075699, mean_q: 0.116972, mean_eps: 0.919156\n",
            "  90570/1000000: episode: 133, duration: 80.501s, episode steps: 541, steps per second:   7, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.007563, mae: 0.114007, mean_q: 0.170180, mean_eps: 0.918731\n",
            "  91526/1000000: episode: 134, duration: 138.561s, episode steps: 956, steps per second:   7, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.006644, mae: 0.113837, mean_q: 0.166495, mean_eps: 0.918057\n",
            "  92346/1000000: episode: 135, duration: 121.435s, episode steps: 820, steps per second:   7, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.006293, mae: 0.113910, mean_q: 0.167583, mean_eps: 0.917258\n",
            "  93398/1000000: episode: 136, duration: 154.383s, episode steps: 1052, steps per second:   7, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.005782, mae: 0.112515, mean_q: 0.164385, mean_eps: 0.916416\n",
            "  94482/1000000: episode: 137, duration: 159.861s, episode steps: 1084, steps per second:   7, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.005236, mae: 0.112334, mean_q: 0.164035, mean_eps: 0.915454\n",
            "  95157/1000000: episode: 138, duration: 102.383s, episode steps: 675, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.005165, mae: 0.111252, mean_q: 0.162678, mean_eps: 0.914663\n",
            "  95804/1000000: episode: 139, duration: 96.979s, episode steps: 647, steps per second:   7, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.005259, mae: 0.110646, mean_q: 0.161782, mean_eps: 0.914068\n",
            "  96529/1000000: episode: 140, duration: 108.041s, episode steps: 725, steps per second:   7, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.005215, mae: 0.111812, mean_q: 0.163759, mean_eps: 0.913451\n",
            "  97096/1000000: episode: 141, duration: 84.597s, episode steps: 567, steps per second:   7, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.004864, mae: 0.112123, mean_q: 0.163002, mean_eps: 0.912869\n",
            "  97864/1000000: episode: 142, duration: 114.380s, episode steps: 768, steps per second:   7, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.004783, mae: 0.110621, mean_q: 0.163539, mean_eps: 0.912268\n",
            "  98253/1000000: episode: 143, duration: 58.184s, episode steps: 389, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.004297, mae: 0.110313, mean_q: 0.163217, mean_eps: 0.911748\n",
            "  99627/1000000: episode: 144, duration: 204.478s, episode steps: 1374, steps per second:   7, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.004404, mae: 0.108940, mean_q: 0.160479, mean_eps: 0.910954\n",
            " 100295/1000000: episode: 145, duration: 100.159s, episode steps: 668, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.005922, mae: 0.122420, mean_q: 0.176728, mean_eps: 0.910036\n",
            " 100942/1000000: episode: 146, duration: 96.317s, episode steps: 647, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.007156, mae: 0.143526, mean_q: 0.203722, mean_eps: 0.909444\n",
            " 101624/1000000: episode: 147, duration: 101.070s, episode steps: 682, steps per second:   7, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.006319, mae: 0.137898, mean_q: 0.195697, mean_eps: 0.908846\n",
            " 102116/1000000: episode: 148, duration: 73.886s, episode steps: 492, steps per second:   7, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.006859, mae: 0.142359, mean_q: 0.200567, mean_eps: 0.908317\n",
            " 103423/1000000: episode: 149, duration: 193.163s, episode steps: 1307, steps per second:   7, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.005784, mae: 0.138563, mean_q: 0.196287, mean_eps: 0.907508\n",
            " 104292/1000000: episode: 150, duration: 128.783s, episode steps: 869, steps per second:   7, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.005814, mae: 0.140470, mean_q: 0.198322, mean_eps: 0.906529\n",
            " 104959/1000000: episode: 151, duration: 97.971s, episode steps: 667, steps per second:   7, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.005492, mae: 0.139124, mean_q: 0.197229, mean_eps: 0.905837\n",
            " 105546/1000000: episode: 152, duration: 85.051s, episode steps: 587, steps per second:   7, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.005480, mae: 0.137171, mean_q: 0.193724, mean_eps: 0.905273\n",
            " 106350/1000000: episode: 153, duration: 119.911s, episode steps: 804, steps per second:   7, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.005176, mae: 0.140360, mean_q: 0.198409, mean_eps: 0.904647\n",
            " 107159/1000000: episode: 154, duration: 118.217s, episode steps: 809, steps per second:   7, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.005571, mae: 0.137631, mean_q: 0.194986, mean_eps: 0.903921\n",
            " 108220/1000000: episode: 155, duration: 154.381s, episode steps: 1061, steps per second:   7, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.005419, mae: 0.138754, mean_q: 0.198006, mean_eps: 0.903080\n",
            " 108782/1000000: episode: 156, duration: 81.604s, episode steps: 562, steps per second:   7, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.004910, mae: 0.136320, mean_q: 0.194879, mean_eps: 0.902350\n",
            " 109732/1000000: episode: 157, duration: 137.952s, episode steps: 950, steps per second:   7, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.004933, mae: 0.136264, mean_q: 0.195175, mean_eps: 0.901669\n",
            " 110244/1000000: episode: 158, duration: 74.165s, episode steps: 512, steps per second:   7, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.006629, mae: 0.153609, mean_q: 0.217513, mean_eps: 0.901011\n",
            " 111076/1000000: episode: 159, duration: 120.887s, episode steps: 832, steps per second:   7, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.007760, mae: 0.171923, mean_q: 0.240432, mean_eps: 0.900406\n",
            " 111740/1000000: episode: 160, duration: 96.219s, episode steps: 664, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.006890, mae: 0.167692, mean_q: 0.234768, mean_eps: 0.899733\n",
            " 112497/1000000: episode: 161, duration: 109.980s, episode steps: 757, steps per second:   7, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.006529, mae: 0.165715, mean_q: 0.230817, mean_eps: 0.899094\n",
            " 112908/1000000: episode: 162, duration: 59.471s, episode steps: 411, steps per second:   7, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.006983, mae: 0.169105, mean_q: 0.235513, mean_eps: 0.898568\n",
            " 113415/1000000: episode: 163, duration: 73.646s, episode steps: 507, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.005904, mae: 0.165732, mean_q: 0.230224, mean_eps: 0.898155\n",
            " 113980/1000000: episode: 164, duration: 81.225s, episode steps: 565, steps per second:   7, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.006416, mae: 0.166375, mean_q: 0.231779, mean_eps: 0.897673\n",
            " 115466/1000000: episode: 165, duration: 218.222s, episode steps: 1486, steps per second:   7, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.005794, mae: 0.166355, mean_q: 0.232751, mean_eps: 0.896750\n",
            " 116066/1000000: episode: 166, duration: 87.398s, episode steps: 600, steps per second:   7, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.005493, mae: 0.159157, mean_q: 0.221981, mean_eps: 0.895811\n",
            " 116475/1000000: episode: 167, duration: 59.590s, episode steps: 409, steps per second:   7, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.005336, mae: 0.159678, mean_q: 0.223347, mean_eps: 0.895357\n",
            " 116832/1000000: episode: 168, duration: 52.321s, episode steps: 357, steps per second:   7, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.005018, mae: 0.162687, mean_q: 0.226509, mean_eps: 0.895012\n",
            " 117810/1000000: episode: 169, duration: 142.489s, episode steps: 978, steps per second:   7, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.005514, mae: 0.161669, mean_q: 0.226100, mean_eps: 0.894412\n",
            " 118512/1000000: episode: 170, duration: 102.063s, episode steps: 702, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.005567, mae: 0.163872, mean_q: 0.230257, mean_eps: 0.893656\n",
            " 119031/1000000: episode: 171, duration: 76.582s, episode steps: 519, steps per second:   7, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.004948, mae: 0.161730, mean_q: 0.227209, mean_eps: 0.893106\n",
            " 119669/1000000: episode: 172, duration: 92.983s, episode steps: 638, steps per second:   7, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.004427, mae: 0.156417, mean_q: 0.219714, mean_eps: 0.892585\n",
            " 120174/1000000: episode: 173, duration: 74.457s, episode steps: 505, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.006222, mae: 0.172942, mean_q: 0.241799, mean_eps: 0.892071\n",
            " 120991/1000000: episode: 174, duration: 119.331s, episode steps: 817, steps per second:   7, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.008522, mae: 0.201517, mean_q: 0.277714, mean_eps: 0.891476\n",
            " 122327/1000000: episode: 175, duration: 195.298s, episode steps: 1336, steps per second:   7, episode reward: 12.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.007089, mae: 0.199797, mean_q: 0.274992, mean_eps: 0.890507\n",
            " 123340/1000000: episode: 176, duration: 148.459s, episode steps: 1013, steps per second:   7, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.006121, mae: 0.197308, mean_q: 0.271290, mean_eps: 0.889450\n",
            " 124015/1000000: episode: 177, duration: 98.236s, episode steps: 675, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006607, mae: 0.195395, mean_q: 0.268326, mean_eps: 0.888691\n",
            " 124841/1000000: episode: 178, duration: 119.859s, episode steps: 826, steps per second:   7, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.006441, mae: 0.198222, mean_q: 0.273243, mean_eps: 0.888015\n",
            " 125984/1000000: episode: 179, duration: 170.162s, episode steps: 1143, steps per second:   7, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.006063, mae: 0.195490, mean_q: 0.270013, mean_eps: 0.887129\n",
            " 126715/1000000: episode: 180, duration: 107.672s, episode steps: 731, steps per second:   7, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006101, mae: 0.200611, mean_q: 0.276104, mean_eps: 0.886286\n",
            " 127358/1000000: episode: 181, duration: 94.469s, episode steps: 643, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.005815, mae: 0.196167, mean_q: 0.269695, mean_eps: 0.885668\n",
            " 128154/1000000: episode: 182, duration: 116.563s, episode steps: 796, steps per second:   7, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.005674, mae: 0.198769, mean_q: 0.273692, mean_eps: 0.885020\n",
            " 128678/1000000: episode: 183, duration: 76.270s, episode steps: 524, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.005848, mae: 0.195026, mean_q: 0.268682, mean_eps: 0.884426\n",
            " 129466/1000000: episode: 184, duration: 115.544s, episode steps: 788, steps per second:   7, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.005425, mae: 0.195039, mean_q: 0.268666, mean_eps: 0.883836\n",
            " 130121/1000000: episode: 185, duration: 95.439s, episode steps: 655, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.006128, mae: 0.202460, mean_q: 0.278057, mean_eps: 0.883186\n",
            " 130919/1000000: episode: 186, duration: 116.975s, episode steps: 798, steps per second:   7, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008416, mae: 0.235932, mean_q: 0.319414, mean_eps: 0.882532\n",
            " 131449/1000000: episode: 187, duration: 78.104s, episode steps: 530, steps per second:   7, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.007674, mae: 0.229938, mean_q: 0.310442, mean_eps: 0.881935\n",
            " 131967/1000000: episode: 188, duration: 75.789s, episode steps: 518, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.007625, mae: 0.237238, mean_q: 0.320502, mean_eps: 0.881463\n",
            " 132367/1000000: episode: 189, duration: 59.060s, episode steps: 400, steps per second:   7, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.007058, mae: 0.228569, mean_q: 0.308934, mean_eps: 0.881050\n",
            " 133004/1000000: episode: 190, duration: 93.239s, episode steps: 637, steps per second:   7, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.006721, mae: 0.230345, mean_q: 0.312849, mean_eps: 0.880583\n",
            " 133667/1000000: episode: 191, duration: 96.697s, episode steps: 663, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.006507, mae: 0.229230, mean_q: 0.309330, mean_eps: 0.879999\n",
            " 134352/1000000: episode: 192, duration: 99.736s, episode steps: 685, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.006562, mae: 0.232090, mean_q: 0.314406, mean_eps: 0.879392\n",
            " 134936/1000000: episode: 193, duration: 85.295s, episode steps: 584, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.005987, mae: 0.228295, mean_q: 0.311186, mean_eps: 0.878821\n",
            " 135725/1000000: episode: 194, duration: 115.114s, episode steps: 789, steps per second:   7, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.006050, mae: 0.226422, mean_q: 0.307365, mean_eps: 0.878203\n",
            " 136475/1000000: episode: 195, duration: 109.326s, episode steps: 750, steps per second:   7, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.006577, mae: 0.229431, mean_q: 0.311375, mean_eps: 0.877510\n",
            " 137019/1000000: episode: 196, duration: 79.687s, episode steps: 544, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.005642, mae: 0.226759, mean_q: 0.308651, mean_eps: 0.876928\n",
            " 137945/1000000: episode: 197, duration: 135.278s, episode steps: 926, steps per second:   7, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.005740, mae: 0.224624, mean_q: 0.305470, mean_eps: 0.876267\n",
            " 138785/1000000: episode: 198, duration: 122.285s, episode steps: 840, steps per second:   7, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.005762, mae: 0.222957, mean_q: 0.302390, mean_eps: 0.875472\n",
            " 139396/1000000: episode: 199, duration: 88.896s, episode steps: 611, steps per second:   7, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.005603, mae: 0.227539, mean_q: 0.310268, mean_eps: 0.874819\n",
            " 140328/1000000: episode: 200, duration: 136.739s, episode steps: 932, steps per second:   7, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.006263, mae: 0.239840, mean_q: 0.325296, mean_eps: 0.874125\n",
            " 140959/1000000: episode: 201, duration: 94.710s, episode steps: 631, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.008288, mae: 0.260661, mean_q: 0.350568, mean_eps: 0.873421\n",
            " 141644/1000000: episode: 202, duration: 102.549s, episode steps: 685, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.007563, mae: 0.260236, mean_q: 0.347791, mean_eps: 0.872829\n",
            " 142818/1000000: episode: 203, duration: 174.023s, episode steps: 1174, steps per second:   7, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.007055, mae: 0.260110, mean_q: 0.349474, mean_eps: 0.871993\n",
            " 143434/1000000: episode: 204, duration: 91.286s, episode steps: 616, steps per second:   7, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.006814, mae: 0.259169, mean_q: 0.347419, mean_eps: 0.871187\n",
            " 144249/1000000: episode: 205, duration: 119.647s, episode steps: 815, steps per second:   7, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.006295, mae: 0.257478, mean_q: 0.345457, mean_eps: 0.870543\n",
            " 144950/1000000: episode: 206, duration: 102.947s, episode steps: 701, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.006690, mae: 0.258005, mean_q: 0.346608, mean_eps: 0.869861\n",
            " 145630/1000000: episode: 207, duration: 100.176s, episode steps: 680, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.006209, mae: 0.256397, mean_q: 0.344183, mean_eps: 0.869239\n",
            " 146787/1000000: episode: 208, duration: 169.951s, episode steps: 1157, steps per second:   7, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.006170, mae: 0.258027, mean_q: 0.346497, mean_eps: 0.868413\n",
            " 147423/1000000: episode: 209, duration: 94.149s, episode steps: 636, steps per second:   7, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.006119, mae: 0.256743, mean_q: 0.345578, mean_eps: 0.867606\n",
            " 148012/1000000: episode: 210, duration: 86.944s, episode steps: 589, steps per second:   7, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.005691, mae: 0.253388, mean_q: 0.341363, mean_eps: 0.867055\n",
            " 148387/1000000: episode: 211, duration: 55.600s, episode steps: 375, steps per second:   7, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.005688, mae: 0.251503, mean_q: 0.339989, mean_eps: 0.866621\n",
            " 149237/1000000: episode: 212, duration: 124.408s, episode steps: 850, steps per second:   7, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.005562, mae: 0.250867, mean_q: 0.337155, mean_eps: 0.866070\n",
            " 149874/1000000: episode: 213, duration: 93.173s, episode steps: 637, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.005191, mae: 0.251565, mean_q: 0.338015, mean_eps: 0.865401\n",
            " 151014/1000000: episode: 214, duration: 167.073s, episode steps: 1140, steps per second:   7, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.008748, mae: 0.292804, mean_q: 0.389965, mean_eps: 0.864601\n",
            " 151729/1000000: episode: 215, duration: 107.330s, episode steps: 715, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.007638, mae: 0.291527, mean_q: 0.387412, mean_eps: 0.863766\n",
            " 152435/1000000: episode: 216, duration: 104.902s, episode steps: 706, steps per second:   7, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.007271, mae: 0.293516, mean_q: 0.390904, mean_eps: 0.863127\n",
            " 153322/1000000: episode: 217, duration: 131.095s, episode steps: 887, steps per second:   7, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.007260, mae: 0.291167, mean_q: 0.387029, mean_eps: 0.862410\n",
            " 154000/1000000: episode: 218, duration: 99.328s, episode steps: 678, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.007171, mae: 0.291780, mean_q: 0.388539, mean_eps: 0.861706\n",
            " 154633/1000000: episode: 219, duration: 93.269s, episode steps: 633, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.006372, mae: 0.292197, mean_q: 0.388148, mean_eps: 0.861116\n",
            " 155168/1000000: episode: 220, duration: 80.299s, episode steps: 535, steps per second:   7, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.006694, mae: 0.292563, mean_q: 0.389817, mean_eps: 0.860590\n",
            " 155697/1000000: episode: 221, duration: 78.594s, episode steps: 529, steps per second:   7, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.006502, mae: 0.288386, mean_q: 0.383194, mean_eps: 0.860111\n",
            " 156281/1000000: episode: 222, duration: 87.689s, episode steps: 584, steps per second:   7, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.006113, mae: 0.291589, mean_q: 0.388448, mean_eps: 0.859610\n",
            " 156801/1000000: episode: 223, duration: 77.270s, episode steps: 520, steps per second:   7, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.006323, mae: 0.288430, mean_q: 0.383957, mean_eps: 0.859114\n",
            " 157350/1000000: episode: 224, duration: 82.326s, episode steps: 549, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.006646, mae: 0.292980, mean_q: 0.389711, mean_eps: 0.858632\n",
            " 157722/1000000: episode: 225, duration: 55.797s, episode steps: 372, steps per second:   7, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.006135, mae: 0.291809, mean_q: 0.389156, mean_eps: 0.858218\n",
            " 158783/1000000: episode: 226, duration: 156.408s, episode steps: 1061, steps per second:   7, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.006067, mae: 0.288694, mean_q: 0.385081, mean_eps: 0.857573\n",
            " 159334/1000000: episode: 227, duration: 81.954s, episode steps: 551, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.006357, mae: 0.291547, mean_q: 0.390055, mean_eps: 0.856848\n",
            " 160116/1000000: episode: 228, duration: 120.234s, episode steps: 782, steps per second:   7, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.006039, mae: 0.289173, mean_q: 0.385422, mean_eps: 0.856248\n",
            " 161023/1000000: episode: 229, duration: 135.532s, episode steps: 907, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007939, mae: 0.327924, mean_q: 0.433963, mean_eps: 0.855488\n",
            " 161563/1000000: episode: 230, duration: 81.365s, episode steps: 540, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007600, mae: 0.327617, mean_q: 0.433794, mean_eps: 0.854837\n",
            " 162280/1000000: episode: 231, duration: 107.047s, episode steps: 717, steps per second:   7, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.007118, mae: 0.323055, mean_q: 0.426793, mean_eps: 0.854271\n",
            " 162779/1000000: episode: 232, duration: 74.676s, episode steps: 499, steps per second:   7, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.007280, mae: 0.321427, mean_q: 0.423555, mean_eps: 0.853724\n",
            " 163241/1000000: episode: 233, duration: 69.104s, episode steps: 462, steps per second:   7, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.007362, mae: 0.321527, mean_q: 0.424662, mean_eps: 0.853291\n",
            " 163655/1000000: episode: 234, duration: 61.655s, episode steps: 414, steps per second:   7, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.006452, mae: 0.318569, mean_q: 0.421622, mean_eps: 0.852897\n",
            " 164058/1000000: episode: 235, duration: 60.010s, episode steps: 403, steps per second:   7, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.007261, mae: 0.321257, mean_q: 0.422889, mean_eps: 0.852530\n",
            " 164701/1000000: episode: 236, duration: 95.145s, episode steps: 643, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.006578, mae: 0.319054, mean_q: 0.420999, mean_eps: 0.852059\n",
            " 165213/1000000: episode: 237, duration: 76.528s, episode steps: 512, steps per second:   7, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.006189, mae: 0.315939, mean_q: 0.418127, mean_eps: 0.851539\n",
            " 165789/1000000: episode: 238, duration: 86.313s, episode steps: 576, steps per second:   7, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.006359, mae: 0.317977, mean_q: 0.419632, mean_eps: 0.851050\n",
            " 166807/1000000: episode: 239, duration: 151.667s, episode steps: 1018, steps per second:   7, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.006206, mae: 0.315564, mean_q: 0.416136, mean_eps: 0.850332\n",
            " 167701/1000000: episode: 240, duration: 131.836s, episode steps: 894, steps per second:   7, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.006786, mae: 0.318476, mean_q: 0.420898, mean_eps: 0.849472\n",
            " 168646/1000000: episode: 241, duration: 139.280s, episode steps: 945, steps per second:   7, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.006172, mae: 0.319658, mean_q: 0.423000, mean_eps: 0.848644\n",
            " 169037/1000000: episode: 242, duration: 57.023s, episode steps: 391, steps per second:   7, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.006541, mae: 0.318709, mean_q: 0.422358, mean_eps: 0.848043\n",
            " 169778/1000000: episode: 243, duration: 112.748s, episode steps: 741, steps per second:   7, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.005787, mae: 0.317883, mean_q: 0.420793, mean_eps: 0.847534\n",
            " 170411/1000000: episode: 244, duration: 95.276s, episode steps: 633, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.007224, mae: 0.334515, mean_q: 0.441219, mean_eps: 0.846915\n",
            " 171032/1000000: episode: 245, duration: 92.020s, episode steps: 621, steps per second:   7, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008185, mae: 0.341862, mean_q: 0.449297, mean_eps: 0.846351\n",
            " 171624/1000000: episode: 246, duration: 87.033s, episode steps: 592, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.006955, mae: 0.343344, mean_q: 0.451392, mean_eps: 0.845805\n",
            " 172282/1000000: episode: 247, duration: 96.665s, episode steps: 658, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007506, mae: 0.338593, mean_q: 0.444728, mean_eps: 0.845243\n",
            " 173190/1000000: episode: 248, duration: 133.245s, episode steps: 908, steps per second:   7, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.006854, mae: 0.340075, mean_q: 0.446488, mean_eps: 0.844538\n",
            " 174264/1000000: episode: 249, duration: 159.876s, episode steps: 1074, steps per second:   7, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.006536, mae: 0.338872, mean_q: 0.445991, mean_eps: 0.843646\n",
            " 174879/1000000: episode: 250, duration: 91.001s, episode steps: 615, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.005656, mae: 0.335023, mean_q: 0.442077, mean_eps: 0.842886\n",
            " 175547/1000000: episode: 251, duration: 99.141s, episode steps: 668, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.005995, mae: 0.335074, mean_q: 0.440055, mean_eps: 0.842309\n",
            " 176234/1000000: episode: 252, duration: 100.876s, episode steps: 687, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.005712, mae: 0.334276, mean_q: 0.440270, mean_eps: 0.841699\n",
            " 177010/1000000: episode: 253, duration: 114.658s, episode steps: 776, steps per second:   7, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.005361, mae: 0.332762, mean_q: 0.437932, mean_eps: 0.841041\n",
            " 177541/1000000: episode: 254, duration: 77.768s, episode steps: 531, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.006418, mae: 0.333149, mean_q: 0.437811, mean_eps: 0.840453\n",
            " 178562/1000000: episode: 255, duration: 151.397s, episode steps: 1021, steps per second:   7, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.006076, mae: 0.334930, mean_q: 0.439368, mean_eps: 0.839754\n",
            " 179214/1000000: episode: 256, duration: 96.212s, episode steps: 652, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.005738, mae: 0.337646, mean_q: 0.444568, mean_eps: 0.839001\n",
            " 179714/1000000: episode: 257, duration: 74.429s, episode steps: 500, steps per second:   7, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.005181, mae: 0.332313, mean_q: 0.437371, mean_eps: 0.838483\n",
            " 180405/1000000: episode: 258, duration: 103.613s, episode steps: 691, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007622, mae: 0.361357, mean_q: 0.474109, mean_eps: 0.837947\n",
            " 181165/1000000: episode: 259, duration: 114.107s, episode steps: 760, steps per second:   7, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.008588, mae: 0.373642, mean_q: 0.488362, mean_eps: 0.837294\n",
            " 182051/1000000: episode: 260, duration: 132.608s, episode steps: 886, steps per second:   7, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.007545, mae: 0.364678, mean_q: 0.476880, mean_eps: 0.836553\n",
            " 182861/1000000: episode: 261, duration: 121.457s, episode steps: 810, steps per second:   7, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.007881, mae: 0.370311, mean_q: 0.483847, mean_eps: 0.835790\n",
            " 183502/1000000: episode: 262, duration: 95.880s, episode steps: 641, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.007298, mae: 0.369660, mean_q: 0.483653, mean_eps: 0.835137\n",
            " 184075/1000000: episode: 263, duration: 85.476s, episode steps: 573, steps per second:   7, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.007534, mae: 0.369276, mean_q: 0.483710, mean_eps: 0.834591\n",
            " 184767/1000000: episode: 264, duration: 103.947s, episode steps: 692, steps per second:   7, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.006938, mae: 0.367915, mean_q: 0.482150, mean_eps: 0.834022\n",
            " 185411/1000000: episode: 265, duration: 96.553s, episode steps: 644, steps per second:   7, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.006918, mae: 0.365542, mean_q: 0.477794, mean_eps: 0.833420\n",
            " 185941/1000000: episode: 266, duration: 79.637s, episode steps: 530, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.006302, mae: 0.363662, mean_q: 0.475286, mean_eps: 0.832892\n",
            " 186775/1000000: episode: 267, duration: 125.084s, episode steps: 834, steps per second:   7, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.006604, mae: 0.368988, mean_q: 0.481671, mean_eps: 0.832278\n",
            " 187776/1000000: episode: 268, duration: 149.839s, episode steps: 1001, steps per second:   7, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.006415, mae: 0.366088, mean_q: 0.478796, mean_eps: 0.831452\n",
            " 188304/1000000: episode: 269, duration: 79.021s, episode steps: 528, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.006055, mae: 0.360346, mean_q: 0.471977, mean_eps: 0.830764\n",
            " 189035/1000000: episode: 270, duration: 109.142s, episode steps: 731, steps per second:   7, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.006043, mae: 0.363754, mean_q: 0.476057, mean_eps: 0.830198\n",
            " 189559/1000000: episode: 271, duration: 76.631s, episode steps: 524, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.006023, mae: 0.364796, mean_q: 0.477306, mean_eps: 0.829633\n",
            " 190218/1000000: episode: 272, duration: 96.216s, episode steps: 659, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.006706, mae: 0.377405, mean_q: 0.492182, mean_eps: 0.829101\n",
            " 190869/1000000: episode: 273, duration: 95.510s, episode steps: 651, steps per second:   7, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.008636, mae: 0.399379, mean_q: 0.518073, mean_eps: 0.828511\n",
            " 191343/1000000: episode: 274, duration: 69.518s, episode steps: 474, steps per second:   7, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.008152, mae: 0.401714, mean_q: 0.521411, mean_eps: 0.828005\n",
            " 192001/1000000: episode: 275, duration: 96.314s, episode steps: 658, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.008172, mae: 0.399086, mean_q: 0.517871, mean_eps: 0.827496\n",
            " 192853/1000000: episode: 276, duration: 124.447s, episode steps: 852, steps per second:   7, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.007459, mae: 0.401206, mean_q: 0.521169, mean_eps: 0.826816\n",
            " 193403/1000000: episode: 277, duration: 81.076s, episode steps: 550, steps per second:   7, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.007452, mae: 0.397880, mean_q: 0.516936, mean_eps: 0.826185\n",
            " 193944/1000000: episode: 278, duration: 97.408s, episode steps: 541, steps per second:   6, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.006952, mae: 0.394189, mean_q: 0.511708, mean_eps: 0.825694\n",
            " 194333/1000000: episode: 279, duration: 57.537s, episode steps: 389, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.007043, mae: 0.397335, mean_q: 0.515415, mean_eps: 0.825276\n",
            " 195307/1000000: episode: 280, duration: 143.197s, episode steps: 974, steps per second:   7, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.006896, mae: 0.393963, mean_q: 0.510784, mean_eps: 0.824662\n",
            " 196044/1000000: episode: 281, duration: 107.508s, episode steps: 737, steps per second:   7, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.006977, mae: 0.394788, mean_q: 0.513764, mean_eps: 0.823893\n",
            " 196730/1000000: episode: 282, duration: 100.585s, episode steps: 686, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.006367, mae: 0.395692, mean_q: 0.514607, mean_eps: 0.823252\n",
            " 197227/1000000: episode: 283, duration: 72.584s, episode steps: 497, steps per second:   7, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.006744, mae: 0.390151, mean_q: 0.506570, mean_eps: 0.822720\n",
            " 197910/1000000: episode: 284, duration: 99.557s, episode steps: 683, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.006913, mae: 0.395502, mean_q: 0.513508, mean_eps: 0.822189\n",
            " 198263/1000000: episode: 285, duration: 51.465s, episode steps: 353, steps per second:   7, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.006639, mae: 0.388793, mean_q: 0.505626, mean_eps: 0.821723\n",
            " 198853/1000000: episode: 286, duration: 86.685s, episode steps: 590, steps per second:   7, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.006222, mae: 0.393605, mean_q: 0.513274, mean_eps: 0.821298\n",
            " 199468/1000000: episode: 287, duration: 89.680s, episode steps: 615, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.006341, mae: 0.394796, mean_q: 0.514400, mean_eps: 0.820756\n",
            " 200038/1000000: episode: 288, duration: 83.092s, episode steps: 570, steps per second:   7, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.005771, mae: 0.397648, mean_q: 0.517994, mean_eps: 0.820223\n",
            " 200656/1000000: episode: 289, duration: 90.520s, episode steps: 618, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.009206, mae: 0.449015, mean_q: 0.582489, mean_eps: 0.819688\n",
            " 201426/1000000: episode: 290, duration: 112.643s, episode steps: 770, steps per second:   7, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.009077, mae: 0.448378, mean_q: 0.578593, mean_eps: 0.819064\n",
            " 202106/1000000: episode: 291, duration: 100.830s, episode steps: 680, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.008663, mae: 0.445468, mean_q: 0.575025, mean_eps: 0.818411\n",
            " 203130/1000000: episode: 292, duration: 149.973s, episode steps: 1024, steps per second:   7, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.007902, mae: 0.443355, mean_q: 0.572478, mean_eps: 0.817644\n",
            " 203993/1000000: episode: 293, duration: 127.098s, episode steps: 863, steps per second:   7, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.007266, mae: 0.437321, mean_q: 0.564796, mean_eps: 0.816795\n",
            " 205001/1000000: episode: 294, duration: 147.460s, episode steps: 1008, steps per second:   7, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007326, mae: 0.440399, mean_q: 0.567883, mean_eps: 0.815953\n",
            " 205498/1000000: episode: 295, duration: 73.860s, episode steps: 497, steps per second:   7, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006843, mae: 0.434682, mean_q: 0.559950, mean_eps: 0.815276\n",
            " 206523/1000000: episode: 296, duration: 149.611s, episode steps: 1025, steps per second:   7, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006888, mae: 0.436372, mean_q: 0.564757, mean_eps: 0.814591\n",
            " 207330/1000000: episode: 297, duration: 118.861s, episode steps: 807, steps per second:   7, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.006807, mae: 0.432892, mean_q: 0.559740, mean_eps: 0.813767\n",
            " 208353/1000000: episode: 298, duration: 150.379s, episode steps: 1023, steps per second:   7, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.006375, mae: 0.430159, mean_q: 0.556364, mean_eps: 0.812943\n",
            " 208749/1000000: episode: 299, duration: 58.311s, episode steps: 396, steps per second:   7, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.006411, mae: 0.433470, mean_q: 0.560996, mean_eps: 0.812305\n",
            " 209446/1000000: episode: 300, duration: 101.962s, episode steps: 697, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.006015, mae: 0.431050, mean_q: 0.556640, mean_eps: 0.811813\n",
            " 209982/1000000: episode: 301, duration: 79.739s, episode steps: 536, steps per second:   7, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.006491, mae: 0.430632, mean_q: 0.555080, mean_eps: 0.811258\n",
            " 210521/1000000: episode: 302, duration: 79.998s, episode steps: 539, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.009841, mae: 0.467064, mean_q: 0.601637, mean_eps: 0.810774\n",
            " 211208/1000000: episode: 303, duration: 100.661s, episode steps: 687, steps per second:   7, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.009461, mae: 0.475743, mean_q: 0.612455, mean_eps: 0.810222\n",
            " 211862/1000000: episode: 304, duration: 96.257s, episode steps: 654, steps per second:   7, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008718, mae: 0.474382, mean_q: 0.610667, mean_eps: 0.809619\n",
            " 212531/1000000: episode: 305, duration: 99.087s, episode steps: 669, steps per second:   7, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.008728, mae: 0.469102, mean_q: 0.603237, mean_eps: 0.809024\n",
            " 213213/1000000: episode: 306, duration: 99.749s, episode steps: 682, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.008456, mae: 0.473634, mean_q: 0.608275, mean_eps: 0.808416\n",
            " 213910/1000000: episode: 307, duration: 102.435s, episode steps: 697, steps per second:   7, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.007672, mae: 0.470729, mean_q: 0.604837, mean_eps: 0.807795\n",
            " 214721/1000000: episode: 308, duration: 118.818s, episode steps: 811, steps per second:   7, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.007595, mae: 0.472582, mean_q: 0.607397, mean_eps: 0.807117\n",
            " 215695/1000000: episode: 309, duration: 144.087s, episode steps: 974, steps per second:   7, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.007457, mae: 0.471355, mean_q: 0.605690, mean_eps: 0.806313\n",
            " 216196/1000000: episode: 310, duration: 74.175s, episode steps: 501, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.007132, mae: 0.463533, mean_q: 0.595978, mean_eps: 0.805649\n",
            " 216746/1000000: episode: 311, duration: 81.220s, episode steps: 550, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.007080, mae: 0.467517, mean_q: 0.601675, mean_eps: 0.805177\n",
            " 217335/1000000: episode: 312, duration: 86.588s, episode steps: 589, steps per second:   7, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.007221, mae: 0.469878, mean_q: 0.603627, mean_eps: 0.804664\n",
            " 218025/1000000: episode: 313, duration: 102.010s, episode steps: 690, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.007406, mae: 0.470724, mean_q: 0.605553, mean_eps: 0.804088\n",
            " 218512/1000000: episode: 314, duration: 71.713s, episode steps: 487, steps per second:   7, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.006591, mae: 0.466382, mean_q: 0.600067, mean_eps: 0.803559\n",
            " 219071/1000000: episode: 315, duration: 81.883s, episode steps: 559, steps per second:   7, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.006864, mae: 0.472544, mean_q: 0.606975, mean_eps: 0.803088\n",
            " 219577/1000000: episode: 316, duration: 74.804s, episode steps: 506, steps per second:   7, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.006793, mae: 0.473166, mean_q: 0.609971, mean_eps: 0.802609\n",
            " 219957/1000000: episode: 317, duration: 55.401s, episode steps: 380, steps per second:   7, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.006814, mae: 0.470383, mean_q: 0.604960, mean_eps: 0.802210\n",
            " 220342/1000000: episode: 318, duration: 57.431s, episode steps: 385, steps per second:   7, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.009843, mae: 0.514437, mean_q: 0.658776, mean_eps: 0.801866\n",
            " 220947/1000000: episode: 319, duration: 89.646s, episode steps: 605, steps per second:   7, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.009728, mae: 0.516963, mean_q: 0.660411, mean_eps: 0.801420\n",
            " 221769/1000000: episode: 320, duration: 120.928s, episode steps: 822, steps per second:   7, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009559, mae: 0.513863, mean_q: 0.657529, mean_eps: 0.800778\n",
            " 223184/1000000: episode: 321, duration: 207.247s, episode steps: 1415, steps per second:   7, episode reward: 19.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.008610, mae: 0.513722, mean_q: 0.656667, mean_eps: 0.799772\n",
            " 224671/1000000: episode: 322, duration: 217.990s, episode steps: 1487, steps per second:   7, episode reward: 10.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.007976, mae: 0.508133, mean_q: 0.649047, mean_eps: 0.798466\n",
            " 225868/1000000: episode: 323, duration: 173.635s, episode steps: 1197, steps per second:   7, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.007272, mae: 0.508114, mean_q: 0.649054, mean_eps: 0.797258\n",
            " 227029/1000000: episode: 324, duration: 169.520s, episode steps: 1161, steps per second:   7, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.007003, mae: 0.504976, mean_q: 0.645028, mean_eps: 0.796197\n",
            " 227736/1000000: episode: 325, duration: 103.717s, episode steps: 707, steps per second:   7, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.007286, mae: 0.506330, mean_q: 0.646673, mean_eps: 0.795356\n",
            " 228525/1000000: episode: 326, duration: 115.879s, episode steps: 789, steps per second:   7, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.007317, mae: 0.503309, mean_q: 0.643136, mean_eps: 0.794683\n",
            " 229030/1000000: episode: 327, duration: 73.738s, episode steps: 505, steps per second:   7, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.007231, mae: 0.503493, mean_q: 0.642476, mean_eps: 0.794101\n",
            " 229981/1000000: episode: 328, duration: 138.891s, episode steps: 951, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.007133, mae: 0.505992, mean_q: 0.645939, mean_eps: 0.793446\n",
            " 230774/1000000: episode: 329, duration: 115.996s, episode steps: 793, steps per second:   7, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009757, mae: 0.541064, mean_q: 0.688544, mean_eps: 0.792661\n",
            " 231721/1000000: episode: 330, duration: 138.162s, episode steps: 947, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009156, mae: 0.540056, mean_q: 0.686614, mean_eps: 0.791878\n",
            " 232227/1000000: episode: 331, duration: 74.385s, episode steps: 506, steps per second:   7, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.008870, mae: 0.538133, mean_q: 0.684014, mean_eps: 0.791224\n",
            " 232906/1000000: episode: 332, duration: 99.354s, episode steps: 679, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.008334, mae: 0.534916, mean_q: 0.678722, mean_eps: 0.790691\n",
            " 233432/1000000: episode: 333, duration: 76.315s, episode steps: 526, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008230, mae: 0.540925, mean_q: 0.687113, mean_eps: 0.790148\n",
            " 233942/1000000: episode: 334, duration: 74.122s, episode steps: 510, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007909, mae: 0.543919, mean_q: 0.691565, mean_eps: 0.789682\n",
            " 234636/1000000: episode: 335, duration: 100.968s, episode steps: 694, steps per second:   7, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.007708, mae: 0.537795, mean_q: 0.682634, mean_eps: 0.789140\n",
            " 235027/1000000: episode: 336, duration: 57.056s, episode steps: 391, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.007598, mae: 0.535651, mean_q: 0.679879, mean_eps: 0.788652\n",
            " 235572/1000000: episode: 337, duration: 78.577s, episode steps: 545, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.007559, mae: 0.535095, mean_q: 0.680549, mean_eps: 0.788231\n",
            " 236099/1000000: episode: 338, duration: 76.932s, episode steps: 527, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.007706, mae: 0.535488, mean_q: 0.680487, mean_eps: 0.787748\n",
            " 236797/1000000: episode: 339, duration: 100.877s, episode steps: 698, steps per second:   7, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007368, mae: 0.535085, mean_q: 0.681292, mean_eps: 0.787197\n",
            " 237488/1000000: episode: 340, duration: 100.538s, episode steps: 691, steps per second:   7, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.007612, mae: 0.531528, mean_q: 0.676147, mean_eps: 0.786572\n",
            " 238154/1000000: episode: 341, duration: 97.575s, episode steps: 666, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.007360, mae: 0.533724, mean_q: 0.679019, mean_eps: 0.785962\n",
            " 239073/1000000: episode: 342, duration: 133.641s, episode steps: 919, steps per second:   7, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007205, mae: 0.532291, mean_q: 0.677520, mean_eps: 0.785248\n",
            " 239677/1000000: episode: 343, duration: 88.216s, episode steps: 604, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.006806, mae: 0.530088, mean_q: 0.674981, mean_eps: 0.784563\n",
            " 240608/1000000: episode: 344, duration: 135.205s, episode steps: 931, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.008856, mae: 0.557140, mean_q: 0.706918, mean_eps: 0.783872\n",
            " 241126/1000000: episode: 345, duration: 75.445s, episode steps: 518, steps per second:   7, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.009048, mae: 0.565931, mean_q: 0.717333, mean_eps: 0.783220\n",
            " 242051/1000000: episode: 346, duration: 135.022s, episode steps: 925, steps per second:   7, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.008650, mae: 0.571896, mean_q: 0.724825, mean_eps: 0.782571\n",
            " 242861/1000000: episode: 347, duration: 117.524s, episode steps: 810, steps per second:   7, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.008249, mae: 0.569493, mean_q: 0.721345, mean_eps: 0.781790\n",
            " 243469/1000000: episode: 348, duration: 88.168s, episode steps: 608, steps per second:   7, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.008270, mae: 0.572365, mean_q: 0.725462, mean_eps: 0.781152\n",
            " 244198/1000000: episode: 349, duration: 105.838s, episode steps: 729, steps per second:   7, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.008108, mae: 0.570313, mean_q: 0.722237, mean_eps: 0.780550\n",
            " 244990/1000000: episode: 350, duration: 115.129s, episode steps: 792, steps per second:   7, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.007414, mae: 0.564734, mean_q: 0.714736, mean_eps: 0.779866\n",
            " 245735/1000000: episode: 351, duration: 107.962s, episode steps: 745, steps per second:   7, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.007762, mae: 0.564541, mean_q: 0.714324, mean_eps: 0.779174\n",
            " 246784/1000000: episode: 352, duration: 152.198s, episode steps: 1049, steps per second:   7, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.007342, mae: 0.563254, mean_q: 0.712554, mean_eps: 0.778367\n",
            " 247328/1000000: episode: 353, duration: 78.632s, episode steps: 544, steps per second:   7, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007100, mae: 0.556207, mean_q: 0.703921, mean_eps: 0.777650\n",
            " 247946/1000000: episode: 354, duration: 90.199s, episode steps: 618, steps per second:   7, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.006944, mae: 0.561291, mean_q: 0.710879, mean_eps: 0.777127\n",
            " 248474/1000000: episode: 355, duration: 77.287s, episode steps: 528, steps per second:   7, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.007052, mae: 0.562993, mean_q: 0.712348, mean_eps: 0.776611\n",
            " 249607/1000000: episode: 356, duration: 165.591s, episode steps: 1133, steps per second:   7, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.006867, mae: 0.554085, mean_q: 0.701208, mean_eps: 0.775864\n",
            " 250523/1000000: episode: 357, duration: 133.332s, episode steps: 916, steps per second:   7, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.009214, mae: 0.587555, mean_q: 0.743309, mean_eps: 0.774942\n",
            " 251036/1000000: episode: 358, duration: 75.551s, episode steps: 513, steps per second:   7, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010291, mae: 0.599924, mean_q: 0.757401, mean_eps: 0.774299\n",
            " 251733/1000000: episode: 359, duration: 100.877s, episode steps: 697, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.009323, mae: 0.600288, mean_q: 0.757699, mean_eps: 0.773754\n",
            " 252646/1000000: episode: 360, duration: 132.939s, episode steps: 913, steps per second:   7, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.008872, mae: 0.604178, mean_q: 0.763333, mean_eps: 0.773030\n",
            " 253376/1000000: episode: 361, duration: 106.087s, episode steps: 730, steps per second:   7, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.008006, mae: 0.593298, mean_q: 0.748194, mean_eps: 0.772291\n",
            " 253803/1000000: episode: 362, duration: 62.666s, episode steps: 427, steps per second:   7, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.008358, mae: 0.596203, mean_q: 0.751182, mean_eps: 0.771770\n",
            " 254558/1000000: episode: 363, duration: 110.514s, episode steps: 755, steps per second:   7, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.008323, mae: 0.593834, mean_q: 0.748873, mean_eps: 0.771238\n",
            " 255181/1000000: episode: 364, duration: 91.573s, episode steps: 623, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.008147, mae: 0.596910, mean_q: 0.754243, mean_eps: 0.770618\n",
            " 256178/1000000: episode: 365, duration: 146.123s, episode steps: 997, steps per second:   7, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.007719, mae: 0.594685, mean_q: 0.751915, mean_eps: 0.769889\n",
            " 256875/1000000: episode: 366, duration: 102.771s, episode steps: 697, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.007881, mae: 0.595260, mean_q: 0.752228, mean_eps: 0.769127\n",
            " 258060/1000000: episode: 367, duration: 173.151s, episode steps: 1185, steps per second:   7, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.007382, mae: 0.591994, mean_q: 0.749107, mean_eps: 0.768280\n",
            " 259362/1000000: episode: 368, duration: 190.406s, episode steps: 1302, steps per second:   7, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.006948, mae: 0.592215, mean_q: 0.749219, mean_eps: 0.767161\n",
            " 259757/1000000: episode: 369, duration: 58.208s, episode steps: 395, steps per second:   7, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.007634, mae: 0.591885, mean_q: 0.748147, mean_eps: 0.766397\n",
            " 260514/1000000: episode: 370, duration: 110.342s, episode steps: 757, steps per second:   7, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.009762, mae: 0.617196, mean_q: 0.777615, mean_eps: 0.765879\n",
            " 261239/1000000: episode: 371, duration: 106.106s, episode steps: 725, steps per second:   7, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.009632, mae: 0.627966, mean_q: 0.791248, mean_eps: 0.765212\n",
            " 262188/1000000: episode: 372, duration: 139.133s, episode steps: 949, steps per second:   7, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.009356, mae: 0.626573, mean_q: 0.788254, mean_eps: 0.764458\n",
            " 262878/1000000: episode: 373, duration: 100.659s, episode steps: 690, steps per second:   7, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.009020, mae: 0.629644, mean_q: 0.792693, mean_eps: 0.763721\n",
            " 263810/1000000: episode: 374, duration: 136.381s, episode steps: 932, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.008627, mae: 0.621529, mean_q: 0.782431, mean_eps: 0.762991\n",
            " 264487/1000000: episode: 375, duration: 98.175s, episode steps: 677, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.008309, mae: 0.622721, mean_q: 0.783720, mean_eps: 0.762267\n",
            " 265120/1000000: episode: 376, duration: 92.520s, episode steps: 633, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007867, mae: 0.628148, mean_q: 0.791615, mean_eps: 0.761677\n",
            " 265674/1000000: episode: 377, duration: 81.132s, episode steps: 554, steps per second:   7, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007894, mae: 0.625655, mean_q: 0.789034, mean_eps: 0.761143\n",
            " 266352/1000000: episode: 378, duration: 99.292s, episode steps: 678, steps per second:   7, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.008116, mae: 0.620280, mean_q: 0.781458, mean_eps: 0.760589\n",
            " 266949/1000000: episode: 379, duration: 87.580s, episode steps: 597, steps per second:   7, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.007247, mae: 0.619213, mean_q: 0.779427, mean_eps: 0.760015\n",
            " 267559/1000000: episode: 380, duration: 90.186s, episode steps: 610, steps per second:   7, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.007477, mae: 0.622257, mean_q: 0.784089, mean_eps: 0.759472\n",
            " 268217/1000000: episode: 381, duration: 96.448s, episode steps: 658, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.007321, mae: 0.625227, mean_q: 0.788536, mean_eps: 0.758901\n",
            " 268711/1000000: episode: 382, duration: 72.347s, episode steps: 494, steps per second:   7, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.007680, mae: 0.618630, mean_q: 0.779180, mean_eps: 0.758383\n",
            " 269830/1000000: episode: 383, duration: 163.888s, episode steps: 1119, steps per second:   7, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007010, mae: 0.615413, mean_q: 0.775275, mean_eps: 0.757657\n",
            " 270233/1000000: episode: 384, duration: 59.030s, episode steps: 403, steps per second:   7, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.008943, mae: 0.646607, mean_q: 0.813382, mean_eps: 0.756972\n",
            " 270575/1000000: episode: 385, duration: 50.134s, episode steps: 342, steps per second:   7, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.010946, mae: 0.667751, mean_q: 0.838404, mean_eps: 0.756637\n",
            " 271088/1000000: episode: 386, duration: 75.289s, episode steps: 513, steps per second:   7, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.009976, mae: 0.669679, mean_q: 0.840233, mean_eps: 0.756252\n",
            " 271772/1000000: episode: 387, duration: 101.067s, episode steps: 684, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010096, mae: 0.678274, mean_q: 0.850114, mean_eps: 0.755713\n",
            " 272318/1000000: episode: 388, duration: 80.044s, episode steps: 546, steps per second:   7, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.009235, mae: 0.669859, mean_q: 0.841386, mean_eps: 0.755160\n",
            " 273042/1000000: episode: 389, duration: 105.394s, episode steps: 724, steps per second:   7, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.009034, mae: 0.669648, mean_q: 0.839390, mean_eps: 0.754588\n",
            " 273728/1000000: episode: 390, duration: 100.877s, episode steps: 686, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.008641, mae: 0.668405, mean_q: 0.838821, mean_eps: 0.753954\n",
            " 274109/1000000: episode: 391, duration: 56.028s, episode steps: 381, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.008592, mae: 0.671747, mean_q: 0.842340, mean_eps: 0.753474\n",
            " 275062/1000000: episode: 392, duration: 138.346s, episode steps: 953, steps per second:   7, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.008090, mae: 0.670949, mean_q: 0.841839, mean_eps: 0.752874\n",
            " 275454/1000000: episode: 393, duration: 57.313s, episode steps: 392, steps per second:   7, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008185, mae: 0.670329, mean_q: 0.840350, mean_eps: 0.752268\n",
            " 275994/1000000: episode: 394, duration: 80.364s, episode steps: 540, steps per second:   7, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.008274, mae: 0.676497, mean_q: 0.847931, mean_eps: 0.751849\n",
            " 276673/1000000: episode: 395, duration: 99.366s, episode steps: 679, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.007853, mae: 0.670883, mean_q: 0.840677, mean_eps: 0.751300\n",
            " 277275/1000000: episode: 396, duration: 88.503s, episode steps: 602, steps per second:   7, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.007870, mae: 0.669441, mean_q: 0.839668, mean_eps: 0.750724\n",
            " 278063/1000000: episode: 397, duration: 114.728s, episode steps: 788, steps per second:   7, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.007435, mae: 0.667420, mean_q: 0.838072, mean_eps: 0.750098\n",
            " 278609/1000000: episode: 398, duration: 79.840s, episode steps: 546, steps per second:   7, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.007786, mae: 0.668472, mean_q: 0.839503, mean_eps: 0.749498\n",
            " 279135/1000000: episode: 399, duration: 76.469s, episode steps: 526, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.007393, mae: 0.659990, mean_q: 0.827941, mean_eps: 0.749016\n",
            " 279772/1000000: episode: 400, duration: 93.107s, episode steps: 637, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.007824, mae: 0.663490, mean_q: 0.832193, mean_eps: 0.748492\n",
            " 280225/1000000: episode: 401, duration: 66.402s, episode steps: 453, steps per second:   7, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.009076, mae: 0.683070, mean_q: 0.855919, mean_eps: 0.748002\n",
            " 280699/1000000: episode: 402, duration: 69.175s, episode steps: 474, steps per second:   7, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.010745, mae: 0.695140, mean_q: 0.870117, mean_eps: 0.747585\n",
            " 281083/1000000: episode: 403, duration: 55.551s, episode steps: 384, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.010728, mae: 0.689215, mean_q: 0.862090, mean_eps: 0.747199\n",
            " 281802/1000000: episode: 404, duration: 104.783s, episode steps: 719, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.009389, mae: 0.690211, mean_q: 0.864449, mean_eps: 0.746702\n",
            " 282198/1000000: episode: 405, duration: 57.998s, episode steps: 396, steps per second:   7, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.009615, mae: 0.694053, mean_q: 0.867922, mean_eps: 0.746200\n",
            " 282711/1000000: episode: 406, duration: 74.687s, episode steps: 513, steps per second:   7, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.009186, mae: 0.684116, mean_q: 0.856440, mean_eps: 0.745791\n",
            " 283234/1000000: episode: 407, duration: 76.652s, episode steps: 523, steps per second:   7, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.008852, mae: 0.689688, mean_q: 0.862813, mean_eps: 0.745325\n",
            " 283766/1000000: episode: 408, duration: 77.201s, episode steps: 532, steps per second:   7, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.008701, mae: 0.690029, mean_q: 0.863059, mean_eps: 0.744850\n",
            " 284435/1000000: episode: 409, duration: 97.187s, episode steps: 669, steps per second:   7, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008323, mae: 0.689801, mean_q: 0.864528, mean_eps: 0.744310\n",
            " 284833/1000000: episode: 410, duration: 58.055s, episode steps: 398, steps per second:   7, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.008069, mae: 0.677783, mean_q: 0.848084, mean_eps: 0.743830\n",
            " 285505/1000000: episode: 411, duration: 97.630s, episode steps: 672, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008502, mae: 0.688517, mean_q: 0.862842, mean_eps: 0.743348\n",
            " 286310/1000000: episode: 412, duration: 116.526s, episode steps: 805, steps per second:   7, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.008008, mae: 0.690466, mean_q: 0.863073, mean_eps: 0.742684\n",
            " 286948/1000000: episode: 413, duration: 92.851s, episode steps: 638, steps per second:   7, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.007856, mae: 0.685094, mean_q: 0.858002, mean_eps: 0.742034\n",
            " 287396/1000000: episode: 414, duration: 66.103s, episode steps: 448, steps per second:   7, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.007613, mae: 0.682500, mean_q: 0.854091, mean_eps: 0.741546\n",
            " 288331/1000000: episode: 415, duration: 136.352s, episode steps: 935, steps per second:   7, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.007943, mae: 0.690650, mean_q: 0.864323, mean_eps: 0.740923\n",
            " 288712/1000000: episode: 416, duration: 56.290s, episode steps: 381, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.007517, mae: 0.690782, mean_q: 0.864848, mean_eps: 0.740331\n",
            " 289498/1000000: episode: 417, duration: 114.280s, episode steps: 786, steps per second:   7, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.007885, mae: 0.692190, mean_q: 0.865922, mean_eps: 0.739806\n",
            " 290184/1000000: episode: 418, duration: 100.093s, episode steps: 686, steps per second:   7, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.008688, mae: 0.702493, mean_q: 0.877975, mean_eps: 0.739144\n",
            " 290888/1000000: episode: 419, duration: 102.145s, episode steps: 704, steps per second:   7, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011595, mae: 0.723230, mean_q: 0.903988, mean_eps: 0.738518\n",
            " 292055/1000000: episode: 420, duration: 169.637s, episode steps: 1167, steps per second:   7, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010291, mae: 0.719783, mean_q: 0.898816, mean_eps: 0.737676\n",
            " 292527/1000000: episode: 421, duration: 69.025s, episode steps: 472, steps per second:   7, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.009687, mae: 0.717460, mean_q: 0.895160, mean_eps: 0.736939\n",
            " 293668/1000000: episode: 422, duration: 165.895s, episode steps: 1141, steps per second:   7, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009303, mae: 0.714635, mean_q: 0.891946, mean_eps: 0.736213\n",
            " 294286/1000000: episode: 423, duration: 89.537s, episode steps: 618, steps per second:   7, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008539, mae: 0.717861, mean_q: 0.895491, mean_eps: 0.735421\n",
            " 294825/1000000: episode: 424, duration: 78.511s, episode steps: 539, steps per second:   7, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.008837, mae: 0.715280, mean_q: 0.894430, mean_eps: 0.734901\n",
            " 295542/1000000: episode: 425, duration: 103.356s, episode steps: 717, steps per second:   7, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.007904, mae: 0.706004, mean_q: 0.882126, mean_eps: 0.734335\n",
            " 296210/1000000: episode: 426, duration: 97.331s, episode steps: 668, steps per second:   7, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.008596, mae: 0.709504, mean_q: 0.886072, mean_eps: 0.733712\n",
            " 296613/1000000: episode: 427, duration: 58.151s, episode steps: 403, steps per second:   7, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.007767, mae: 0.705712, mean_q: 0.881043, mean_eps: 0.733230\n",
            " 297208/1000000: episode: 428, duration: 86.568s, episode steps: 595, steps per second:   7, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007723, mae: 0.713005, mean_q: 0.889174, mean_eps: 0.732781\n",
            " 298168/1000000: episode: 429, duration: 138.805s, episode steps: 960, steps per second:   7, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.008165, mae: 0.711538, mean_q: 0.887825, mean_eps: 0.732081\n",
            " 298557/1000000: episode: 430, duration: 56.294s, episode steps: 389, steps per second:   7, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.007512, mae: 0.700905, mean_q: 0.875564, mean_eps: 0.731474\n",
            " 299134/1000000: episode: 431, duration: 83.605s, episode steps: 577, steps per second:   7, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.008444, mae: 0.712551, mean_q: 0.890626, mean_eps: 0.731040\n",
            " 299612/1000000: episode: 432, duration: 69.364s, episode steps: 478, steps per second:   7, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.008082, mae: 0.711368, mean_q: 0.887810, mean_eps: 0.730565\n",
            " 300315/1000000: episode: 433, duration: 102.327s, episode steps: 703, steps per second:   7, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.009036, mae: 0.732595, mean_q: 0.914356, mean_eps: 0.730033\n",
            " 301674/1000000: episode: 434, duration: 196.617s, episode steps: 1359, steps per second:   7, episode reward: 15.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.010875, mae: 0.748005, mean_q: 0.930654, mean_eps: 0.729105\n",
            " 302327/1000000: episode: 435, duration: 94.794s, episode steps: 653, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.009892, mae: 0.749000, mean_q: 0.934292, mean_eps: 0.728200\n",
            " 303070/1000000: episode: 436, duration: 107.513s, episode steps: 743, steps per second:   7, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009372, mae: 0.743206, mean_q: 0.925715, mean_eps: 0.727572\n",
            " 303862/1000000: episode: 437, duration: 115.000s, episode steps: 792, steps per second:   7, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.009311, mae: 0.745644, mean_q: 0.928110, mean_eps: 0.726881\n",
            " 304941/1000000: episode: 438, duration: 156.311s, episode steps: 1079, steps per second:   7, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.009069, mae: 0.744144, mean_q: 0.927374, mean_eps: 0.726039\n",
            " 305867/1000000: episode: 439, duration: 133.871s, episode steps: 926, steps per second:   7, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.008637, mae: 0.746176, mean_q: 0.930312, mean_eps: 0.725137\n",
            " 306728/1000000: episode: 440, duration: 125.001s, episode steps: 861, steps per second:   7, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.008315, mae: 0.742761, mean_q: 0.925364, mean_eps: 0.724333\n",
            " 307147/1000000: episode: 441, duration: 60.723s, episode steps: 419, steps per second:   7, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.008191, mae: 0.744810, mean_q: 0.928944, mean_eps: 0.723757\n",
            " 307869/1000000: episode: 442, duration: 104.743s, episode steps: 722, steps per second:   7, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.008077, mae: 0.744896, mean_q: 0.928754, mean_eps: 0.723243\n",
            " 308478/1000000: episode: 443, duration: 87.927s, episode steps: 609, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008305, mae: 0.741810, mean_q: 0.924119, mean_eps: 0.722644\n",
            " 309469/1000000: episode: 444, duration: 143.423s, episode steps: 991, steps per second:   7, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.007750, mae: 0.739662, mean_q: 0.921884, mean_eps: 0.721924\n",
            " 310211/1000000: episode: 445, duration: 107.295s, episode steps: 742, steps per second:   7, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.008756, mae: 0.750635, mean_q: 0.935238, mean_eps: 0.721144\n",
            " 311022/1000000: episode: 446, duration: 117.518s, episode steps: 811, steps per second:   7, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.011456, mae: 0.774871, mean_q: 0.962717, mean_eps: 0.720446\n",
            " 311547/1000000: episode: 447, duration: 76.446s, episode steps: 525, steps per second:   7, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.009602, mae: 0.768649, mean_q: 0.955615, mean_eps: 0.719844\n",
            " 312180/1000000: episode: 448, duration: 91.200s, episode steps: 633, steps per second:   7, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010585, mae: 0.783140, mean_q: 0.973807, mean_eps: 0.719323\n",
            " 312939/1000000: episode: 449, duration: 110.118s, episode steps: 759, steps per second:   7, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.009660, mae: 0.768242, mean_q: 0.955004, mean_eps: 0.718697\n",
            " 313438/1000000: episode: 450, duration: 72.457s, episode steps: 499, steps per second:   7, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.009279, mae: 0.770354, mean_q: 0.958968, mean_eps: 0.718131\n",
            " 314807/1000000: episode: 451, duration: 198.628s, episode steps: 1369, steps per second:   7, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.008870, mae: 0.766734, mean_q: 0.953461, mean_eps: 0.717290\n",
            " 316041/1000000: episode: 452, duration: 178.334s, episode steps: 1234, steps per second:   7, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.008868, mae: 0.767495, mean_q: 0.952745, mean_eps: 0.716119\n",
            " 316603/1000000: episode: 453, duration: 81.478s, episode steps: 562, steps per second:   7, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.008729, mae: 0.759998, mean_q: 0.944087, mean_eps: 0.715311\n",
            " 317601/1000000: episode: 454, duration: 143.811s, episode steps: 998, steps per second:   7, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.008737, mae: 0.768397, mean_q: 0.954612, mean_eps: 0.714609\n",
            " 318183/1000000: episode: 455, duration: 84.328s, episode steps: 582, steps per second:   7, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.008703, mae: 0.761694, mean_q: 0.946132, mean_eps: 0.713898\n",
            " 318770/1000000: episode: 456, duration: 85.223s, episode steps: 587, steps per second:   7, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.008261, mae: 0.762862, mean_q: 0.949523, mean_eps: 0.713372\n",
            " 319702/1000000: episode: 457, duration: 134.701s, episode steps: 932, steps per second:   7, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.007716, mae: 0.761760, mean_q: 0.946953, mean_eps: 0.712688\n",
            " 320617/1000000: episode: 458, duration: 132.237s, episode steps: 915, steps per second:   7, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.010451, mae: 0.780953, mean_q: 0.970245, mean_eps: 0.711857\n",
            " 321140/1000000: episode: 459, duration: 75.786s, episode steps: 523, steps per second:   7, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.011058, mae: 0.780485, mean_q: 0.969395, mean_eps: 0.711210\n",
            " 321465/1000000: episode: 460, duration: 46.876s, episode steps: 325, steps per second:   7, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.010608, mae: 0.777261, mean_q: 0.963409, mean_eps: 0.710828\n",
            " 322448/1000000: episode: 461, duration: 142.084s, episode steps: 983, steps per second:   7, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.010327, mae: 0.788008, mean_q: 0.978179, mean_eps: 0.710240\n",
            " 323079/1000000: episode: 462, duration: 92.326s, episode steps: 631, steps per second:   7, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.009548, mae: 0.774085, mean_q: 0.960594, mean_eps: 0.709513\n",
            " 323582/1000000: episode: 463, duration: 73.158s, episode steps: 503, steps per second:   7, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.009724, mae: 0.787148, mean_q: 0.976606, mean_eps: 0.709003\n",
            " 323978/1000000: episode: 464, duration: 57.471s, episode steps: 396, steps per second:   7, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009175, mae: 0.779944, mean_q: 0.969202, mean_eps: 0.708598\n",
            " 324546/1000000: episode: 465, duration: 82.304s, episode steps: 568, steps per second:   7, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009752, mae: 0.792709, mean_q: 0.982690, mean_eps: 0.708165\n",
            " 325212/1000000: episode: 466, duration: 96.954s, episode steps: 666, steps per second:   7, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.008856, mae: 0.786321, mean_q: 0.976067, mean_eps: 0.707609\n",
            " 325916/1000000: episode: 467, duration: 103.311s, episode steps: 704, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.009462, mae: 0.792188, mean_q: 0.981742, mean_eps: 0.706993\n",
            " 326771/1000000: episode: 468, duration: 124.072s, episode steps: 855, steps per second:   7, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.008598, mae: 0.784301, mean_q: 0.974210, mean_eps: 0.706291\n",
            " 327917/1000000: episode: 469, duration: 166.182s, episode steps: 1146, steps per second:   7, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.008451, mae: 0.778282, mean_q: 0.965952, mean_eps: 0.705391\n",
            " 328619/1000000: episode: 470, duration: 102.219s, episode steps: 702, steps per second:   7, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.008516, mae: 0.778606, mean_q: 0.966175, mean_eps: 0.704559\n",
            " 329051/1000000: episode: 471, duration: 62.935s, episode steps: 432, steps per second:   7, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.007775, mae: 0.773935, mean_q: 0.960417, mean_eps: 0.704049\n",
            " 329430/1000000: episode: 472, duration: 55.892s, episode steps: 379, steps per second:   7, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.008525, mae: 0.783279, mean_q: 0.971270, mean_eps: 0.703684\n",
            " 330214/1000000: episode: 473, duration: 114.874s, episode steps: 784, steps per second:   7, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.009681, mae: 0.795851, mean_q: 0.986584, mean_eps: 0.703161\n",
            " 330980/1000000: episode: 474, duration: 111.076s, episode steps: 766, steps per second:   7, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011534, mae: 0.820687, mean_q: 1.016042, mean_eps: 0.702463\n",
            " 331647/1000000: episode: 475, duration: 97.946s, episode steps: 667, steps per second:   7, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.011021, mae: 0.820456, mean_q: 1.015522, mean_eps: 0.701818\n",
            " 332300/1000000: episode: 476, duration: 95.343s, episode steps: 653, steps per second:   7, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.010113, mae: 0.813827, mean_q: 1.008510, mean_eps: 0.701224\n",
            " 332681/1000000: episode: 477, duration: 55.582s, episode steps: 381, steps per second:   7, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.010430, mae: 0.814043, mean_q: 1.008790, mean_eps: 0.700759\n",
            " 333004/1000000: episode: 478, duration: 46.982s, episode steps: 323, steps per second:   7, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.010460, mae: 0.806807, mean_q: 0.999233, mean_eps: 0.700442\n",
            " 334142/1000000: episode: 479, duration: 165.531s, episode steps: 1138, steps per second:   7, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.009658, mae: 0.817554, mean_q: 1.012841, mean_eps: 0.699785\n",
            " 334728/1000000: episode: 480, duration: 85.049s, episode steps: 586, steps per second:   7, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.009296, mae: 0.816990, mean_q: 1.012806, mean_eps: 0.699009\n",
            " 335121/1000000: episode: 481, duration: 57.230s, episode steps: 393, steps per second:   7, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.009291, mae: 0.817501, mean_q: 1.011195, mean_eps: 0.698568\n",
            " 336371/1000000: episode: 482, duration: 181.509s, episode steps: 1250, steps per second:   7, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.009431, mae: 0.816824, mean_q: 1.011370, mean_eps: 0.697829\n",
            " 337203/1000000: episode: 483, duration: 120.822s, episode steps: 832, steps per second:   7, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.009392, mae: 0.813316, mean_q: 1.007918, mean_eps: 0.696892\n",
            " 337731/1000000: episode: 484, duration: 77.352s, episode steps: 528, steps per second:   7, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009106, mae: 0.812161, mean_q: 1.005027, mean_eps: 0.696280\n",
            " 338324/1000000: episode: 485, duration: 86.312s, episode steps: 593, steps per second:   7, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.008688, mae: 0.814094, mean_q: 1.009130, mean_eps: 0.695776\n",
            " 338923/1000000: episode: 486, duration: 87.069s, episode steps: 599, steps per second:   7, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.008177, mae: 0.814158, mean_q: 1.008576, mean_eps: 0.695239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Continuando con la ejecución en el peso 325,000\n",
        "\n",
        "# Cargar pesos desde el paso 325,000\n",
        "dqn.load_weights(\"dqn_weights_325000.h5f\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6hdiK0kNIYgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso de partida\n",
        "starting_step = 325000\n",
        "remaining_steps = 1000000 - starting_step\n",
        "\n",
        "\n",
        "# Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "#  Entrenamiento con pasos continuados desde 325000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8R8ZWJlSX4V",
        "outputId": "b6e454d3-ded3-4967-888e-a70e522e228e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 675000 steps ...\n",
            "    964/675000: episode: 1, duration: 3.492s, episode steps: 964, steps per second: 276, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1513/675000: episode: 2, duration: 1.993s, episode steps: 549, steps per second: 275, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2209/675000: episode: 3, duration: 2.896s, episode steps: 696, steps per second: 240, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2593/675000: episode: 4, duration: 2.313s, episode steps: 384, steps per second: 166, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3243/675000: episode: 5, duration: 2.493s, episode steps: 650, steps per second: 261, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4078/675000: episode: 6, duration: 3.031s, episode steps: 835, steps per second: 275, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4503/675000: episode: 7, duration: 1.542s, episode steps: 425, steps per second: 276, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4934/675000: episode: 8, duration: 1.540s, episode steps: 431, steps per second: 280, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5916/675000: episode: 9, duration: 4.617s, episode steps: 982, steps per second: 213, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   6513/675000: episode: 10, duration: 2.343s, episode steps: 597, steps per second: 255, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   7361/675000: episode: 11, duration: 2.997s, episode steps: 848, steps per second: 283, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8190/675000: episode: 12, duration: 2.962s, episode steps: 829, steps per second: 280, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8954/675000: episode: 13, duration: 2.976s, episode steps: 764, steps per second: 257, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   9637/675000: episode: 14, duration: 3.485s, episode steps: 683, steps per second: 196, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  10326/675000: episode: 15, duration: 2.474s, episode steps: 689, steps per second: 279, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  10909/675000: episode: 16, duration: 2.082s, episode steps: 583, steps per second: 280, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  11481/675000: episode: 17, duration: 2.029s, episode steps: 572, steps per second: 282, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  12905/675000: episode: 18, duration: 6.283s, episode steps: 1424, steps per second: 227, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  13630/675000: episode: 19, duration: 2.518s, episode steps: 725, steps per second: 288, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  14082/675000: episode: 20, duration: 1.649s, episode steps: 452, steps per second: 274, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  15034/675000: episode: 21, duration: 3.951s, episode steps: 952, steps per second: 241, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  15939/675000: episode: 22, duration: 4.306s, episode steps: 905, steps per second: 210, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  16595/675000: episode: 23, duration: 2.701s, episode steps: 656, steps per second: 243, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  17430/675000: episode: 24, duration: 3.011s, episode steps: 835, steps per second: 277, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  18143/675000: episode: 25, duration: 2.586s, episode steps: 713, steps per second: 276, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  18944/675000: episode: 26, duration: 3.084s, episode steps: 801, steps per second: 260, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  19730/675000: episode: 27, duration: 3.991s, episode steps: 786, steps per second: 197, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20325/675000: episode: 28, duration: 2.127s, episode steps: 595, steps per second: 280, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20924/675000: episode: 29, duration: 2.148s, episode steps: 599, steps per second: 279, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  21801/675000: episode: 30, duration: 3.130s, episode steps: 877, steps per second: 280, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  22570/675000: episode: 31, duration: 3.474s, episode steps: 769, steps per second: 221, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23090/675000: episode: 32, duration: 2.516s, episode steps: 520, steps per second: 207, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23703/675000: episode: 33, duration: 2.182s, episode steps: 613, steps per second: 281, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24428/675000: episode: 34, duration: 2.628s, episode steps: 725, steps per second: 276, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24836/675000: episode: 35, duration: 1.471s, episode steps: 408, steps per second: 277, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  25492/675000: episode: 36, duration: 2.445s, episode steps: 656, steps per second: 268, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26087/675000: episode: 37, duration: 3.166s, episode steps: 595, steps per second: 188, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26724/675000: episode: 38, duration: 2.604s, episode steps: 637, steps per second: 245, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  27250/675000: episode: 39, duration: 1.895s, episode steps: 526, steps per second: 278, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  27951/675000: episode: 40, duration: 2.497s, episode steps: 701, steps per second: 281, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  28543/675000: episode: 41, duration: 2.170s, episode steps: 592, steps per second: 273, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  29222/675000: episode: 42, duration: 2.960s, episode steps: 679, steps per second: 229, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  29839/675000: episode: 43, duration: 3.046s, episode steps: 617, steps per second: 203, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30659/675000: episode: 44, duration: 2.963s, episode steps: 820, steps per second: 277, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31381/675000: episode: 45, duration: 2.610s, episode steps: 722, steps per second: 277, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31812/675000: episode: 46, duration: 1.574s, episode steps: 431, steps per second: 274, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  32454/675000: episode: 47, duration: 2.527s, episode steps: 642, steps per second: 254, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  33478/675000: episode: 48, duration: 4.762s, episode steps: 1024, steps per second: 215, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34222/675000: episode: 49, duration: 2.700s, episode steps: 744, steps per second: 276, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34618/675000: episode: 50, duration: 1.434s, episode steps: 396, steps per second: 276, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  35266/675000: episode: 51, duration: 2.296s, episode steps: 648, steps per second: 282, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  35883/675000: episode: 52, duration: 2.473s, episode steps: 617, steps per second: 249, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36510/675000: episode: 53, duration: 3.296s, episode steps: 627, steps per second: 190, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  37181/675000: episode: 54, duration: 2.407s, episode steps: 671, steps per second: 279, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  37945/675000: episode: 55, duration: 2.743s, episode steps: 764, steps per second: 279, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  38882/675000: episode: 56, duration: 3.349s, episode steps: 937, steps per second: 280, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  39635/675000: episode: 57, duration: 3.758s, episode steps: 753, steps per second: 200, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40313/675000: episode: 58, duration: 2.691s, episode steps: 678, steps per second: 252, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40939/675000: episode: 59, duration: 2.248s, episode steps: 626, steps per second: 278, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41324/675000: episode: 60, duration: 1.365s, episode steps: 385, steps per second: 282, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41806/675000: episode: 61, duration: 1.759s, episode steps: 482, steps per second: 274, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  42640/675000: episode: 62, duration: 3.282s, episode steps: 834, steps per second: 254, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  43422/675000: episode: 63, duration: 3.806s, episode steps: 782, steps per second: 205, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44317/675000: episode: 64, duration: 3.190s, episode steps: 895, steps per second: 281, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44801/675000: episode: 65, duration: 1.730s, episode steps: 484, steps per second: 280, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  45579/675000: episode: 66, duration: 2.766s, episode steps: 778, steps per second: 281, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  46345/675000: episode: 67, duration: 3.708s, episode steps: 766, steps per second: 207, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  46892/675000: episode: 68, duration: 2.327s, episode steps: 547, steps per second: 235, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  47642/675000: episode: 69, duration: 2.649s, episode steps: 750, steps per second: 283, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48937/675000: episode: 70, duration: 4.616s, episode steps: 1295, steps per second: 281, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  49311/675000: episode: 71, duration: 1.389s, episode steps: 374, steps per second: 269, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  50196/675000: episode: 72, duration: 40.975s, episode steps: 885, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014748, mae: 0.892788, mean_q: 1.090333, mean_eps: 0.954912\n",
            "  50745/675000: episode: 73, duration: 105.810s, episode steps: 549, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.014155, mae: 0.893926, mean_q: 1.086903, mean_eps: 0.954577\n",
            "  51274/675000: episode: 74, duration: 102.888s, episode steps: 529, steps per second:   5, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.014563, mae: 0.890990, mean_q: 1.080688, mean_eps: 0.954092\n",
            "  52568/675000: episode: 75, duration: 248.886s, episode steps: 1294, steps per second:   5, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013776, mae: 0.886152, mean_q: 1.074804, mean_eps: 0.953272\n",
            "  53211/675000: episode: 76, duration: 124.904s, episode steps: 643, steps per second:   5, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.012555, mae: 0.877071, mean_q: 1.064029, mean_eps: 0.952400\n",
            "  54580/675000: episode: 77, duration: 264.547s, episode steps: 1369, steps per second:   5, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.011766, mae: 0.878007, mean_q: 1.065372, mean_eps: 0.951495\n",
            "  55392/675000: episode: 78, duration: 159.454s, episode steps: 812, steps per second:   5, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012226, mae: 0.872668, mean_q: 1.058530, mean_eps: 0.950513\n",
            "  56055/675000: episode: 79, duration: 129.147s, episode steps: 663, steps per second:   5, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.011318, mae: 0.872129, mean_q: 1.057477, mean_eps: 0.949849\n",
            "  57085/675000: episode: 80, duration: 206.933s, episode steps: 1030, steps per second:   5, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.011079, mae: 0.877104, mean_q: 1.064645, mean_eps: 0.949087\n",
            "  58090/675000: episode: 81, duration: 195.969s, episode steps: 1005, steps per second:   5, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.011269, mae: 0.867325, mean_q: 1.052681, mean_eps: 0.948172\n",
            "  58887/675000: episode: 82, duration: 152.775s, episode steps: 797, steps per second:   5, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.010377, mae: 0.871926, mean_q: 1.059425, mean_eps: 0.947361\n",
            "  59707/675000: episode: 83, duration: 158.377s, episode steps: 820, steps per second:   5, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.010617, mae: 0.871655, mean_q: 1.057919, mean_eps: 0.946633\n",
            "  60551/675000: episode: 84, duration: 164.599s, episode steps: 844, steps per second:   5, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.010463, mae: 0.887944, mean_q: 1.078765, mean_eps: 0.945884\n",
            "  61203/675000: episode: 85, duration: 126.235s, episode steps: 652, steps per second:   5, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.010037, mae: 0.893005, mean_q: 1.085499, mean_eps: 0.945211\n",
            "  61816/675000: episode: 86, duration: 118.364s, episode steps: 613, steps per second:   5, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010075, mae: 0.895805, mean_q: 1.089231, mean_eps: 0.944642\n",
            "  62498/675000: episode: 87, duration: 132.860s, episode steps: 682, steps per second:   5, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.009462, mae: 0.891990, mean_q: 1.084770, mean_eps: 0.944059\n",
            "  63225/675000: episode: 88, duration: 141.824s, episode steps: 727, steps per second:   5, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.009143, mae: 0.885015, mean_q: 1.077576, mean_eps: 0.943425\n",
            "  64059/675000: episode: 89, duration: 162.215s, episode steps: 834, steps per second:   5, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.009014, mae: 0.885562, mean_q: 1.077405, mean_eps: 0.942723\n",
            "  64571/675000: episode: 90, duration: 99.859s, episode steps: 512, steps per second:   5, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.008856, mae: 0.887359, mean_q: 1.078841, mean_eps: 0.942117\n",
            "  65204/675000: episode: 91, duration: 122.686s, episode steps: 633, steps per second:   5, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008741, mae: 0.881143, mean_q: 1.072826, mean_eps: 0.941602\n",
            "  65692/675000: episode: 92, duration: 95.582s, episode steps: 488, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.008548, mae: 0.889619, mean_q: 1.082857, mean_eps: 0.941097\n",
            "  66285/675000: episode: 93, duration: 116.956s, episode steps: 593, steps per second:   5, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.008977, mae: 0.881348, mean_q: 1.072703, mean_eps: 0.940611\n",
            "  66926/675000: episode: 94, duration: 123.641s, episode steps: 641, steps per second:   5, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.008057, mae: 0.876543, mean_q: 1.066726, mean_eps: 0.940056\n",
            "  67434/675000: episode: 95, duration: 97.253s, episode steps: 508, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.007809, mae: 0.878085, mean_q: 1.069605, mean_eps: 0.939538\n",
            "  68546/675000: episode: 96, duration: 215.887s, episode steps: 1112, steps per second:   5, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.008413, mae: 0.878851, mean_q: 1.069782, mean_eps: 0.938809\n",
            "  69327/675000: episode: 97, duration: 150.027s, episode steps: 781, steps per second:   5, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.007698, mae: 0.873677, mean_q: 1.064701, mean_eps: 0.937958\n",
            "  69717/675000: episode: 98, duration: 75.912s, episode steps: 390, steps per second:   5, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.008490, mae: 0.875835, mean_q: 1.066588, mean_eps: 0.937431\n",
            "  70124/675000: episode: 99, duration: 77.615s, episode steps: 407, steps per second:   5, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.008459, mae: 0.885780, mean_q: 1.079391, mean_eps: 0.937072\n",
            "  70515/675000: episode: 100, duration: 75.850s, episode steps: 391, steps per second:   5, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.010250, mae: 0.915829, mean_q: 1.116680, mean_eps: 0.936713\n",
            "  71140/675000: episode: 101, duration: 120.348s, episode steps: 625, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.009618, mae: 0.919662, mean_q: 1.120803, mean_eps: 0.936256\n",
            "  71726/675000: episode: 102, duration: 112.173s, episode steps: 586, steps per second:   5, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.008764, mae: 0.919499, mean_q: 1.120389, mean_eps: 0.935711\n",
            "  72148/675000: episode: 103, duration: 81.090s, episode steps: 422, steps per second:   5, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009134, mae: 0.914626, mean_q: 1.113071, mean_eps: 0.935257\n",
            "  72721/675000: episode: 104, duration: 109.692s, episode steps: 573, steps per second:   5, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.008848, mae: 0.909738, mean_q: 1.107917, mean_eps: 0.934809\n",
            "  73681/675000: episode: 105, duration: 185.888s, episode steps: 960, steps per second:   5, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.008653, mae: 0.917067, mean_q: 1.117169, mean_eps: 0.934120\n",
            "  74629/675000: episode: 106, duration: 184.323s, episode steps: 948, steps per second:   5, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.008285, mae: 0.912767, mean_q: 1.112760, mean_eps: 0.933261\n",
            "  75447/675000: episode: 107, duration: 157.969s, episode steps: 818, steps per second:   5, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008477, mae: 0.918335, mean_q: 1.119053, mean_eps: 0.932466\n",
            "  75953/675000: episode: 108, duration: 98.368s, episode steps: 506, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.007764, mae: 0.913399, mean_q: 1.112799, mean_eps: 0.931870\n",
            "  76779/675000: episode: 109, duration: 157.715s, episode steps: 826, steps per second:   5, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.007857, mae: 0.914858, mean_q: 1.115286, mean_eps: 0.931271\n",
            "  77626/675000: episode: 110, duration: 162.936s, episode steps: 847, steps per second:   5, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.008044, mae: 0.912846, mean_q: 1.112030, mean_eps: 0.930518\n",
            "  78202/675000: episode: 111, duration: 112.559s, episode steps: 576, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.007909, mae: 0.919828, mean_q: 1.122345, mean_eps: 0.929878\n",
            "  79054/675000: episode: 112, duration: 166.069s, episode steps: 852, steps per second:   5, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.007411, mae: 0.909990, mean_q: 1.108822, mean_eps: 0.929235\n",
            "  80157/675000: episode: 113, duration: 212.717s, episode steps: 1103, steps per second:   5, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.007648, mae: 0.912710, mean_q: 1.113072, mean_eps: 0.928356\n",
            "  80715/675000: episode: 114, duration: 107.528s, episode steps: 558, steps per second:   5, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.008910, mae: 0.928335, mean_q: 1.132830, mean_eps: 0.927608\n",
            "  81092/675000: episode: 115, duration: 72.384s, episode steps: 377, steps per second:   5, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.008784, mae: 0.918142, mean_q: 1.118648, mean_eps: 0.927187\n",
            "  81854/675000: episode: 116, duration: 146.886s, episode steps: 762, steps per second:   5, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.008723, mae: 0.937020, mean_q: 1.143660, mean_eps: 0.926675\n",
            "  82626/675000: episode: 117, duration: 148.369s, episode steps: 772, steps per second:   5, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.008437, mae: 0.927808, mean_q: 1.132417, mean_eps: 0.925984\n",
            "  83423/675000: episode: 118, duration: 153.066s, episode steps: 797, steps per second:   5, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.008137, mae: 0.931003, mean_q: 1.135728, mean_eps: 0.925278\n",
            "  83779/675000: episode: 119, duration: 68.359s, episode steps: 356, steps per second:   5, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.008745, mae: 0.923514, mean_q: 1.127464, mean_eps: 0.924760\n",
            "  84564/675000: episode: 120, duration: 150.833s, episode steps: 785, steps per second:   5, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.007823, mae: 0.928503, mean_q: 1.134098, mean_eps: 0.924246\n",
            "  85246/675000: episode: 121, duration: 131.735s, episode steps: 682, steps per second:   5, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007536, mae: 0.927744, mean_q: 1.133329, mean_eps: 0.923586\n",
            "  86150/675000: episode: 122, duration: 174.442s, episode steps: 904, steps per second:   5, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.007263, mae: 0.925079, mean_q: 1.129282, mean_eps: 0.922872\n",
            "  86661/675000: episode: 123, duration: 98.591s, episode steps: 511, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.007608, mae: 0.925095, mean_q: 1.131007, mean_eps: 0.922235\n",
            "  87446/675000: episode: 124, duration: 151.247s, episode steps: 785, steps per second:   5, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.007917, mae: 0.926876, mean_q: 1.131890, mean_eps: 0.921652\n",
            "  88029/675000: episode: 125, duration: 112.572s, episode steps: 583, steps per second:   5, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.007633, mae: 0.921411, mean_q: 1.125516, mean_eps: 0.921037\n",
            "  88682/675000: episode: 126, duration: 124.743s, episode steps: 653, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007847, mae: 0.924878, mean_q: 1.129435, mean_eps: 0.920481\n",
            "  89421/675000: episode: 127, duration: 145.206s, episode steps: 739, steps per second:   5, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007424, mae: 0.925597, mean_q: 1.130885, mean_eps: 0.919854\n",
            "  90220/675000: episode: 128, duration: 155.467s, episode steps: 799, steps per second:   5, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.007838, mae: 0.936980, mean_q: 1.144757, mean_eps: 0.919162\n",
            "  90756/675000: episode: 129, duration: 104.988s, episode steps: 536, steps per second:   5, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.009101, mae: 0.944010, mean_q: 1.152874, mean_eps: 0.918561\n",
            "  91781/675000: episode: 130, duration: 199.730s, episode steps: 1025, steps per second:   5, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.008850, mae: 0.946800, mean_q: 1.153632, mean_eps: 0.917859\n",
            "  92467/675000: episode: 131, duration: 131.041s, episode steps: 686, steps per second:   5, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.008338, mae: 0.955329, mean_q: 1.164473, mean_eps: 0.917089\n",
            "  92976/675000: episode: 132, duration: 98.949s, episode steps: 509, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.008368, mae: 0.943859, mean_q: 1.150569, mean_eps: 0.916551\n",
            "  93677/675000: episode: 133, duration: 134.506s, episode steps: 701, steps per second:   5, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007603, mae: 0.947534, mean_q: 1.155983, mean_eps: 0.916007\n",
            "  94205/675000: episode: 134, duration: 101.519s, episode steps: 528, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.007617, mae: 0.934718, mean_q: 1.139980, mean_eps: 0.915454\n",
            "  94959/675000: episode: 135, duration: 145.210s, episode steps: 754, steps per second:   5, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.007473, mae: 0.954494, mean_q: 1.165510, mean_eps: 0.914877\n",
            "  95513/675000: episode: 136, duration: 106.937s, episode steps: 554, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.007702, mae: 0.940872, mean_q: 1.147212, mean_eps: 0.914288\n",
            "  96001/675000: episode: 137, duration: 93.807s, episode steps: 488, steps per second:   5, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.007025, mae: 0.947329, mean_q: 1.156473, mean_eps: 0.913819\n",
            "  96604/675000: episode: 138, duration: 114.720s, episode steps: 603, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.007563, mae: 0.948766, mean_q: 1.157848, mean_eps: 0.913328\n",
            "  97180/675000: episode: 139, duration: 110.571s, episode steps: 576, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.007069, mae: 0.938020, mean_q: 1.143634, mean_eps: 0.912798\n",
            "  98004/675000: episode: 140, duration: 158.828s, episode steps: 824, steps per second:   5, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.007312, mae: 0.945403, mean_q: 1.153696, mean_eps: 0.912168\n",
            "  99395/675000: episode: 141, duration: 267.410s, episode steps: 1391, steps per second:   5, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.006994, mae: 0.938736, mean_q: 1.145228, mean_eps: 0.911171\n",
            " 100319/675000: episode: 142, duration: 180.470s, episode steps: 924, steps per second:   5, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.007454, mae: 0.952272, mean_q: 1.162482, mean_eps: 0.910129\n",
            " 101021/675000: episode: 143, duration: 138.054s, episode steps: 702, steps per second:   5, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.008541, mae: 0.968454, mean_q: 1.181731, mean_eps: 0.909397\n",
            " 101404/675000: episode: 144, duration: 75.927s, episode steps: 383, steps per second:   5, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.008726, mae: 0.966208, mean_q: 1.179187, mean_eps: 0.908909\n",
            " 101996/675000: episode: 145, duration: 114.467s, episode steps: 592, steps per second:   5, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.008791, mae: 0.974303, mean_q: 1.187516, mean_eps: 0.908470\n",
            " 102367/675000: episode: 146, duration: 73.178s, episode steps: 371, steps per second:   5, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008045, mae: 0.967699, mean_q: 1.179705, mean_eps: 0.908037\n",
            " 102884/675000: episode: 147, duration: 100.128s, episode steps: 517, steps per second:   5, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.007913, mae: 0.979498, mean_q: 1.194575, mean_eps: 0.907637\n",
            " 103309/675000: episode: 148, duration: 82.859s, episode steps: 425, steps per second:   5, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.008500, mae: 0.980610, mean_q: 1.197188, mean_eps: 0.907214\n",
            " 103804/675000: episode: 149, duration: 96.729s, episode steps: 495, steps per second:   5, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.007528, mae: 0.975279, mean_q: 1.189991, mean_eps: 0.906800\n",
            " 104289/675000: episode: 150, duration: 94.329s, episode steps: 485, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.007948, mae: 0.973152, mean_q: 1.186676, mean_eps: 0.906359\n",
            " 105324/675000: episode: 151, duration: 200.733s, episode steps: 1035, steps per second:   5, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.007396, mae: 0.978397, mean_q: 1.194351, mean_eps: 0.905675\n",
            " 105694/675000: episode: 152, duration: 74.217s, episode steps: 370, steps per second:   5, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008581, mae: 0.980031, mean_q: 1.196141, mean_eps: 0.905042\n",
            " 106348/675000: episode: 153, duration: 125.913s, episode steps: 654, steps per second:   5, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.007470, mae: 0.978271, mean_q: 1.192913, mean_eps: 0.904582\n",
            " 106847/675000: episode: 154, duration: 96.164s, episode steps: 499, steps per second:   5, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.007123, mae: 0.973459, mean_q: 1.188096, mean_eps: 0.904063\n",
            " 107252/675000: episode: 155, duration: 78.860s, episode steps: 405, steps per second:   5, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.007384, mae: 0.968615, mean_q: 1.183111, mean_eps: 0.903656\n",
            " 107960/675000: episode: 156, duration: 136.764s, episode steps: 708, steps per second:   5, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.007321, mae: 0.971543, mean_q: 1.186408, mean_eps: 0.903155\n",
            " 108339/675000: episode: 157, duration: 72.349s, episode steps: 379, steps per second:   5, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.007595, mae: 0.972487, mean_q: 1.188323, mean_eps: 0.902666\n",
            " 109008/675000: episode: 158, duration: 129.598s, episode steps: 669, steps per second:   5, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.007148, mae: 0.973972, mean_q: 1.189813, mean_eps: 0.902194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 = 425000\n",
        "\n",
        "# Paso de partida 2\n",
        "starting_step = 100000 #ultimo weight actualizado\n",
        "remaining_steps = 200000 - starting_step\n",
        "\n",
        "\n",
        "# Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV3IQgKqDrTx",
        "outputId": "f334c13f-8d1c-4e97-97c2-847eccbc35cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   641/100000: episode: 1, duration: 6.721s, episode steps: 641, steps per second:  95, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1268/100000: episode: 2, duration: 2.881s, episode steps: 627, steps per second: 218, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1897/100000: episode: 3, duration: 2.492s, episode steps: 629, steps per second: 252, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2286/100000: episode: 4, duration: 1.526s, episode steps: 389, steps per second: 255, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2820/100000: episode: 5, duration: 2.423s, episode steps: 534, steps per second: 220, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3438/100000: episode: 6, duration: 3.702s, episode steps: 618, steps per second: 167, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4192/100000: episode: 7, duration: 2.978s, episode steps: 754, steps per second: 253, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4829/100000: episode: 8, duration: 2.535s, episode steps: 637, steps per second: 251, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5787/100000: episode: 9, duration: 3.904s, episode steps: 958, steps per second: 245, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6280/100000: episode: 10, duration: 3.425s, episode steps: 493, steps per second: 144, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6667/100000: episode: 11, duration: 1.801s, episode steps: 387, steps per second: 215, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7708/100000: episode: 12, duration: 5.492s, episode steps: 1041, steps per second: 190, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8452/100000: episode: 13, duration: 4.156s, episode steps: 744, steps per second: 179, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9234/100000: episode: 14, duration: 4.835s, episode steps: 782, steps per second: 162, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9971/100000: episode: 15, duration: 3.038s, episode steps: 737, steps per second: 243, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10352/100000: episode: 16, duration: 1.594s, episode steps: 381, steps per second: 239, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10959/100000: episode: 17, duration: 2.540s, episode steps: 607, steps per second: 239, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12036/100000: episode: 18, duration: 5.987s, episode steps: 1077, steps per second: 180, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12635/100000: episode: 19, duration: 2.844s, episode steps: 599, steps per second: 211, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13540/100000: episode: 20, duration: 4.032s, episode steps: 905, steps per second: 224, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14483/100000: episode: 21, duration: 5.410s, episode steps: 943, steps per second: 174, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15062/100000: episode: 22, duration: 2.322s, episode steps: 579, steps per second: 249, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15606/100000: episode: 23, duration: 2.208s, episode steps: 544, steps per second: 246, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15955/100000: episode: 24, duration: 1.450s, episode steps: 349, steps per second: 241, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16459/100000: episode: 25, duration: 2.061s, episode steps: 504, steps per second: 245, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16851/100000: episode: 26, duration: 1.601s, episode steps: 392, steps per second: 245, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17374/100000: episode: 27, duration: 3.419s, episode steps: 523, steps per second: 153, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18124/100000: episode: 28, duration: 3.300s, episode steps: 750, steps per second: 227, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18648/100000: episode: 29, duration: 2.146s, episode steps: 524, steps per second: 244, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19143/100000: episode: 30, duration: 2.029s, episode steps: 495, steps per second: 244, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19669/100000: episode: 31, duration: 2.153s, episode steps: 526, steps per second: 244, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20568/100000: episode: 32, duration: 5.200s, episode steps: 899, steps per second: 173, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21323/100000: episode: 33, duration: 3.078s, episode steps: 755, steps per second: 245, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22439/100000: episode: 34, duration: 4.510s, episode steps: 1116, steps per second: 247, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23375/100000: episode: 35, duration: 5.065s, episode steps: 936, steps per second: 185, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23988/100000: episode: 36, duration: 2.659s, episode steps: 613, steps per second: 231, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25147/100000: episode: 37, duration: 5.217s, episode steps: 1159, steps per second: 222, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25854/100000: episode: 38, duration: 3.102s, episode steps: 707, steps per second: 228, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26380/100000: episode: 39, duration: 3.501s, episode steps: 526, steps per second: 150, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27451/100000: episode: 40, duration: 4.285s, episode steps: 1071, steps per second: 250, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28401/100000: episode: 41, duration: 3.695s, episode steps: 950, steps per second: 257, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29128/100000: episode: 42, duration: 3.580s, episode steps: 727, steps per second: 203, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29630/100000: episode: 43, duration: 2.885s, episode steps: 502, steps per second: 174, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30856/100000: episode: 44, duration: 4.893s, episode steps: 1226, steps per second: 251, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31340/100000: episode: 45, duration: 1.919s, episode steps: 484, steps per second: 252, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32049/100000: episode: 46, duration: 3.109s, episode steps: 709, steps per second: 228, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32558/100000: episode: 47, duration: 3.277s, episode steps: 509, steps per second: 155, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32962/100000: episode: 48, duration: 1.632s, episode steps: 404, steps per second: 248, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33439/100000: episode: 49, duration: 1.937s, episode steps: 477, steps per second: 246, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33848/100000: episode: 50, duration: 1.673s, episode steps: 409, steps per second: 244, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34897/100000: episode: 51, duration: 4.119s, episode steps: 1049, steps per second: 255, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35533/100000: episode: 52, duration: 4.093s, episode steps: 636, steps per second: 155, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36457/100000: episode: 53, duration: 3.767s, episode steps: 924, steps per second: 245, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37085/100000: episode: 54, duration: 2.541s, episode steps: 628, steps per second: 247, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37936/100000: episode: 55, duration: 3.504s, episode steps: 851, steps per second: 243, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38447/100000: episode: 56, duration: 3.432s, episode steps: 511, steps per second: 149, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39640/100000: episode: 57, duration: 5.273s, episode steps: 1193, steps per second: 226, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40390/100000: episode: 58, duration: 3.202s, episode steps: 750, steps per second: 234, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40936/100000: episode: 59, duration: 2.424s, episode steps: 546, steps per second: 225, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41428/100000: episode: 60, duration: 3.447s, episode steps: 492, steps per second: 143, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41938/100000: episode: 61, duration: 2.083s, episode steps: 510, steps per second: 245, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42510/100000: episode: 62, duration: 2.292s, episode steps: 572, steps per second: 250, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43022/100000: episode: 63, duration: 2.053s, episode steps: 512, steps per second: 249, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43438/100000: episode: 64, duration: 1.729s, episode steps: 416, steps per second: 241, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44003/100000: episode: 65, duration: 2.563s, episode steps: 565, steps per second: 220, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44943/100000: episode: 66, duration: 5.031s, episode steps: 940, steps per second: 187, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45402/100000: episode: 67, duration: 1.853s, episode steps: 459, steps per second: 248, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46532/100000: episode: 68, duration: 4.609s, episode steps: 1130, steps per second: 245, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47485/100000: episode: 69, duration: 5.362s, episode steps: 953, steps per second: 178, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47839/100000: episode: 70, duration: 1.462s, episode steps: 354, steps per second: 242, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48166/100000: episode: 71, duration: 1.332s, episode steps: 327, steps per second: 246, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48798/100000: episode: 72, duration: 2.651s, episode steps: 632, steps per second: 238, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49638/100000: episode: 73, duration: 3.530s, episode steps: 840, steps per second: 238, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 50030/100000: episode: 74, duration: 9.635s, episode steps: 392, steps per second:  41, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.006832, mae: 0.029681, mean_q: 0.036759, mean_eps: 0.774932\n",
            " 50803/100000: episode: 75, duration: 164.991s, episode steps: 773, steps per second:   5, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007734, mae: 0.033104, mean_q: 0.042827, mean_eps: 0.773128\n",
            " 51754/100000: episode: 76, duration: 206.767s, episode steps: 951, steps per second:   5, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.007015, mae: 0.029925, mean_q: 0.036879, mean_eps: 0.769249\n",
            " 52759/100000: episode: 77, duration: 213.899s, episode steps: 1005, steps per second:   5, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.007787, mae: 0.033912, mean_q: 0.042073, mean_eps: 0.764848\n",
            " 53514/100000: episode: 78, duration: 163.200s, episode steps: 755, steps per second:   5, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.007943, mae: 0.033837, mean_q: 0.042598, mean_eps: 0.760888\n",
            " 54356/100000: episode: 79, duration: 183.793s, episode steps: 842, steps per second:   5, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.007823, mae: 0.034534, mean_q: 0.045047, mean_eps: 0.757295\n",
            " 55060/100000: episode: 80, duration: 150.351s, episode steps: 704, steps per second:   5, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.007839, mae: 0.033094, mean_q: 0.043641, mean_eps: 0.753816\n",
            " 55678/100000: episode: 81, duration: 130.173s, episode steps: 618, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.007664, mae: 0.033011, mean_q: 0.044292, mean_eps: 0.750842\n",
            " 56196/100000: episode: 82, duration: 109.323s, episode steps: 518, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.006609, mae: 0.032812, mean_q: 0.044605, mean_eps: 0.748286\n",
            " 56847/100000: episode: 83, duration: 140.048s, episode steps: 651, steps per second:   5, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007480, mae: 0.033851, mean_q: 0.048151, mean_eps: 0.745655\n",
            " 57465/100000: episode: 84, duration: 131.809s, episode steps: 618, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.007150, mae: 0.031869, mean_q: 0.047173, mean_eps: 0.742800\n",
            " 58212/100000: episode: 85, duration: 158.078s, episode steps: 747, steps per second:   5, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.006995, mae: 0.031621, mean_q: 0.046789, mean_eps: 0.739729\n",
            " 58801/100000: episode: 86, duration: 125.563s, episode steps: 589, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.007590, mae: 0.032185, mean_q: 0.048866, mean_eps: 0.736723\n",
            " 59454/100000: episode: 87, duration: 137.776s, episode steps: 653, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.006474, mae: 0.030745, mean_q: 0.047785, mean_eps: 0.733928\n",
            " 60027/100000: episode: 88, duration: 120.892s, episode steps: 573, steps per second:   5, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.007322, mae: 0.034327, mean_q: 0.053638, mean_eps: 0.731170\n",
            " 60615/100000: episode: 89, duration: 124.733s, episode steps: 588, steps per second:   5, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.007370, mae: 0.061908, mean_q: 0.091439, mean_eps: 0.728558\n",
            " 61238/100000: episode: 90, duration: 133.189s, episode steps: 623, steps per second:   5, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.007394, mae: 0.060505, mean_q: 0.090061, mean_eps: 0.725833\n",
            " 61856/100000: episode: 91, duration: 130.133s, episode steps: 618, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.006734, mae: 0.058692, mean_q: 0.089085, mean_eps: 0.723041\n",
            " 62471/100000: episode: 92, duration: 131.425s, episode steps: 615, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.006821, mae: 0.059799, mean_q: 0.090066, mean_eps: 0.720267\n",
            " 62902/100000: episode: 93, duration: 92.628s, episode steps: 431, steps per second:   5, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.007377, mae: 0.060771, mean_q: 0.093850, mean_eps: 0.717913\n",
            " 63490/100000: episode: 94, duration: 124.670s, episode steps: 588, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006858, mae: 0.059200, mean_q: 0.091841, mean_eps: 0.715620\n",
            " 64304/100000: episode: 95, duration: 171.394s, episode steps: 814, steps per second:   5, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.006676, mae: 0.059360, mean_q: 0.094315, mean_eps: 0.712466\n",
            " 64712/100000: episode: 96, duration: 86.382s, episode steps: 408, steps per second:   5, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.006389, mae: 0.057901, mean_q: 0.091562, mean_eps: 0.709716\n",
            " 65379/100000: episode: 97, duration: 142.730s, episode steps: 667, steps per second:   5, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.006795, mae: 0.058775, mean_q: 0.093159, mean_eps: 0.707298\n",
            " 65793/100000: episode: 98, duration: 87.430s, episode steps: 414, steps per second:   5, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.007031, mae: 0.059694, mean_q: 0.095246, mean_eps: 0.704865\n",
            " 66319/100000: episode: 99, duration: 111.377s, episode steps: 526, steps per second:   5, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.006498, mae: 0.057485, mean_q: 0.092406, mean_eps: 0.702750\n",
            " 66734/100000: episode: 100, duration: 88.598s, episode steps: 415, steps per second:   5, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.006364, mae: 0.057818, mean_q: 0.094484, mean_eps: 0.700633\n",
            " 67268/100000: episode: 101, duration: 112.798s, episode steps: 534, steps per second:   5, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.005694, mae: 0.056102, mean_q: 0.090959, mean_eps: 0.698498\n",
            " 68435/100000: episode: 102, duration: 246.923s, episode steps: 1167, steps per second:   5, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.005792, mae: 0.056295, mean_q: 0.092429, mean_eps: 0.694671\n",
            " 69022/100000: episode: 103, duration: 123.709s, episode steps: 587, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.006037, mae: 0.056751, mean_q: 0.095542, mean_eps: 0.690724\n",
            " 69755/100000: episode: 104, duration: 155.180s, episode steps: 733, steps per second:   5, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.005823, mae: 0.056537, mean_q: 0.095478, mean_eps: 0.687754\n",
            " 70377/100000: episode: 105, duration: 133.923s, episode steps: 622, steps per second:   5, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.006798, mae: 0.069310, mean_q: 0.115537, mean_eps: 0.684705\n",
            " 71523/100000: episode: 106, duration: 243.235s, episode steps: 1146, steps per second:   5, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.006906, mae: 0.076370, mean_q: 0.122650, mean_eps: 0.680727\n",
            " 72141/100000: episode: 107, duration: 131.555s, episode steps: 618, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.006473, mae: 0.074461, mean_q: 0.116947, mean_eps: 0.676758\n",
            " 72822/100000: episode: 108, duration: 143.392s, episode steps: 681, steps per second:   5, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.005980, mae: 0.072442, mean_q: 0.115583, mean_eps: 0.673835\n",
            " 73979/100000: episode: 109, duration: 244.630s, episode steps: 1157, steps per second:   5, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.006004, mae: 0.073967, mean_q: 0.117022, mean_eps: 0.669700\n",
            " 74684/100000: episode: 110, duration: 148.941s, episode steps: 705, steps per second:   5, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.005645, mae: 0.071746, mean_q: 0.116521, mean_eps: 0.665510\n",
            " 75239/100000: episode: 111, duration: 117.972s, episode steps: 555, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.005820, mae: 0.073286, mean_q: 0.119067, mean_eps: 0.662675\n",
            " 75791/100000: episode: 112, duration: 116.821s, episode steps: 552, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.005405, mae: 0.071656, mean_q: 0.115722, mean_eps: 0.660185\n",
            " 76611/100000: episode: 113, duration: 174.936s, episode steps: 820, steps per second:   5, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.005341, mae: 0.071126, mean_q: 0.116503, mean_eps: 0.657098\n",
            " 77268/100000: episode: 114, duration: 139.254s, episode steps: 657, steps per second:   5, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.005077, mae: 0.069755, mean_q: 0.113111, mean_eps: 0.653774\n",
            " 77967/100000: episode: 115, duration: 149.090s, episode steps: 699, steps per second:   5, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.005202, mae: 0.069464, mean_q: 0.113048, mean_eps: 0.650724\n",
            " 78617/100000: episode: 116, duration: 138.911s, episode steps: 650, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.005709, mae: 0.070880, mean_q: 0.117170, mean_eps: 0.647688\n",
            " 79272/100000: episode: 117, duration: 139.030s, episode steps: 655, steps per second:   5, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.004840, mae: 0.069152, mean_q: 0.114125, mean_eps: 0.644752\n",
            " 79643/100000: episode: 118, duration: 80.367s, episode steps: 371, steps per second:   5, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.005064, mae: 0.068821, mean_q: 0.114009, mean_eps: 0.642444\n",
            " 80024/100000: episode: 119, duration: 81.037s, episode steps: 381, steps per second:   5, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.005746, mae: 0.070843, mean_q: 0.117790, mean_eps: 0.640752\n",
            " 80685/100000: episode: 120, duration: 140.765s, episode steps: 661, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.007657, mae: 0.112152, mean_q: 0.173189, mean_eps: 0.638407\n",
            " 81339/100000: episode: 121, duration: 138.678s, episode steps: 654, steps per second:   5, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.006843, mae: 0.108298, mean_q: 0.163918, mean_eps: 0.635448\n",
            " 82473/100000: episode: 122, duration: 240.510s, episode steps: 1134, steps per second:   5, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.006362, mae: 0.106865, mean_q: 0.160843, mean_eps: 0.631425\n",
            " 83280/100000: episode: 123, duration: 172.587s, episode steps: 807, steps per second:   5, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.005964, mae: 0.107144, mean_q: 0.161302, mean_eps: 0.627058\n",
            " 83918/100000: episode: 124, duration: 136.750s, episode steps: 638, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.005579, mae: 0.103890, mean_q: 0.155684, mean_eps: 0.623807\n",
            " 84540/100000: episode: 125, duration: 131.485s, episode steps: 622, steps per second:   5, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.005516, mae: 0.105548, mean_q: 0.158385, mean_eps: 0.620972\n",
            " 84938/100000: episode: 126, duration: 86.709s, episode steps: 398, steps per second:   5, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.005476, mae: 0.104155, mean_q: 0.157566, mean_eps: 0.618677\n",
            " 85613/100000: episode: 127, duration: 144.368s, episode steps: 675, steps per second:   5, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.005659, mae: 0.104365, mean_q: 0.157253, mean_eps: 0.616263\n",
            " 86120/100000: episode: 128, duration: 108.040s, episode steps: 507, steps per second:   5, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.005308, mae: 0.104297, mean_q: 0.158365, mean_eps: 0.613603\n",
            " 87051/100000: episode: 129, duration: 196.729s, episode steps: 931, steps per second:   5, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.005092, mae: 0.102914, mean_q: 0.155827, mean_eps: 0.610367\n",
            " 87807/100000: episode: 130, duration: 160.863s, episode steps: 756, steps per second:   5, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.004875, mae: 0.100295, mean_q: 0.151943, mean_eps: 0.606572\n",
            " 88631/100000: episode: 131, duration: 175.579s, episode steps: 824, steps per second:   5, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.004893, mae: 0.103069, mean_q: 0.158380, mean_eps: 0.603017\n",
            " 89212/100000: episode: 132, duration: 124.419s, episode steps: 581, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.004483, mae: 0.101086, mean_q: 0.154980, mean_eps: 0.599855\n",
            " 89727/100000: episode: 133, duration: 109.754s, episode steps: 515, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.005505, mae: 0.103727, mean_q: 0.159652, mean_eps: 0.597389\n",
            " 90747/100000: episode: 134, duration: 218.440s, episode steps: 1020, steps per second:   5, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.007120, mae: 0.127459, mean_q: 0.190019, mean_eps: 0.593936\n",
            " 91376/100000: episode: 135, duration: 132.724s, episode steps: 629, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.007242, mae: 0.135529, mean_q: 0.197823, mean_eps: 0.590225\n",
            " 92241/100000: episode: 136, duration: 186.864s, episode steps: 865, steps per second:   5, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.006326, mae: 0.135754, mean_q: 0.200029, mean_eps: 0.586864\n",
            " 93040/100000: episode: 137, duration: 170.195s, episode steps: 799, steps per second:   5, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.006043, mae: 0.133749, mean_q: 0.196095, mean_eps: 0.583120\n",
            " 93679/100000: episode: 138, duration: 136.119s, episode steps: 639, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.006244, mae: 0.136710, mean_q: 0.201587, mean_eps: 0.579885\n",
            " 94429/100000: episode: 139, duration: 161.209s, episode steps: 750, steps per second:   5, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.006218, mae: 0.132253, mean_q: 0.194958, mean_eps: 0.576759\n",
            " 95054/100000: episode: 140, duration: 133.776s, episode steps: 625, steps per second:   5, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.005758, mae: 0.132040, mean_q: 0.195056, mean_eps: 0.573665\n",
            " 96229/100000: episode: 141, duration: 251.083s, episode steps: 1175, steps per second:   5, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.005369, mae: 0.130214, mean_q: 0.193011, mean_eps: 0.569616\n",
            " 96978/100000: episode: 142, duration: 160.727s, episode steps: 749, steps per second:   5, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.005578, mae: 0.129142, mean_q: 0.191024, mean_eps: 0.565287\n",
            " 97380/100000: episode: 143, duration: 85.925s, episode steps: 402, steps per second:   5, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.005212, mae: 0.125426, mean_q: 0.186017, mean_eps: 0.562697\n",
            " 98038/100000: episode: 144, duration: 140.441s, episode steps: 658, steps per second:   5, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.005570, mae: 0.129543, mean_q: 0.191975, mean_eps: 0.560312\n",
            " 98697/100000: episode: 145, duration: 141.185s, episode steps: 659, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.005437, mae: 0.128887, mean_q: 0.190554, mean_eps: 0.557348\n",
            " 99030/100000: episode: 146, duration: 72.470s, episode steps: 333, steps per second:   5, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.004832, mae: 0.126607, mean_q: 0.189476, mean_eps: 0.555117\n",
            " 99616/100000: episode: 147, duration: 126.612s, episode steps: 586, steps per second:   5, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.004754, mae: 0.125222, mean_q: 0.186114, mean_eps: 0.553049\n",
            "done, took 10895.285 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x781e784d5490>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.load_weights(\"dqn_weights_100000.h5f\")\n",
        "\n",
        "# Test sin visualizar\n",
        "test_episodes = 10\n",
        "results = dqn.test(env, nb_episodes=test_episodes, visualize=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1oxC2S-w6Ws",
        "outputId": "972cc29a-f76f-4a49-e068-5ec87f0fabb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 8.000, steps: 575\n",
            "Episode 2: reward: 11.000, steps: 775\n",
            "Episode 3: reward: 9.000, steps: 821\n",
            "Episode 4: reward: 7.000, steps: 662\n",
            "Episode 5: reward: 17.000, steps: 1113\n",
            "Episode 6: reward: 7.000, steps: 749\n",
            "Episode 7: reward: 6.000, steps: 593\n",
            "Episode 8: reward: 10.000, steps: 697\n",
            "Episode 9: reward: 13.000, steps: 842\n",
            "Episode 10: reward: 6.000, steps: 537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 + 100000 = 525000\n",
        "\n",
        "# Paso de partida 3\n",
        "starting_step = 100000 #ultimo weight actualizado\n",
        "remaining_steps = 200000 - starting_step\n",
        "\n",
        "\n",
        "# Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaqcU9Ulx8-D",
        "outputId": "22e982b4-7c7f-4c6b-ced3-c8aeb4c8b7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "  1151/100000: episode: 1, duration: 6.220s, episode steps: 1151, steps per second: 185, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1817/100000: episode: 2, duration: 2.648s, episode steps: 666, steps per second: 252, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2485/100000: episode: 3, duration: 2.714s, episode steps: 668, steps per second: 246, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3296/100000: episode: 4, duration: 3.234s, episode steps: 811, steps per second: 251, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4126/100000: episode: 5, duration: 4.890s, episode steps: 830, steps per second: 170, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5026/100000: episode: 6, duration: 3.613s, episode steps: 900, steps per second: 249, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5943/100000: episode: 7, duration: 3.711s, episode steps: 917, steps per second: 247, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6470/100000: episode: 8, duration: 2.427s, episode steps: 527, steps per second: 217, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7184/100000: episode: 9, duration: 4.009s, episode steps: 714, steps per second: 178, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7823/100000: episode: 10, duration: 2.622s, episode steps: 639, steps per second: 244, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8670/100000: episode: 11, duration: 3.466s, episode steps: 847, steps per second: 244, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9191/100000: episode: 12, duration: 2.094s, episode steps: 521, steps per second: 249, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9780/100000: episode: 13, duration: 3.415s, episode steps: 589, steps per second: 172, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10295/100000: episode: 14, duration: 2.532s, episode steps: 515, steps per second: 203, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11073/100000: episode: 15, duration: 3.112s, episode steps: 778, steps per second: 250, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11586/100000: episode: 16, duration: 2.075s, episode steps: 513, steps per second: 247, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12279/100000: episode: 17, duration: 2.768s, episode steps: 693, steps per second: 250, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12853/100000: episode: 18, duration: 3.431s, episode steps: 574, steps per second: 167, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13641/100000: episode: 19, duration: 3.543s, episode steps: 788, steps per second: 222, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14343/100000: episode: 20, duration: 2.873s, episode steps: 702, steps per second: 244, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15226/100000: episode: 21, duration: 3.559s, episode steps: 883, steps per second: 248, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15890/100000: episode: 22, duration: 3.689s, episode steps: 664, steps per second: 180, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16471/100000: episode: 23, duration: 2.805s, episode steps: 581, steps per second: 207, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16969/100000: episode: 24, duration: 1.983s, episode steps: 498, steps per second: 251, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17817/100000: episode: 25, duration: 3.388s, episode steps: 848, steps per second: 250, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18666/100000: episode: 26, duration: 3.647s, episode steps: 849, steps per second: 233, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19398/100000: episode: 27, duration: 4.152s, episode steps: 732, steps per second: 176, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20181/100000: episode: 28, duration: 3.102s, episode steps: 783, steps per second: 252, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20907/100000: episode: 29, duration: 2.899s, episode steps: 726, steps per second: 250, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21562/100000: episode: 30, duration: 2.669s, episode steps: 655, steps per second: 245, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22595/100000: episode: 31, duration: 5.683s, episode steps: 1033, steps per second: 182, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23271/100000: episode: 32, duration: 2.791s, episode steps: 676, steps per second: 242, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24329/100000: episode: 33, duration: 4.315s, episode steps: 1058, steps per second: 245, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24982/100000: episode: 34, duration: 3.675s, episode steps: 653, steps per second: 178, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25512/100000: episode: 35, duration: 2.664s, episode steps: 530, steps per second: 199, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26148/100000: episode: 36, duration: 2.540s, episode steps: 636, steps per second: 250, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26633/100000: episode: 37, duration: 1.959s, episode steps: 485, steps per second: 248, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27529/100000: episode: 38, duration: 3.635s, episode steps: 896, steps per second: 246, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28210/100000: episode: 39, duration: 4.280s, episode steps: 681, steps per second: 159, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28829/100000: episode: 40, duration: 2.513s, episode steps: 619, steps per second: 246, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29383/100000: episode: 41, duration: 2.251s, episode steps: 554, steps per second: 246, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29768/100000: episode: 42, duration: 1.558s, episode steps: 385, steps per second: 247, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30434/100000: episode: 43, duration: 2.708s, episode steps: 666, steps per second: 246, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31236/100000: episode: 44, duration: 4.739s, episode steps: 802, steps per second: 169, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31867/100000: episode: 45, duration: 2.505s, episode steps: 631, steps per second: 252, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32599/100000: episode: 46, duration: 2.924s, episode steps: 732, steps per second: 250, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33096/100000: episode: 47, duration: 1.982s, episode steps: 497, steps per second: 251, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33724/100000: episode: 48, duration: 2.471s, episode steps: 628, steps per second: 254, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34455/100000: episode: 49, duration: 4.973s, episode steps: 731, steps per second: 147, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34803/100000: episode: 50, duration: 1.385s, episode steps: 348, steps per second: 251, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35481/100000: episode: 51, duration: 2.726s, episode steps: 678, steps per second: 249, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35979/100000: episode: 52, duration: 1.992s, episode steps: 498, steps per second: 250, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36684/100000: episode: 53, duration: 2.850s, episode steps: 705, steps per second: 247, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37444/100000: episode: 54, duration: 4.543s, episode steps: 760, steps per second: 167, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38229/100000: episode: 55, duration: 3.209s, episode steps: 785, steps per second: 245, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39121/100000: episode: 56, duration: 3.548s, episode steps: 892, steps per second: 251, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39843/100000: episode: 57, duration: 3.223s, episode steps: 722, steps per second: 224, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40484/100000: episode: 58, duration: 3.763s, episode steps: 641, steps per second: 170, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41171/100000: episode: 59, duration: 2.800s, episode steps: 687, steps per second: 245, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41619/100000: episode: 60, duration: 1.774s, episode steps: 448, steps per second: 253, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42285/100000: episode: 61, duration: 2.672s, episode steps: 666, steps per second: 249, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42864/100000: episode: 62, duration: 2.595s, episode steps: 579, steps per second: 223, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43782/100000: episode: 63, duration: 4.888s, episode steps: 918, steps per second: 188, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44460/100000: episode: 64, duration: 2.720s, episode steps: 678, steps per second: 249, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44857/100000: episode: 65, duration: 1.617s, episode steps: 397, steps per second: 245, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45249/100000: episode: 66, duration: 1.583s, episode steps: 392, steps per second: 248, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46668/100000: episode: 67, duration: 7.294s, episode steps: 1419, steps per second: 195, episode reward: 15.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47674/100000: episode: 68, duration: 4.213s, episode steps: 1006, steps per second: 239, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48778/100000: episode: 69, duration: 4.564s, episode steps: 1104, steps per second: 242, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49202/100000: episode: 70, duration: 2.888s, episode steps: 424, steps per second: 147, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49806/100000: episode: 71, duration: 2.740s, episode steps: 604, steps per second: 220, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 50961/100000: episode: 72, duration: 206.040s, episode steps: 1155, steps per second:   6, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.009507, mae: 0.142794, mean_q: 0.191693, mean_eps: 0.772838\n",
            " 51394/100000: episode: 73, duration: 91.128s, episode steps: 433, steps per second:   5, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009900, mae: 0.142716, mean_q: 0.187388, mean_eps: 0.769703\n",
            " 52317/100000: episode: 74, duration: 197.634s, episode steps: 923, steps per second:   5, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008665, mae: 0.138493, mean_q: 0.182857, mean_eps: 0.766652\n",
            " 52924/100000: episode: 75, duration: 128.235s, episode steps: 607, steps per second:   5, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.007913, mae: 0.136531, mean_q: 0.179270, mean_eps: 0.763210\n",
            " 53443/100000: episode: 76, duration: 110.271s, episode steps: 519, steps per second:   5, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008348, mae: 0.136205, mean_q: 0.179042, mean_eps: 0.760676\n",
            " 54359/100000: episode: 77, duration: 196.538s, episode steps: 916, steps per second:   5, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.007539, mae: 0.133881, mean_q: 0.175251, mean_eps: 0.757448\n",
            " 55112/100000: episode: 78, duration: 160.836s, episode steps: 753, steps per second:   5, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.007690, mae: 0.134746, mean_q: 0.177379, mean_eps: 0.753692\n",
            " 55922/100000: episode: 79, duration: 172.342s, episode steps: 810, steps per second:   5, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.008267, mae: 0.136366, mean_q: 0.179187, mean_eps: 0.750176\n",
            " 56489/100000: episode: 80, duration: 120.743s, episode steps: 567, steps per second:   5, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.007204, mae: 0.133424, mean_q: 0.175598, mean_eps: 0.747078\n",
            " 57147/100000: episode: 81, duration: 141.645s, episode steps: 658, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.007730, mae: 0.133086, mean_q: 0.176268, mean_eps: 0.744321\n",
            " 57505/100000: episode: 82, duration: 76.384s, episode steps: 358, steps per second:   5, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.007002, mae: 0.131415, mean_q: 0.174969, mean_eps: 0.742035\n",
            " 58299/100000: episode: 83, duration: 170.337s, episode steps: 794, steps per second:   5, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007051, mae: 0.133393, mean_q: 0.178411, mean_eps: 0.739443\n",
            " 59001/100000: episode: 84, duration: 150.197s, episode steps: 702, steps per second:   5, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.006626, mae: 0.129014, mean_q: 0.172761, mean_eps: 0.736077\n",
            " 59537/100000: episode: 85, duration: 116.211s, episode steps: 536, steps per second:   5, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.006974, mae: 0.131129, mean_q: 0.176448, mean_eps: 0.733292\n",
            " 60330/100000: episode: 86, duration: 169.009s, episode steps: 793, steps per second:   5, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.007104, mae: 0.143868, mean_q: 0.192489, mean_eps: 0.730302\n",
            " 61288/100000: episode: 87, duration: 207.530s, episode steps: 958, steps per second:   5, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.007925, mae: 0.168113, mean_q: 0.222502, mean_eps: 0.726362\n",
            " 62209/100000: episode: 88, duration: 196.762s, episode steps: 921, steps per second:   5, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.007559, mae: 0.170193, mean_q: 0.226215, mean_eps: 0.722134\n",
            " 62871/100000: episode: 89, duration: 142.640s, episode steps: 662, steps per second:   5, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.007120, mae: 0.165663, mean_q: 0.220371, mean_eps: 0.718572\n",
            " 63279/100000: episode: 90, duration: 86.701s, episode steps: 408, steps per second:   5, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.006668, mae: 0.168749, mean_q: 0.225432, mean_eps: 0.716165\n",
            " 63974/100000: episode: 91, duration: 149.398s, episode steps: 695, steps per second:   5, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.006525, mae: 0.164611, mean_q: 0.220188, mean_eps: 0.713683\n",
            " 64757/100000: episode: 92, duration: 167.225s, episode steps: 783, steps per second:   5, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.006673, mae: 0.166710, mean_q: 0.223686, mean_eps: 0.710358\n",
            " 65313/100000: episode: 93, duration: 119.631s, episode steps: 556, steps per second:   5, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.006996, mae: 0.167447, mean_q: 0.226110, mean_eps: 0.707345\n",
            " 65831/100000: episode: 94, duration: 110.594s, episode steps: 518, steps per second:   5, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.005686, mae: 0.164366, mean_q: 0.223012, mean_eps: 0.704928\n",
            " 66237/100000: episode: 95, duration: 86.598s, episode steps: 406, steps per second:   5, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.006398, mae: 0.165852, mean_q: 0.224372, mean_eps: 0.702849\n",
            " 67009/100000: episode: 96, duration: 164.764s, episode steps: 772, steps per second:   5, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.006091, mae: 0.163230, mean_q: 0.220851, mean_eps: 0.700199\n",
            " 67709/100000: episode: 97, duration: 149.662s, episode steps: 700, steps per second:   5, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.005928, mae: 0.162627, mean_q: 0.220117, mean_eps: 0.696887\n",
            " 68246/100000: episode: 98, duration: 114.245s, episode steps: 537, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.005837, mae: 0.164489, mean_q: 0.223385, mean_eps: 0.694103\n",
            " 69170/100000: episode: 99, duration: 198.521s, episode steps: 924, steps per second:   5, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006107, mae: 0.166259, mean_q: 0.226316, mean_eps: 0.690816\n",
            " 70092/100000: episode: 100, duration: 196.435s, episode steps: 922, steps per second:   5, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.005450, mae: 0.165156, mean_q: 0.225591, mean_eps: 0.686663\n",
            " 70634/100000: episode: 101, duration: 114.990s, episode steps: 542, steps per second:   5, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.006978, mae: 0.187698, mean_q: 0.253157, mean_eps: 0.683369\n",
            " 71011/100000: episode: 102, duration: 80.880s, episode steps: 377, steps per second:   5, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006482, mae: 0.186147, mean_q: 0.250727, mean_eps: 0.681301\n",
            " 72200/100000: episode: 103, duration: 254.846s, episode steps: 1189, steps per second:   5, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.006597, mae: 0.188212, mean_q: 0.254413, mean_eps: 0.677778\n",
            " 72953/100000: episode: 104, duration: 160.280s, episode steps: 753, steps per second:   5, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.005888, mae: 0.186177, mean_q: 0.252591, mean_eps: 0.673408\n",
            " 73588/100000: episode: 105, duration: 135.209s, episode steps: 635, steps per second:   5, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.006076, mae: 0.187220, mean_q: 0.252966, mean_eps: 0.670285\n",
            " 74089/100000: episode: 106, duration: 107.722s, episode steps: 501, steps per second:   5, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.005390, mae: 0.185422, mean_q: 0.250770, mean_eps: 0.667729\n",
            " 75337/100000: episode: 107, duration: 267.787s, episode steps: 1248, steps per second:   5, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.005640, mae: 0.185342, mean_q: 0.251493, mean_eps: 0.663794\n",
            " 75968/100000: episode: 108, duration: 133.808s, episode steps: 631, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.005525, mae: 0.184002, mean_q: 0.249863, mean_eps: 0.659566\n",
            " 76630/100000: episode: 109, duration: 142.684s, episode steps: 662, steps per second:   5, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.005719, mae: 0.186104, mean_q: 0.253687, mean_eps: 0.656657\n",
            " 77049/100000: episode: 110, duration: 88.431s, episode steps: 419, steps per second:   5, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.004700, mae: 0.181404, mean_q: 0.246810, mean_eps: 0.654224\n",
            " 77439/100000: episode: 111, duration: 83.184s, episode steps: 390, steps per second:   5, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.005239, mae: 0.185001, mean_q: 0.251764, mean_eps: 0.652404\n",
            " 78245/100000: episode: 112, duration: 172.267s, episode steps: 806, steps per second:   5, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.004708, mae: 0.181699, mean_q: 0.248487, mean_eps: 0.649713\n",
            " 78969/100000: episode: 113, duration: 154.243s, episode steps: 724, steps per second:   5, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.005120, mae: 0.181744, mean_q: 0.248657, mean_eps: 0.646271\n",
            " 79396/100000: episode: 114, duration: 92.780s, episode steps: 427, steps per second:   5, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.004959, mae: 0.182118, mean_q: 0.249774, mean_eps: 0.643681\n",
            " 80091/100000: episode: 115, duration: 147.206s, episode steps: 695, steps per second:   5, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.004860, mae: 0.186235, mean_q: 0.254685, mean_eps: 0.641157\n",
            " 80591/100000: episode: 116, duration: 107.116s, episode steps: 500, steps per second:   5, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.006874, mae: 0.222587, mean_q: 0.301225, mean_eps: 0.638468\n",
            " 80963/100000: episode: 117, duration: 79.699s, episode steps: 372, steps per second:   5, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.006646, mae: 0.222013, mean_q: 0.300044, mean_eps: 0.636506\n",
            " 81914/100000: episode: 118, duration: 203.219s, episode steps: 951, steps per second:   5, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.005730, mae: 0.216206, mean_q: 0.291231, mean_eps: 0.633529\n",
            " 82324/100000: episode: 119, duration: 88.930s, episode steps: 410, steps per second:   5, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.005965, mae: 0.216691, mean_q: 0.292121, mean_eps: 0.630467\n",
            " 83397/100000: episode: 120, duration: 230.359s, episode steps: 1073, steps per second:   5, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.005801, mae: 0.217807, mean_q: 0.293692, mean_eps: 0.627130\n",
            " 84176/100000: episode: 121, duration: 165.926s, episode steps: 779, steps per second:   5, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.005523, mae: 0.218104, mean_q: 0.293060, mean_eps: 0.622963\n",
            " 84819/100000: episode: 122, duration: 137.148s, episode steps: 643, steps per second:   5, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.005305, mae: 0.218392, mean_q: 0.294749, mean_eps: 0.619763\n",
            " 85199/100000: episode: 123, duration: 81.179s, episode steps: 380, steps per second:   5, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.005410, mae: 0.217906, mean_q: 0.293188, mean_eps: 0.617462\n",
            " 85711/100000: episode: 124, duration: 109.664s, episode steps: 512, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.005476, mae: 0.216335, mean_q: 0.291987, mean_eps: 0.615455\n",
            " 86301/100000: episode: 125, duration: 125.711s, episode steps: 590, steps per second:   5, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.004979, mae: 0.216462, mean_q: 0.292042, mean_eps: 0.612975\n",
            " 87298/100000: episode: 126, duration: 214.145s, episode steps: 997, steps per second:   5, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.005264, mae: 0.216857, mean_q: 0.292906, mean_eps: 0.609405\n",
            " 88032/100000: episode: 127, duration: 156.423s, episode steps: 734, steps per second:   5, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.005093, mae: 0.216660, mean_q: 0.292376, mean_eps: 0.605510\n",
            " 89130/100000: episode: 128, duration: 235.127s, episode steps: 1098, steps per second:   5, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.004779, mae: 0.216081, mean_q: 0.292436, mean_eps: 0.601388\n",
            " 89795/100000: episode: 129, duration: 141.356s, episode steps: 665, steps per second:   5, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.004516, mae: 0.215396, mean_q: 0.291649, mean_eps: 0.597421\n",
            " 90465/100000: episode: 130, duration: 143.920s, episode steps: 670, steps per second:   5, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.006831, mae: 0.240550, mean_q: 0.323992, mean_eps: 0.594417\n",
            " 91051/100000: episode: 131, duration: 126.623s, episode steps: 586, steps per second:   5, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.007319, mae: 0.254503, mean_q: 0.342590, mean_eps: 0.591591\n",
            " 91428/100000: episode: 132, duration: 81.095s, episode steps: 377, steps per second:   5, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.006948, mae: 0.253420, mean_q: 0.340698, mean_eps: 0.589425\n",
            " 92075/100000: episode: 133, duration: 137.926s, episode steps: 647, steps per second:   5, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.006482, mae: 0.251277, mean_q: 0.336819, mean_eps: 0.587121\n",
            " 92518/100000: episode: 134, duration: 94.799s, episode steps: 443, steps per second:   5, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.006137, mae: 0.253020, mean_q: 0.338832, mean_eps: 0.584668\n",
            " 92944/100000: episode: 135, duration: 90.087s, episode steps: 426, steps per second:   5, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.006179, mae: 0.252472, mean_q: 0.338482, mean_eps: 0.582713\n",
            " 93907/100000: episode: 136, duration: 205.181s, episode steps: 963, steps per second:   5, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.005704, mae: 0.249807, mean_q: 0.334407, mean_eps: 0.579588\n",
            " 94434/100000: episode: 137, duration: 113.199s, episode steps: 527, steps per second:   5, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.005417, mae: 0.251953, mean_q: 0.336536, mean_eps: 0.576235\n",
            " 94971/100000: episode: 138, duration: 114.407s, episode steps: 537, steps per second:   5, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.005380, mae: 0.251983, mean_q: 0.336005, mean_eps: 0.573841\n",
            " 95463/100000: episode: 139, duration: 104.213s, episode steps: 492, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.005761, mae: 0.249546, mean_q: 0.333096, mean_eps: 0.571526\n",
            " 96595/100000: episode: 140, duration: 240.883s, episode steps: 1132, steps per second:   5, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.005120, mae: 0.248824, mean_q: 0.332357, mean_eps: 0.567872\n",
            " 97302/100000: episode: 141, duration: 150.626s, episode steps: 707, steps per second:   5, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.005777, mae: 0.247014, mean_q: 0.329896, mean_eps: 0.563734\n",
            " 97842/100000: episode: 142, duration: 114.355s, episode steps: 540, steps per second:   5, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.005362, mae: 0.248260, mean_q: 0.332156, mean_eps: 0.560928\n",
            " 98781/100000: episode: 143, duration: 200.539s, episode steps: 939, steps per second:   5, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.005268, mae: 0.250764, mean_q: 0.335627, mean_eps: 0.557601\n",
            " 99443/100000: episode: 144, duration: 140.451s, episode steps: 662, steps per second:   5, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.004866, mae: 0.248900, mean_q: 0.332480, mean_eps: 0.553998\n",
            "done, took 10915.394 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x781e72853d50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.load_weights(\"dqn_weights_100000.h5f\")\n",
        "test_episodes = 10\n",
        "results = dqn.test(env, nb_episodes=test_episodes, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng-v5wQnkec0",
        "outputId": "c2db62d9-fe02-4e03-ad40-36e141f51519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 4.000, steps: 396\n",
            "Episode 2: reward: 4.000, steps: 411\n",
            "Episode 3: reward: 14.000, steps: 903\n",
            "Episode 4: reward: 23.000, steps: 1126\n",
            "Episode 5: reward: 10.000, steps: 697\n",
            "Episode 6: reward: 10.000, steps: 618\n",
            "Episode 7: reward: 12.000, steps: 682\n",
            "Episode 8: reward: 6.000, steps: 404\n",
            "Episode 9: reward: 6.000, steps: 570\n",
            "Episode 10: reward: 9.000, steps: 484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 + 100000+100000 = 625000\n",
        "\n",
        "dqn.load_weights(\"dqn_weights_100000.h5f\")\n",
        "# Paso de partida 3\n",
        "starting_step = 100000 #ultimo weight actualizado\n",
        "remaining_steps = 200000 - starting_step\n",
        "\n",
        "\n",
        "# Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irGA8TqOlD3z",
        "outputId": "cd396f12-13f5-4772-a488-62cec68ac1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "   610/100000: episode: 1, duration: 2.797s, episode steps: 610, steps per second: 218, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   997/100000: episode: 2, duration: 2.538s, episode steps: 387, steps per second: 152, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1729/100000: episode: 3, duration: 2.987s, episode steps: 732, steps per second: 245, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2272/100000: episode: 4, duration: 2.140s, episode steps: 543, steps per second: 254, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3108/100000: episode: 5, duration: 3.250s, episode steps: 836, steps per second: 257, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3942/100000: episode: 6, duration: 4.200s, episode steps: 834, steps per second: 199, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4665/100000: episode: 7, duration: 3.401s, episode steps: 723, steps per second: 213, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5498/100000: episode: 8, duration: 3.296s, episode steps: 833, steps per second: 253, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5906/100000: episode: 9, duration: 1.645s, episode steps: 408, steps per second: 248, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6384/100000: episode: 10, duration: 1.925s, episode steps: 478, steps per second: 248, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7349/100000: episode: 11, duration: 5.212s, episode steps: 965, steps per second: 185, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8149/100000: episode: 12, duration: 3.129s, episode steps: 800, steps per second: 256, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8956/100000: episode: 13, duration: 3.189s, episode steps: 807, steps per second: 253, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9368/100000: episode: 14, duration: 1.645s, episode steps: 412, steps per second: 251, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9908/100000: episode: 15, duration: 2.451s, episode steps: 540, steps per second: 220, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10465/100000: episode: 16, duration: 3.302s, episode steps: 557, steps per second: 169, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11014/100000: episode: 17, duration: 2.154s, episode steps: 549, steps per second: 255, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12016/100000: episode: 18, duration: 3.944s, episode steps: 1002, steps per second: 254, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12546/100000: episode: 19, duration: 2.076s, episode steps: 530, steps per second: 255, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13305/100000: episode: 20, duration: 3.967s, episode steps: 759, steps per second: 191, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13988/100000: episode: 21, duration: 3.171s, episode steps: 683, steps per second: 215, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14532/100000: episode: 22, duration: 2.135s, episode steps: 544, steps per second: 255, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15313/100000: episode: 23, duration: 3.054s, episode steps: 781, steps per second: 256, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16405/100000: episode: 24, duration: 5.146s, episode steps: 1092, steps per second: 212, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17189/100000: episode: 25, duration: 3.653s, episode steps: 784, steps per second: 215, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18234/100000: episode: 26, duration: 4.098s, episode steps: 1045, steps per second: 255, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18980/100000: episode: 27, duration: 2.966s, episode steps: 746, steps per second: 251, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19463/100000: episode: 28, duration: 2.674s, episode steps: 483, steps per second: 181, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20113/100000: episode: 29, duration: 3.311s, episode steps: 650, steps per second: 196, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20628/100000: episode: 30, duration: 2.044s, episode steps: 515, steps per second: 252, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21944/100000: episode: 31, duration: 5.133s, episode steps: 1316, steps per second: 256, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22336/100000: episode: 32, duration: 1.696s, episode steps: 392, steps per second: 231, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22747/100000: episode: 33, duration: 2.636s, episode steps: 411, steps per second: 156, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23259/100000: episode: 34, duration: 2.319s, episode steps: 512, steps per second: 221, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23810/100000: episode: 35, duration: 2.195s, episode steps: 551, steps per second: 251, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24338/100000: episode: 36, duration: 2.128s, episode steps: 528, steps per second: 248, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24983/100000: episode: 37, duration: 2.578s, episode steps: 645, steps per second: 250, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25680/100000: episode: 38, duration: 3.780s, episode steps: 697, steps per second: 184, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26354/100000: episode: 39, duration: 3.297s, episode steps: 674, steps per second: 204, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27398/100000: episode: 40, duration: 4.260s, episode steps: 1044, steps per second: 245, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28565/100000: episode: 41, duration: 5.218s, episode steps: 1167, steps per second: 224, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29221/100000: episode: 42, duration: 3.522s, episode steps: 656, steps per second: 186, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30542/100000: episode: 43, duration: 5.263s, episode steps: 1321, steps per second: 251, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30957/100000: episode: 44, duration: 1.663s, episode steps: 415, steps per second: 249, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31492/100000: episode: 45, duration: 2.281s, episode steps: 535, steps per second: 234, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32040/100000: episode: 46, duration: 3.506s, episode steps: 548, steps per second: 156, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32441/100000: episode: 47, duration: 1.586s, episode steps: 401, steps per second: 253, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33218/100000: episode: 48, duration: 3.084s, episode steps: 777, steps per second: 252, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33905/100000: episode: 49, duration: 2.755s, episode steps: 687, steps per second: 249, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34676/100000: episode: 50, duration: 3.475s, episode steps: 771, steps per second: 222, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35088/100000: episode: 51, duration: 2.686s, episode steps: 412, steps per second: 153, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35714/100000: episode: 52, duration: 2.509s, episode steps: 626, steps per second: 249, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36376/100000: episode: 53, duration: 2.622s, episode steps: 662, steps per second: 252, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36996/100000: episode: 54, duration: 2.466s, episode steps: 620, steps per second: 251, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37539/100000: episode: 55, duration: 2.206s, episode steps: 543, steps per second: 246, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38068/100000: episode: 56, duration: 3.304s, episode steps: 529, steps per second: 160, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38600/100000: episode: 57, duration: 2.354s, episode steps: 532, steps per second: 226, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39248/100000: episode: 58, duration: 2.573s, episode steps: 648, steps per second: 252, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39853/100000: episode: 59, duration: 2.394s, episode steps: 605, steps per second: 253, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41035/100000: episode: 60, duration: 5.578s, episode steps: 1182, steps per second: 212, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41545/100000: episode: 61, duration: 2.610s, episode steps: 510, steps per second: 195, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42235/100000: episode: 62, duration: 2.786s, episode steps: 690, steps per second: 248, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42748/100000: episode: 63, duration: 2.011s, episode steps: 513, steps per second: 255, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43824/100000: episode: 64, duration: 4.302s, episode steps: 1076, steps per second: 250, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44624/100000: episode: 65, duration: 4.501s, episode steps: 800, steps per second: 178, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45138/100000: episode: 66, duration: 2.044s, episode steps: 514, steps per second: 251, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45728/100000: episode: 67, duration: 2.342s, episode steps: 590, steps per second: 252, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46507/100000: episode: 68, duration: 3.047s, episode steps: 779, steps per second: 256, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47337/100000: episode: 69, duration: 4.383s, episode steps: 830, steps per second: 189, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47999/100000: episode: 70, duration: 3.000s, episode steps: 662, steps per second: 221, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48531/100000: episode: 71, duration: 2.121s, episode steps: 532, steps per second: 251, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49128/100000: episode: 72, duration: 2.385s, episode steps: 597, steps per second: 250, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 50299/100000: episode: 73, duration: 68.856s, episode steps: 1171, steps per second:  17, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.009908, mae: 0.275922, mean_q: 0.359381, mean_eps: 0.774327\n",
            " 51166/100000: episode: 74, duration: 187.965s, episode steps: 867, steps per second:   5, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.009945, mae: 0.277955, mean_q: 0.357212, mean_eps: 0.771706\n",
            " 51880/100000: episode: 75, duration: 156.512s, episode steps: 714, steps per second:   5, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.008318, mae: 0.271962, mean_q: 0.349091, mean_eps: 0.768149\n",
            " 52778/100000: episode: 76, duration: 193.088s, episode steps: 898, steps per second:   5, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.008567, mae: 0.271810, mean_q: 0.347971, mean_eps: 0.764522\n",
            " 53438/100000: episode: 77, duration: 141.629s, episode steps: 660, steps per second:   5, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.008339, mae: 0.269046, mean_q: 0.344864, mean_eps: 0.761016\n",
            " 53967/100000: episode: 78, duration: 114.901s, episode steps: 529, steps per second:   5, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.007420, mae: 0.267733, mean_q: 0.341773, mean_eps: 0.758341\n",
            " 54572/100000: episode: 79, duration: 129.313s, episode steps: 605, steps per second:   5, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007177, mae: 0.268518, mean_q: 0.343593, mean_eps: 0.755790\n",
            " 55703/100000: episode: 80, duration: 243.029s, episode steps: 1131, steps per second:   5, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.007450, mae: 0.267326, mean_q: 0.342150, mean_eps: 0.751884\n",
            " 57198/100000: episode: 81, duration: 320.036s, episode steps: 1495, steps per second:   5, episode reward: 14.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007327, mae: 0.266661, mean_q: 0.341653, mean_eps: 0.745975\n",
            " 57689/100000: episode: 82, duration: 105.760s, episode steps: 491, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.006519, mae: 0.271472, mean_q: 0.349042, mean_eps: 0.741507\n",
            " 58600/100000: episode: 83, duration: 194.599s, episode steps: 911, steps per second:   5, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.007065, mae: 0.267631, mean_q: 0.343186, mean_eps: 0.738352\n",
            " 59103/100000: episode: 84, duration: 108.082s, episode steps: 503, steps per second:   5, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.006506, mae: 0.264403, mean_q: 0.339518, mean_eps: 0.735171\n",
            " 59718/100000: episode: 85, duration: 130.472s, episode steps: 615, steps per second:   5, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.006236, mae: 0.263582, mean_q: 0.338720, mean_eps: 0.732655\n",
            " 60348/100000: episode: 86, duration: 135.217s, episode steps: 630, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.006959, mae: 0.274287, mean_q: 0.350313, mean_eps: 0.729854\n",
            " 60993/100000: episode: 87, duration: 137.726s, episode steps: 645, steps per second:   5, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.007368, mae: 0.282574, mean_q: 0.360521, mean_eps: 0.726985\n",
            " 61392/100000: episode: 88, duration: 85.868s, episode steps: 399, steps per second:   5, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.007070, mae: 0.283580, mean_q: 0.361542, mean_eps: 0.724636\n",
            " 61821/100000: episode: 89, duration: 91.610s, episode steps: 429, steps per second:   5, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.006333, mae: 0.280111, mean_q: 0.358121, mean_eps: 0.722773\n",
            " 62197/100000: episode: 90, duration: 80.780s, episode steps: 376, steps per second:   5, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.005933, mae: 0.278164, mean_q: 0.354410, mean_eps: 0.720962\n",
            " 62908/100000: episode: 91, duration: 151.896s, episode steps: 711, steps per second:   5, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.006598, mae: 0.279452, mean_q: 0.357310, mean_eps: 0.718516\n",
            " 63934/100000: episode: 92, duration: 219.637s, episode steps: 1026, steps per second:   5, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.006643, mae: 0.279172, mean_q: 0.357437, mean_eps: 0.714608\n",
            " 64562/100000: episode: 93, duration: 135.355s, episode steps: 628, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.005923, mae: 0.276755, mean_q: 0.354466, mean_eps: 0.710886\n",
            " 65123/100000: episode: 94, duration: 120.733s, episode steps: 561, steps per second:   5, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.006105, mae: 0.275101, mean_q: 0.351900, mean_eps: 0.708211\n",
            " 65959/100000: episode: 95, duration: 179.058s, episode steps: 836, steps per second:   5, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.006067, mae: 0.278489, mean_q: 0.357209, mean_eps: 0.705068\n",
            " 66902/100000: episode: 96, duration: 200.625s, episode steps: 943, steps per second:   5, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.005722, mae: 0.277119, mean_q: 0.355186, mean_eps: 0.701065\n",
            " 67622/100000: episode: 97, duration: 154.899s, episode steps: 720, steps per second:   5, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.005342, mae: 0.275584, mean_q: 0.353323, mean_eps: 0.697323\n",
            " 68212/100000: episode: 98, duration: 126.169s, episode steps: 590, steps per second:   5, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.004873, mae: 0.271652, mean_q: 0.348974, mean_eps: 0.694376\n",
            " 68801/100000: episode: 99, duration: 125.584s, episode steps: 589, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.005968, mae: 0.276806, mean_q: 0.355866, mean_eps: 0.691723\n",
            " 69414/100000: episode: 100, duration: 130.348s, episode steps: 613, steps per second:   5, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.005760, mae: 0.277674, mean_q: 0.358543, mean_eps: 0.689018\n",
            " 70329/100000: episode: 101, duration: 196.775s, episode steps: 915, steps per second:   5, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.005788, mae: 0.287977, mean_q: 0.369704, mean_eps: 0.685581\n",
            " 71398/100000: episode: 102, duration: 226.900s, episode steps: 1069, steps per second:   5, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.007197, mae: 0.312297, mean_q: 0.400102, mean_eps: 0.681117\n",
            " 72023/100000: episode: 103, duration: 133.757s, episode steps: 625, steps per second:   5, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.006109, mae: 0.309324, mean_q: 0.397734, mean_eps: 0.677305\n",
            " 72662/100000: episode: 104, duration: 136.192s, episode steps: 639, steps per second:   5, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006846, mae: 0.312728, mean_q: 0.401760, mean_eps: 0.674461\n",
            " 73403/100000: episode: 105, duration: 156.993s, episode steps: 741, steps per second:   5, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.005558, mae: 0.308410, mean_q: 0.395829, mean_eps: 0.671356\n",
            " 74067/100000: episode: 106, duration: 143.080s, episode steps: 664, steps per second:   5, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.005809, mae: 0.307801, mean_q: 0.395758, mean_eps: 0.668195\n",
            " 74855/100000: episode: 107, duration: 167.522s, episode steps: 788, steps per second:   5, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.005911, mae: 0.309702, mean_q: 0.398550, mean_eps: 0.664928\n",
            " 75530/100000: episode: 108, duration: 143.371s, episode steps: 675, steps per second:   5, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.005487, mae: 0.307862, mean_q: 0.395754, mean_eps: 0.661636\n",
            " 76376/100000: episode: 109, duration: 180.051s, episode steps: 846, steps per second:   5, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.005477, mae: 0.310922, mean_q: 0.399857, mean_eps: 0.658214\n",
            " 76881/100000: episode: 110, duration: 108.639s, episode steps: 505, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.005336, mae: 0.306441, mean_q: 0.394295, mean_eps: 0.655174\n",
            " 77515/100000: episode: 111, duration: 137.053s, episode steps: 634, steps per second:   5, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.005657, mae: 0.309390, mean_q: 0.397867, mean_eps: 0.652611\n",
            " 78113/100000: episode: 112, duration: 129.334s, episode steps: 598, steps per second:   5, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.004779, mae: 0.305953, mean_q: 0.393467, mean_eps: 0.649839\n",
            " 78921/100000: episode: 113, duration: 173.117s, episode steps: 808, steps per second:   5, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.005099, mae: 0.308626, mean_q: 0.397586, mean_eps: 0.646676\n",
            " 79752/100000: episode: 114, duration: 178.075s, episode steps: 831, steps per second:   5, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.005017, mae: 0.306184, mean_q: 0.393741, mean_eps: 0.642988\n",
            " 80307/100000: episode: 115, duration: 120.000s, episode steps: 555, steps per second:   5, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.006282, mae: 0.328394, mean_q: 0.421664, mean_eps: 0.639869\n",
            " 80955/100000: episode: 116, duration: 138.581s, episode steps: 648, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.007173, mae: 0.343328, mean_q: 0.438072, mean_eps: 0.637163\n",
            " 81343/100000: episode: 117, duration: 82.973s, episode steps: 388, steps per second:   5, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.006728, mae: 0.346929, mean_q: 0.443036, mean_eps: 0.634832\n",
            " 82024/100000: episode: 118, duration: 144.752s, episode steps: 681, steps per second:   5, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.006601, mae: 0.346050, mean_q: 0.442807, mean_eps: 0.632427\n",
            " 82676/100000: episode: 119, duration: 140.775s, episode steps: 652, steps per second:   5, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.006317, mae: 0.343251, mean_q: 0.437937, mean_eps: 0.629427\n",
            " 83326/100000: episode: 120, duration: 139.124s, episode steps: 650, steps per second:   5, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.005981, mae: 0.344780, mean_q: 0.440659, mean_eps: 0.626498\n",
            " 83910/100000: episode: 121, duration: 125.516s, episode steps: 584, steps per second:   5, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.005765, mae: 0.341783, mean_q: 0.436640, mean_eps: 0.623721\n",
            " 84282/100000: episode: 122, duration: 80.351s, episode steps: 372, steps per second:   5, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.006139, mae: 0.349219, mean_q: 0.447515, mean_eps: 0.621570\n",
            " 84930/100000: episode: 123, duration: 138.778s, episode steps: 648, steps per second:   5, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.005926, mae: 0.343666, mean_q: 0.438382, mean_eps: 0.619275\n",
            " 85880/100000: episode: 124, duration: 203.258s, episode steps: 950, steps per second:   5, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.005788, mae: 0.341094, mean_q: 0.436894, mean_eps: 0.615680\n",
            " 86450/100000: episode: 125, duration: 121.434s, episode steps: 570, steps per second:   5, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.005561, mae: 0.342856, mean_q: 0.438498, mean_eps: 0.612260\n",
            " 87450/100000: episode: 126, duration: 215.073s, episode steps: 1000, steps per second:   5, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.005304, mae: 0.342313, mean_q: 0.438104, mean_eps: 0.608727\n",
            " 88092/100000: episode: 127, duration: 137.376s, episode steps: 642, steps per second:   5, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.005920, mae: 0.344250, mean_q: 0.439961, mean_eps: 0.605033\n",
            " 88611/100000: episode: 128, duration: 111.540s, episode steps: 519, steps per second:   5, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.005446, mae: 0.342708, mean_q: 0.438729, mean_eps: 0.602420\n",
            " 89185/100000: episode: 129, duration: 122.883s, episode steps: 574, steps per second:   5, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.005520, mae: 0.342313, mean_q: 0.437406, mean_eps: 0.599961\n",
            " 90134/100000: episode: 130, duration: 203.374s, episode steps: 949, steps per second:   5, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.005966, mae: 0.343010, mean_q: 0.438535, mean_eps: 0.596534\n",
            " 91329/100000: episode: 131, duration: 255.264s, episode steps: 1195, steps per second:   5, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007533, mae: 0.371585, mean_q: 0.474592, mean_eps: 0.591711\n",
            " 92060/100000: episode: 132, duration: 157.747s, episode steps: 731, steps per second:   5, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.007162, mae: 0.368004, mean_q: 0.468155, mean_eps: 0.587377\n",
            " 92517/100000: episode: 133, duration: 97.691s, episode steps: 457, steps per second:   5, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.006773, mae: 0.370785, mean_q: 0.472556, mean_eps: 0.584704\n",
            " 92904/100000: episode: 134, duration: 82.939s, episode steps: 387, steps per second:   5, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.006515, mae: 0.368532, mean_q: 0.470881, mean_eps: 0.582805\n",
            " 93346/100000: episode: 135, duration: 94.914s, episode steps: 442, steps per second:   5, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.007180, mae: 0.376244, mean_q: 0.479945, mean_eps: 0.580940\n",
            " 93826/100000: episode: 136, duration: 103.891s, episode steps: 480, steps per second:   5, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.006093, mae: 0.369110, mean_q: 0.470710, mean_eps: 0.578865\n",
            " 94365/100000: episode: 137, duration: 115.471s, episode steps: 539, steps per second:   5, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.006154, mae: 0.368760, mean_q: 0.469785, mean_eps: 0.576573\n",
            " 94730/100000: episode: 138, duration: 78.788s, episode steps: 365, steps per second:   5, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.006066, mae: 0.366949, mean_q: 0.466790, mean_eps: 0.574538\n",
            " 95254/100000: episode: 139, duration: 111.761s, episode steps: 524, steps per second:   5, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.006288, mae: 0.371188, mean_q: 0.473384, mean_eps: 0.572538\n",
            " 95796/100000: episode: 140, duration: 115.079s, episode steps: 542, steps per second:   5, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.005985, mae: 0.368878, mean_q: 0.471031, mean_eps: 0.570140\n",
            " 96864/100000: episode: 141, duration: 230.296s, episode steps: 1068, steps per second:   5, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.006247, mae: 0.368294, mean_q: 0.471159, mean_eps: 0.566517\n",
            " 97257/100000: episode: 142, duration: 85.024s, episode steps: 393, steps per second:   5, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.005782, mae: 0.368173, mean_q: 0.470118, mean_eps: 0.563230\n",
            " 97768/100000: episode: 143, duration: 109.620s, episode steps: 511, steps per second:   5, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.005643, mae: 0.374581, mean_q: 0.478770, mean_eps: 0.561196\n",
            " 98241/100000: episode: 144, duration: 101.768s, episode steps: 473, steps per second:   5, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.005710, mae: 0.369630, mean_q: 0.471666, mean_eps: 0.558982\n",
            " 98676/100000: episode: 145, duration: 92.995s, episode steps: 435, steps per second:   5, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.005925, mae: 0.366239, mean_q: 0.467507, mean_eps: 0.556939\n",
            " 99159/100000: episode: 146, duration: 104.513s, episode steps: 483, steps per second:   5, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.005824, mae: 0.367796, mean_q: 0.470738, mean_eps: 0.554874\n",
            "done, took 10943.769 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x781e72815450>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_episodes = 100\n",
        "results = dqn.test(env, nb_episodes=test_episodes, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CaJs3WSdSjg",
        "outputId": "02269040-4a12-46d3-d359-21cd9c068652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 20.000, steps: 979\n",
            "Episode 2: reward: 22.000, steps: 971\n",
            "Episode 3: reward: 26.000, steps: 1098\n",
            "Episode 4: reward: 4.000, steps: 531\n",
            "Episode 5: reward: 8.000, steps: 710\n",
            "Episode 6: reward: 14.000, steps: 854\n",
            "Episode 7: reward: 15.000, steps: 662\n",
            "Episode 8: reward: 10.000, steps: 916\n",
            "Episode 9: reward: 11.000, steps: 679\n",
            "Episode 10: reward: 36.000, steps: 1685\n",
            "Episode 11: reward: 19.000, steps: 963\n",
            "Episode 12: reward: 9.000, steps: 504\n",
            "Episode 13: reward: 20.000, steps: 991\n",
            "Episode 14: reward: 16.000, steps: 834\n",
            "Episode 15: reward: 20.000, steps: 990\n",
            "Episode 16: reward: 24.000, steps: 1014\n",
            "Episode 17: reward: 20.000, steps: 1088\n",
            "Episode 18: reward: 8.000, steps: 521\n",
            "Episode 19: reward: 13.000, steps: 654\n",
            "Episode 20: reward: 14.000, steps: 644\n",
            "Episode 21: reward: 20.000, steps: 1171\n",
            "Episode 22: reward: 5.000, steps: 589\n",
            "Episode 23: reward: 13.000, steps: 906\n",
            "Episode 24: reward: 12.000, steps: 807\n",
            "Episode 25: reward: 16.000, steps: 989\n",
            "Episode 26: reward: 8.000, steps: 681\n",
            "Episode 27: reward: 4.000, steps: 379\n",
            "Episode 28: reward: 6.000, steps: 506\n",
            "Episode 29: reward: 17.000, steps: 853\n",
            "Episode 30: reward: 11.000, steps: 744\n",
            "Episode 31: reward: 7.000, steps: 501\n",
            "Episode 32: reward: 9.000, steps: 500\n",
            "Episode 33: reward: 14.000, steps: 771\n",
            "Episode 34: reward: 16.000, steps: 1013\n",
            "Episode 35: reward: 17.000, steps: 975\n",
            "Episode 36: reward: 10.000, steps: 641\n",
            "Episode 37: reward: 19.000, steps: 905\n",
            "Episode 38: reward: 7.000, steps: 449\n",
            "Episode 39: reward: 13.000, steps: 739\n",
            "Episode 40: reward: 20.000, steps: 862\n",
            "Episode 41: reward: 4.000, steps: 322\n",
            "Episode 42: reward: 8.000, steps: 499\n",
            "Episode 43: reward: 9.000, steps: 769\n",
            "Episode 44: reward: 13.000, steps: 776\n",
            "Episode 45: reward: 6.000, steps: 488\n",
            "Episode 46: reward: 23.000, steps: 1233\n",
            "Episode 47: reward: 11.000, steps: 692\n",
            "Episode 48: reward: 16.000, steps: 758\n",
            "Episode 49: reward: 6.000, steps: 529\n",
            "Episode 50: reward: 15.000, steps: 934\n",
            "Episode 51: reward: 10.000, steps: 455\n",
            "Episode 52: reward: 8.000, steps: 651\n",
            "Episode 53: reward: 25.000, steps: 955\n",
            "Episode 54: reward: 10.000, steps: 657\n",
            "Episode 55: reward: 6.000, steps: 497\n",
            "Episode 56: reward: 13.000, steps: 702\n",
            "Episode 57: reward: 14.000, steps: 667\n",
            "Episode 58: reward: 16.000, steps: 886\n",
            "Episode 59: reward: 9.000, steps: 526\n",
            "Episode 60: reward: 7.000, steps: 520\n",
            "Episode 61: reward: 20.000, steps: 1085\n",
            "Episode 62: reward: 7.000, steps: 656\n",
            "Episode 63: reward: 17.000, steps: 733\n",
            "Episode 64: reward: 13.000, steps: 644\n",
            "Episode 65: reward: 17.000, steps: 1087\n",
            "Episode 66: reward: 17.000, steps: 1194\n",
            "Episode 67: reward: 13.000, steps: 815\n",
            "Episode 68: reward: 14.000, steps: 790\n",
            "Episode 69: reward: 8.000, steps: 449\n",
            "Episode 70: reward: 15.000, steps: 925\n",
            "Episode 71: reward: 12.000, steps: 660\n",
            "Episode 72: reward: 4.000, steps: 519\n",
            "Episode 73: reward: 16.000, steps: 907\n",
            "Episode 74: reward: 12.000, steps: 794\n",
            "Episode 75: reward: 12.000, steps: 774\n",
            "Episode 76: reward: 5.000, steps: 528\n",
            "Episode 77: reward: 13.000, steps: 672\n",
            "Episode 78: reward: 8.000, steps: 395\n",
            "Episode 79: reward: 11.000, steps: 576\n",
            "Episode 80: reward: 19.000, steps: 845\n",
            "Episode 81: reward: 5.000, steps: 500\n",
            "Episode 82: reward: 10.000, steps: 506\n",
            "Episode 83: reward: 10.000, steps: 687\n",
            "Episode 84: reward: 10.000, steps: 513\n",
            "Episode 85: reward: 10.000, steps: 617\n",
            "Episode 86: reward: 14.000, steps: 951\n",
            "Episode 87: reward: 10.000, steps: 498\n",
            "Episode 88: reward: 14.000, steps: 652\n",
            "Episode 89: reward: 17.000, steps: 614\n",
            "Episode 90: reward: 5.000, steps: 384\n",
            "Episode 91: reward: 21.000, steps: 1046\n",
            "Episode 92: reward: 15.000, steps: 667\n",
            "Episode 93: reward: 10.000, steps: 732\n",
            "Episode 94: reward: 11.000, steps: 646\n",
            "Episode 95: reward: 10.000, steps: 389\n",
            "Episode 96: reward: 26.000, steps: 1044\n",
            "Episode 97: reward: 20.000, steps: 719\n",
            "Episode 98: reward: 11.000, steps: 495\n",
            "Episode 99: reward: 13.000, steps: 711\n",
            "Episode 100: reward: 17.000, steps: 870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promedio de la corrida anterior: 6.66"
      ],
      "metadata": {
        "id": "PRC09qPGQBRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 + 100000+100000+100000 = 725000\n",
        "\n",
        "dqn.load_weights(\"dqn_weights_100000.h5f\")\n",
        "# Paso de partida 3\n",
        "starting_step = 100000 #ultimo weight actualizado\n",
        "remaining_steps = 200000 - starting_step\n",
        "\n",
        "\n",
        "# Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GA_LTA_isJL",
        "outputId": "7670d03b-46e2-4e00-e7ba-a2060d417ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   653/100000: episode: 1, duration: 2.927s, episode steps: 653, steps per second: 223, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1686/100000: episode: 2, duration: 3.620s, episode steps: 1033, steps per second: 285, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2319/100000: episode: 3, duration: 2.411s, episode steps: 633, steps per second: 263, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3246/100000: episode: 4, duration: 4.685s, episode steps: 927, steps per second: 198, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3788/100000: episode: 5, duration: 1.935s, episode steps: 542, steps per second: 280, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4181/100000: episode: 6, duration: 1.393s, episode steps: 393, steps per second: 282, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4904/100000: episode: 7, duration: 2.550s, episode steps: 723, steps per second: 284, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5843/100000: episode: 8, duration: 3.806s, episode steps: 939, steps per second: 247, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6505/100000: episode: 9, duration: 3.191s, episode steps: 662, steps per second: 207, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7115/100000: episode: 10, duration: 2.207s, episode steps: 610, steps per second: 276, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7739/100000: episode: 11, duration: 2.193s, episode steps: 624, steps per second: 285, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8127/100000: episode: 12, duration: 1.389s, episode steps: 388, steps per second: 279, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9063/100000: episode: 13, duration: 3.443s, episode steps: 936, steps per second: 272, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9680/100000: episode: 14, duration: 3.418s, episode steps: 617, steps per second: 181, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10016/100000: episode: 15, duration: 1.196s, episode steps: 336, steps per second: 281, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10683/100000: episode: 16, duration: 2.390s, episode steps: 667, steps per second: 279, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11389/100000: episode: 17, duration: 2.496s, episode steps: 706, steps per second: 283, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11998/100000: episode: 18, duration: 2.124s, episode steps: 609, steps per second: 287, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12787/100000: episode: 19, duration: 3.472s, episode steps: 789, steps per second: 227, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13484/100000: episode: 20, duration: 3.204s, episode steps: 697, steps per second: 218, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14275/100000: episode: 21, duration: 2.846s, episode steps: 791, steps per second: 278, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14988/100000: episode: 22, duration: 2.554s, episode steps: 713, steps per second: 279, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15818/100000: episode: 23, duration: 3.020s, episode steps: 830, steps per second: 275, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16820/100000: episode: 24, duration: 4.973s, episode steps: 1002, steps per second: 201, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17784/100000: episode: 25, duration: 3.438s, episode steps: 964, steps per second: 280, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18372/100000: episode: 26, duration: 2.154s, episode steps: 588, steps per second: 273, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18751/100000: episode: 27, duration: 1.360s, episode steps: 379, steps per second: 279, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19105/100000: episode: 28, duration: 1.307s, episode steps: 354, steps per second: 271, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19627/100000: episode: 29, duration: 2.824s, episode steps: 522, steps per second: 185, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20002/100000: episode: 30, duration: 1.767s, episode steps: 375, steps per second: 212, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20684/100000: episode: 31, duration: 2.475s, episode steps: 682, steps per second: 276, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21283/100000: episode: 32, duration: 2.228s, episode steps: 599, steps per second: 269, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21820/100000: episode: 33, duration: 1.923s, episode steps: 537, steps per second: 279, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22436/100000: episode: 34, duration: 2.238s, episode steps: 616, steps per second: 275, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23486/100000: episode: 35, duration: 5.084s, episode steps: 1050, steps per second: 207, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24228/100000: episode: 36, duration: 2.727s, episode steps: 742, steps per second: 272, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25111/100000: episode: 37, duration: 3.701s, episode steps: 883, steps per second: 239, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25728/100000: episode: 38, duration: 2.214s, episode steps: 617, steps per second: 279, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26141/100000: episode: 39, duration: 2.364s, episode steps: 413, steps per second: 175, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26548/100000: episode: 40, duration: 1.927s, episode steps: 407, steps per second: 211, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26943/100000: episode: 41, duration: 1.397s, episode steps: 395, steps per second: 283, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27551/100000: episode: 42, duration: 2.184s, episode steps: 608, steps per second: 278, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28173/100000: episode: 43, duration: 2.247s, episode steps: 622, steps per second: 277, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28741/100000: episode: 44, duration: 2.041s, episode steps: 568, steps per second: 278, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29379/100000: episode: 45, duration: 2.893s, episode steps: 638, steps per second: 221, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30082/100000: episode: 46, duration: 3.316s, episode steps: 703, steps per second: 212, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30741/100000: episode: 47, duration: 2.338s, episode steps: 659, steps per second: 282, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31433/100000: episode: 48, duration: 2.462s, episode steps: 692, steps per second: 281, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32151/100000: episode: 49, duration: 2.590s, episode steps: 718, steps per second: 277, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32692/100000: episode: 50, duration: 2.401s, episode steps: 541, steps per second: 225, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33746/100000: episode: 51, duration: 4.710s, episode steps: 1054, steps per second: 224, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34551/100000: episode: 52, duration: 2.909s, episode steps: 805, steps per second: 277, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35466/100000: episode: 53, duration: 3.308s, episode steps: 915, steps per second: 277, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36139/100000: episode: 54, duration: 2.911s, episode steps: 673, steps per second: 231, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36802/100000: episode: 55, duration: 3.213s, episode steps: 663, steps per second: 206, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37455/100000: episode: 56, duration: 2.325s, episode steps: 653, steps per second: 281, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38333/100000: episode: 57, duration: 3.173s, episode steps: 878, steps per second: 277, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39353/100000: episode: 58, duration: 3.751s, episode steps: 1020, steps per second: 272, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39731/100000: episode: 59, duration: 2.173s, episode steps: 378, steps per second: 174, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40097/100000: episode: 60, duration: 1.660s, episode steps: 366, steps per second: 221, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40742/100000: episode: 61, duration: 2.338s, episode steps: 645, steps per second: 276, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41288/100000: episode: 62, duration: 1.957s, episode steps: 546, steps per second: 279, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41933/100000: episode: 63, duration: 2.266s, episode steps: 645, steps per second: 285, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42312/100000: episode: 64, duration: 1.320s, episode steps: 379, steps per second: 287, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42939/100000: episode: 65, duration: 2.678s, episode steps: 627, steps per second: 234, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43448/100000: episode: 66, duration: 2.576s, episode steps: 509, steps per second: 198, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44155/100000: episode: 67, duration: 2.493s, episode steps: 707, steps per second: 284, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44717/100000: episode: 68, duration: 1.985s, episode steps: 562, steps per second: 283, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45761/100000: episode: 69, duration: 3.708s, episode steps: 1044, steps per second: 282, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46394/100000: episode: 70, duration: 2.779s, episode steps: 633, steps per second: 228, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47072/100000: episode: 71, duration: 3.163s, episode steps: 678, steps per second: 214, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47719/100000: episode: 72, duration: 2.261s, episode steps: 647, steps per second: 286, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48245/100000: episode: 73, duration: 1.869s, episode steps: 526, steps per second: 281, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49139/100000: episode: 74, duration: 3.111s, episode steps: 894, steps per second: 287, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49535/100000: episode: 75, duration: 1.409s, episode steps: 396, steps per second: 281, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 50159/100000: episode: 76, duration: 32.478s, episode steps: 624, steps per second:  19, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013205, mae: 0.400916, mean_q: 0.503530, mean_eps: 0.774642\n",
            " 50777/100000: episode: 77, duration: 111.125s, episode steps: 618, steps per second:   6, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012779, mae: 0.399777, mean_q: 0.495600, mean_eps: 0.772896\n",
            " 51478/100000: episode: 78, duration: 127.555s, episode steps: 701, steps per second:   5, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.011280, mae: 0.394649, mean_q: 0.485916, mean_eps: 0.769929\n",
            " 51858/100000: episode: 79, duration: 69.164s, episode steps: 380, steps per second:   5, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.010363, mae: 0.398488, mean_q: 0.490119, mean_eps: 0.767496\n",
            " 52544/100000: episode: 80, duration: 124.081s, episode steps: 686, steps per second:   6, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009686, mae: 0.394424, mean_q: 0.484455, mean_eps: 0.765098\n",
            " 52908/100000: episode: 81, duration: 66.043s, episode steps: 364, steps per second:   6, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.010165, mae: 0.395324, mean_q: 0.487929, mean_eps: 0.762735\n",
            " 53553/100000: episode: 82, duration: 114.464s, episode steps: 645, steps per second:   6, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.009041, mae: 0.391882, mean_q: 0.483797, mean_eps: 0.760465\n",
            " 54171/100000: episode: 83, duration: 110.669s, episode steps: 618, steps per second:   6, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.009210, mae: 0.389501, mean_q: 0.481114, mean_eps: 0.757623\n",
            " 54875/100000: episode: 84, duration: 124.954s, episode steps: 704, steps per second:   6, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.008694, mae: 0.392919, mean_q: 0.487336, mean_eps: 0.754649\n",
            " 55420/100000: episode: 85, duration: 96.015s, episode steps: 545, steps per second:   6, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.008249, mae: 0.388711, mean_q: 0.483440, mean_eps: 0.751839\n",
            " 56430/100000: episode: 86, duration: 176.035s, episode steps: 1010, steps per second:   6, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.008390, mae: 0.392016, mean_q: 0.487698, mean_eps: 0.748340\n",
            " 57399/100000: episode: 87, duration: 171.733s, episode steps: 969, steps per second:   6, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.007569, mae: 0.389763, mean_q: 0.485321, mean_eps: 0.743887\n",
            " 57956/100000: episode: 88, duration: 97.487s, episode steps: 557, steps per second:   6, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.006844, mae: 0.386489, mean_q: 0.481424, mean_eps: 0.740453\n",
            " 58942/100000: episode: 89, duration: 174.049s, episode steps: 986, steps per second:   6, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.007081, mae: 0.390948, mean_q: 0.487614, mean_eps: 0.736982\n",
            " 59585/100000: episode: 90, duration: 113.687s, episode steps: 643, steps per second:   6, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.007073, mae: 0.390794, mean_q: 0.488279, mean_eps: 0.733317\n",
            " 60091/100000: episode: 91, duration: 88.492s, episode steps: 506, steps per second:   6, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.007616, mae: 0.394932, mean_q: 0.493954, mean_eps: 0.730731\n",
            " 60775/100000: episode: 92, duration: 121.753s, episode steps: 684, steps per second:   6, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.009064, mae: 0.424781, mean_q: 0.529746, mean_eps: 0.728054\n",
            " 61758/100000: episode: 93, duration: 173.120s, episode steps: 983, steps per second:   6, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.007915, mae: 0.420625, mean_q: 0.523425, mean_eps: 0.724303\n",
            " 62294/100000: episode: 94, duration: 94.554s, episode steps: 536, steps per second:   6, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.007419, mae: 0.422889, mean_q: 0.525965, mean_eps: 0.720885\n",
            " 62960/100000: episode: 95, duration: 119.398s, episode steps: 666, steps per second:   6, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.007155, mae: 0.422337, mean_q: 0.526238, mean_eps: 0.718181\n",
            " 63646/100000: episode: 96, duration: 123.319s, episode steps: 686, steps per second:   6, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007038, mae: 0.420533, mean_q: 0.526098, mean_eps: 0.715139\n",
            " 64703/100000: episode: 97, duration: 187.103s, episode steps: 1057, steps per second:   6, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.006524, mae: 0.419975, mean_q: 0.525005, mean_eps: 0.711217\n",
            " 65123/100000: episode: 98, duration: 74.453s, episode steps: 420, steps per second:   6, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.006620, mae: 0.422223, mean_q: 0.527588, mean_eps: 0.707894\n",
            " 65709/100000: episode: 99, duration: 104.369s, episode steps: 586, steps per second:   6, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.006609, mae: 0.418025, mean_q: 0.521094, mean_eps: 0.705630\n",
            " 66157/100000: episode: 100, duration: 79.871s, episode steps: 448, steps per second:   6, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.005907, mae: 0.415243, mean_q: 0.518682, mean_eps: 0.703304\n",
            " 66662/100000: episode: 101, duration: 86.158s, episode steps: 505, steps per second:   6, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.006094, mae: 0.416559, mean_q: 0.521966, mean_eps: 0.701159\n",
            " 67390/100000: episode: 102, duration: 125.900s, episode steps: 728, steps per second:   6, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.006187, mae: 0.418194, mean_q: 0.522331, mean_eps: 0.698385\n",
            " 68014/100000: episode: 103, duration: 113.365s, episode steps: 624, steps per second:   6, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.005903, mae: 0.417452, mean_q: 0.522726, mean_eps: 0.695343\n",
            " 68671/100000: episode: 104, duration: 119.787s, episode steps: 657, steps per second:   5, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.006105, mae: 0.418310, mean_q: 0.523231, mean_eps: 0.692461\n",
            " 69060/100000: episode: 105, duration: 69.687s, episode steps: 389, steps per second:   6, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.005825, mae: 0.419333, mean_q: 0.525001, mean_eps: 0.690108\n",
            " 70032/100000: episode: 106, duration: 174.426s, episode steps: 972, steps per second:   6, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.005859, mae: 0.421798, mean_q: 0.529391, mean_eps: 0.687045\n",
            " 70806/100000: episode: 107, duration: 139.064s, episode steps: 774, steps per second:   6, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.008311, mae: 0.461069, mean_q: 0.574775, mean_eps: 0.683117\n",
            " 71278/100000: episode: 108, duration: 85.059s, episode steps: 472, steps per second:   6, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.007810, mae: 0.453337, mean_q: 0.565869, mean_eps: 0.680313\n",
            " 72438/100000: episode: 109, duration: 212.392s, episode steps: 1160, steps per second:   5, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.007064, mae: 0.456583, mean_q: 0.568923, mean_eps: 0.676641\n",
            " 73729/100000: episode: 110, duration: 231.769s, episode steps: 1291, steps per second:   6, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.006760, mae: 0.456514, mean_q: 0.569257, mean_eps: 0.671126\n",
            " 74062/100000: episode: 111, duration: 61.116s, episode steps: 333, steps per second:   5, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.006775, mae: 0.454644, mean_q: 0.566744, mean_eps: 0.667472\n",
            " 74881/100000: episode: 112, duration: 148.391s, episode steps: 819, steps per second:   6, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.006320, mae: 0.455020, mean_q: 0.568634, mean_eps: 0.664880\n",
            " 75421/100000: episode: 113, duration: 98.893s, episode steps: 540, steps per second:   5, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.006393, mae: 0.455412, mean_q: 0.569732, mean_eps: 0.661823\n",
            " 76197/100000: episode: 114, duration: 140.822s, episode steps: 776, steps per second:   6, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.005789, mae: 0.454297, mean_q: 0.568631, mean_eps: 0.658862\n",
            " 77126/100000: episode: 115, duration: 168.593s, episode steps: 929, steps per second:   6, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.006170, mae: 0.454115, mean_q: 0.567804, mean_eps: 0.655025\n",
            " 77918/100000: episode: 116, duration: 145.367s, episode steps: 792, steps per second:   5, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.006052, mae: 0.457420, mean_q: 0.572706, mean_eps: 0.651153\n",
            " 78706/100000: episode: 117, duration: 139.439s, episode steps: 788, steps per second:   6, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.006201, mae: 0.455783, mean_q: 0.569943, mean_eps: 0.647598\n",
            " 79092/100000: episode: 118, duration: 68.556s, episode steps: 386, steps per second:   6, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.005804, mae: 0.457024, mean_q: 0.570309, mean_eps: 0.644957\n",
            " 80175/100000: episode: 119, duration: 200.041s, episode steps: 1083, steps per second:   5, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.006023, mae: 0.455872, mean_q: 0.570635, mean_eps: 0.641652\n",
            " 80833/100000: episode: 120, duration: 120.954s, episode steps: 658, steps per second:   5, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007865, mae: 0.478241, mean_q: 0.596601, mean_eps: 0.637734\n",
            " 81540/100000: episode: 121, duration: 133.805s, episode steps: 707, steps per second:   5, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.007539, mae: 0.474135, mean_q: 0.591108, mean_eps: 0.634663\n",
            " 81881/100000: episode: 122, duration: 64.336s, episode steps: 341, steps per second:   5, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.007516, mae: 0.474787, mean_q: 0.592971, mean_eps: 0.632305\n",
            " 82527/100000: episode: 123, duration: 117.186s, episode steps: 646, steps per second:   6, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.006842, mae: 0.478603, mean_q: 0.596744, mean_eps: 0.630084\n",
            " 83206/100000: episode: 124, duration: 122.966s, episode steps: 679, steps per second:   6, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.007150, mae: 0.476092, mean_q: 0.593653, mean_eps: 0.627103\n",
            " 83993/100000: episode: 125, duration: 142.498s, episode steps: 787, steps per second:   6, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.006819, mae: 0.479226, mean_q: 0.598774, mean_eps: 0.623804\n",
            " 85000/100000: episode: 126, duration: 180.553s, episode steps: 1007, steps per second:   6, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.006769, mae: 0.478350, mean_q: 0.597663, mean_eps: 0.619768\n",
            " 85628/100000: episode: 127, duration: 115.672s, episode steps: 628, steps per second:   5, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.006845, mae: 0.478559, mean_q: 0.597111, mean_eps: 0.616089\n",
            " 86312/100000: episode: 128, duration: 127.448s, episode steps: 684, steps per second:   5, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.006342, mae: 0.478451, mean_q: 0.597389, mean_eps: 0.613137\n",
            " 86711/100000: episode: 129, duration: 71.953s, episode steps: 399, steps per second:   6, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.005966, mae: 0.472321, mean_q: 0.589751, mean_eps: 0.610700\n",
            " 87282/100000: episode: 130, duration: 104.197s, episode steps: 571, steps per second:   5, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.006405, mae: 0.479493, mean_q: 0.599160, mean_eps: 0.608518\n",
            " 87658/100000: episode: 131, duration: 67.911s, episode steps: 376, steps per second:   6, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.005737, mae: 0.478596, mean_q: 0.598055, mean_eps: 0.606387\n",
            " 88399/100000: episode: 132, duration: 136.154s, episode steps: 741, steps per second:   5, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.006346, mae: 0.480421, mean_q: 0.599248, mean_eps: 0.603874\n",
            " 89094/100000: episode: 133, duration: 129.498s, episode steps: 695, steps per second:   5, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.006255, mae: 0.479435, mean_q: 0.598102, mean_eps: 0.600643\n",
            " 89584/100000: episode: 134, duration: 91.970s, episode steps: 490, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.006287, mae: 0.480330, mean_q: 0.598663, mean_eps: 0.597977\n",
            " 89930/100000: episode: 135, duration: 64.608s, episode steps: 346, steps per second:   5, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.005728, mae: 0.480192, mean_q: 0.599580, mean_eps: 0.596096\n",
            " 90730/100000: episode: 136, duration: 145.877s, episode steps: 800, steps per second:   5, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.008042, mae: 0.498815, mean_q: 0.622134, mean_eps: 0.593517\n",
            " 91598/100000: episode: 137, duration: 155.178s, episode steps: 868, steps per second:   6, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007779, mae: 0.498039, mean_q: 0.620486, mean_eps: 0.589764\n",
            " 92114/100000: episode: 138, duration: 94.525s, episode steps: 516, steps per second:   5, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.007389, mae: 0.494739, mean_q: 0.616968, mean_eps: 0.586650\n",
            " 92636/100000: episode: 139, duration: 96.491s, episode steps: 522, steps per second:   5, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007347, mae: 0.500530, mean_q: 0.623504, mean_eps: 0.584315\n",
            " 92959/100000: episode: 140, duration: 59.470s, episode steps: 323, steps per second:   5, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.006926, mae: 0.490797, mean_q: 0.610959, mean_eps: 0.582413\n",
            " 94181/100000: episode: 141, duration: 224.206s, episode steps: 1222, steps per second:   5, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.006969, mae: 0.493683, mean_q: 0.614339, mean_eps: 0.578937\n",
            " 94859/100000: episode: 142, duration: 123.790s, episode steps: 678, steps per second:   5, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.006451, mae: 0.494394, mean_q: 0.615784, mean_eps: 0.574662\n",
            " 95379/100000: episode: 143, duration: 95.572s, episode steps: 520, steps per second:   5, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.006663, mae: 0.496371, mean_q: 0.617996, mean_eps: 0.571967\n",
            " 95914/100000: episode: 144, duration: 102.339s, episode steps: 535, steps per second:   5, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.006438, mae: 0.495004, mean_q: 0.615993, mean_eps: 0.569593\n",
            " 96613/100000: episode: 145, duration: 130.209s, episode steps: 699, steps per second:   5, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.006296, mae: 0.495735, mean_q: 0.618172, mean_eps: 0.566816\n",
            " 97191/100000: episode: 146, duration: 108.694s, episode steps: 578, steps per second:   5, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.005976, mae: 0.494328, mean_q: 0.615868, mean_eps: 0.563943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.load_weights(\"dqn_weights_100000.h5f\")\n",
        "test_episodes = 10\n",
        "results = dqn.test(env,nb_episodes=test_episodes, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DizYvdoZTlgs",
        "outputId": "095e8c17-5cd1-48e1-cfce-167c909ddb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 8.000, steps: 381\n",
            "Episode 2: reward: 5.000, steps: 439\n",
            "Episode 3: reward: 9.000, steps: 523\n",
            "Episode 4: reward: 18.000, steps: 894\n",
            "Episode 5: reward: 14.000, steps: 888\n",
            "Episode 6: reward: 12.000, steps: 555\n",
            "Episode 7: reward: 26.000, steps: 1096\n",
            "Episode 8: reward: 4.000, steps: 410\n",
            "Episode 9: reward: 16.000, steps: 778\n",
            "Episode 10: reward: 20.000, steps: 1140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promedio de corrida: 13,2 (mejoró el doble ya que empezó con un 6,66). Sin embargo, no llegó al objetivo de una media de recompensa mayor o igual a 20 puntos. Por lo que apartir de estos resultados, se proponen las siguientes recomendaciones para mejorar el entrenamiento:\n",
        "1. Realizar una exploración más prolongada: Se recomienda revisar el rango y la duración del decaimiento del epsilon en las policy epsilon-greedy. En algunos casos, mantener una exploración mínima en torno al 0.05 y el 0.1 durante más pasos puede evitar el estancamiento en policys subóptimas.\n",
        "2. Incrementar los pasos totales si el entorno lo requiere: Para algunos entornos se requieren más de 1 millón de pasos para converger en una policy competitiva. Si se observa una mejora progresiva, vale la pena extender el entrenamiento a 2 millones de pasos o más.\n",
        "3. Realizar ajustes finos al learning rate y target update: Considerando que la tasa de aprendizaje se puede ajustar dinámicamente o probarse con valores más bajos (por ejemplo: 0.0001) si se detectan oscilaciones o divergencia. Así mismo, la frecuencia de actualización del modelo objetivo debería probarse entre 5000 y 15000 pasos según la estabilidad observada."
      ],
      "metadata": {
        "id": "cU6hDN2bV3mq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "## 3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eduardo"
      ],
      "metadata": {
        "id": "VZgTa3gCvjJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edward"
      ],
      "metadata": {
        "id": "u-kRWJD6voEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Luis"
      ],
      "metadata": {
        "id": "TsfWcIlfvqMR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNG_c7Xb7wIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2GbhN5H7v9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Justificación de los parámetros seleccionados y resultados obtenidos - Dueling Q-Network:"
      ],
      "metadata": {
        "id": "5MzLdSpevsRX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "3.2.1 Modelo\n",
        "El modelo utilizado se basa en la arquitectura Dueling Double DQN, propuesta como una mejora al Deep Q-Network original de Mnih et al. (2015). Esta arquitectura incorpora dos mejoras clave:\n",
        "\n",
        "Double DQN: Introducida por Van Hasselt et al. (2016), reduce la sobreestimación de los valores Q al separar las redes utilizadas para seleccionar y evaluar acciones.\n",
        "\n",
        "Dueling DQN: Propuesta por Wang et al. (2016), separa la estimación del valor del estado (V(s)) de la ventaja de cada acción (A(s,a)), lo que permite al agente aprender a evaluar correctamente estados incluso si no conoce el efecto de todas las acciones disponibles.\n",
        "\n",
        "Referencias:\n",
        "\n",
        "- Van Hasselt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement Learning with Double Q-learning. https://arxiv.org/abs/1509.06461\n",
        "\n",
        "- Wang, Z., Schaul, T., Hessel, M., Hasselt, H. V., Lanctot, M., & de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. https://arxiv.org/abs/1511.06581"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.2 Evolución de los hiperparámetros probados\n",
        "\n",
        "Durante el desarrollo del modelo se exploraron distintos valores de los siguientes hiperparámetros:\n",
        "\n",
        "- LEARNING_RATE: Se probó con 0.00025 y 0.0001. Este último mostró mayor estabilidad en fases tempranas.\n",
        "\n",
        "- MEMORY_SIZE: Se fijó en 500,000 transiciones, lo que permitió una buena diversidad de experiencias sin afectar el rendimiento.\n",
        "\n",
        "- POLICY_NB_STEPS: epsilon decayó linealmente desde 1.0 hasta 0.1 o 0.01, con un total de 300,000 a 1,000,000 pasos según el experimento.\n",
        "\n",
        "- WARMUP_STEPS: Se utilizaron 40,000 y 50,000 pasos de warm-up, durante los cuales solo se almacenan transiciones.\n",
        "\n",
        "- MODEL_UPDATE: El target model se actualizó cada 8,500 o 10,000 pasos, buscando equilibrio entre estabilidad y capacidad de adaptación.\n",
        "\n",
        "- MODEL_CHECKPOINT_INTERVAL: Se guardaron modelos cada 10,000 o 25,000 pasos. Esto facilitó la continuidad en sesiones prolongadas.\n",
        "\n",
        "- TRAIN_NB_STEPS: Se realizaron entrenamientos desde 400,000 hasta 1,000,000 pasos, según la ejecución.\n",
        "\n",
        "- TRAIN_INTERVAL: El modelo actualizó sus pesos cada 4 pasos, como se recomienda en los papers base.\n",
        "\n",
        "- CURRENT_STRATEGY: Policy EpsGreedyQPolicy con LinearAnnealedPolicy.\n",
        "\n",
        "Estas configuraciones fueron ajustadas con base en experimentación directa, revisiones bibliográficas y pruebas comparativas internas. Si bien no se alcanzó la media de 20 puntos, el modelo logró un rendimiento promedio de 13.2 puntos en 1,000,000 pasos, lo cual demuestra aprendizaje efectivo y proporciona una base sólida para futuras mejoras."
      ],
      "metadata": {
        "id": "PJuTj9A_FOBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Conclusiones\n",
        "\n",
        "El entrenamiento de modelos DQN avanzados requiere iteración constante, validación de múltiples combinaciones de hiperparámetros y una comprensión clara del comportamiento del entorno. En este caso, si bien no se alcanzó el umbral de 20 puntos, se logró una policy funcional que mejora significativamente respecto al comportamiento aleatorio, validando la efectividad de la arquitectura Dueling Double DQN.\n",
        "\n",
        "Los resultados obtenidos muestran que:\n",
        "\n",
        "- El modelo está aprendiendo una policy válida.\n",
        "\n",
        "- La configuración es estable y permite mejoras incrementales.\n",
        "\n",
        "- Es posible mejorar el rendimiento incluyendo técnicas adicionales como prioritized replay, multi-step targets, o entrenamientos más largos.\n",
        "\n",
        "Esta etapa permite consolidar aprendizajes clave y orientar los próximos entrenamientos del equipo hacia arquitecturas más robustas y estrategias de exploración más eficaces."
      ],
      "metadata": {
        "id": "u_BVduI2JEW7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
