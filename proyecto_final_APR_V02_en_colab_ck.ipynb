{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLkxlehYwrdtLi9alDOdGU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisIrigoyen/trabajo_final_RL/blob/pruebas_CarlosKong/proyecto_final_APR_V02_en_colab_ck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0G89Iq2xnT1",
        "outputId": "66984cac-b264-42e1-a8df-22fed6fb1b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando Google Drive en /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.23.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-8rpezb7_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-8rpezb7_\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
            "Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "# MONTAJE EN GOOGLE DRIVE\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n",
        "\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"Montando Google Drive en\", mount)\n",
        "  drive.mount(mount)\n",
        "  os.makedirs(drive_root, exist_ok=True)\n",
        "  %cd $drive_root\n",
        "%pwd\n",
        "\n",
        "# INSTALACIÓN DE DEPENDENCIAS\n",
        "%pip install gym==0.17.3\n",
        "%pip install git+https://github.com/Kojoley/atari-py.git\n",
        "%pip install keras-rl2==1.0.5\n",
        "%pip install tensorflow==2.12\n",
        "%pip install opencv-python\n",
        "\n",
        "# IMPORTACIONES\n",
        "import numpy as np\n",
        "import gym\n",
        "import cv2\n",
        "#import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTORNO\n",
        "env = gym.make('SpaceInvaders-v0')  # Cambia si deseas: 'Breakout-v0'\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# PREPROCESAMIENTO\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        img = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        img = cv2.resize(img, (84, 84))\n",
        "        return img.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        return batch.astype('float32') / 255.0\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "metadata": {
        "id": "ZOdZHSM3xy2E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODELO DQN\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=(4, 84, 84)))\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "\n",
        "# MEMORIA Y POLICY\n",
        "memory = SequentialMemory(limit=100000, window_length=4)\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                              value_max=1.0, value_min=0.1, value_test=0.05,\n",
        "                              nb_steps=200000)"
      ],
      "metadata": {
        "id": "yjO7g4ygy78G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGENTE DQN\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
        "               nb_steps_warmup=50000, enable_double_dqn=True,\n",
        "               enable_dueling_network=True, dueling_type='avg',\n",
        "               target_model_update=10000, policy=policy,\n",
        "               processor=AtariProcessor())\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "dqn.compile(Adam(lr=0.00025), metrics=['mae'])\n",
        "\n",
        "# CHECKPOINTS Y LOGGING\n",
        "checkpoint_path = drive_root + '/dqn_weights_{step}.h5f'\n",
        "weights_filename = drive_root + '/dqn_final_weights.h5f'\n",
        "\n",
        "callbacks = [\n",
        "    ModelIntervalCheckpoint(checkpoint_path, interval=25000),\n",
        "    FileLogger(drive_root + '/dqn_log.json', interval=10000)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2rntCzIy9D8",
        "outputId": "3ffcf9be-b97f-4956-d364-5f707617223a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 + 100000+75000+100000 = 800000\n",
        "\n",
        "dqn.load_weights(\"dqn_weights_75000.h5f\")\n",
        "# 🔢 Paso de partida 3\n",
        "starting_step = 75000 #ultimo weight actualizado\n",
        "remaining_steps = 200000 - starting_step\n",
        "\n",
        "\n",
        "# 🧩 Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# 🚀 Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htRNej0vy8_Q",
        "outputId": "2b88d6aa-12ef-4d1a-d3b9-03ffb424f0dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 125000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    634/125000: episode: 1, duration: 4.627s, episode steps: 634, steps per second: 137, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1406/125000: episode: 2, duration: 5.157s, episode steps: 772, steps per second: 150, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   1807/125000: episode: 3, duration: 1.898s, episode steps: 401, steps per second: 211, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   2589/125000: episode: 4, duration: 3.808s, episode steps: 782, steps per second: 205, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3305/125000: episode: 5, duration: 5.100s, episode steps: 716, steps per second: 140, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   3784/125000: episode: 6, duration: 2.415s, episode steps: 479, steps per second: 198, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4298/125000: episode: 7, duration: 2.923s, episode steps: 514, steps per second: 176, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   4667/125000: episode: 8, duration: 2.109s, episode steps: 369, steps per second: 175, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   5574/125000: episode: 9, duration: 5.990s, episode steps: 907, steps per second: 151, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   6177/125000: episode: 10, duration: 3.165s, episode steps: 603, steps per second: 191, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   6684/125000: episode: 11, duration: 2.724s, episode steps: 507, steps per second: 186, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   7197/125000: episode: 12, duration: 2.659s, episode steps: 513, steps per second: 193, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8081/125000: episode: 13, duration: 5.898s, episode steps: 884, steps per second: 150, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   8605/125000: episode: 14, duration: 2.764s, episode steps: 524, steps per second: 190, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   9363/125000: episode: 15, duration: 4.168s, episode steps: 758, steps per second: 182, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  10247/125000: episode: 16, duration: 6.038s, episode steps: 884, steps per second: 146, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  11119/125000: episode: 17, duration: 4.669s, episode steps: 872, steps per second: 187, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  11642/125000: episode: 18, duration: 2.564s, episode steps: 523, steps per second: 204, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  12037/125000: episode: 19, duration: 2.095s, episode steps: 395, steps per second: 189, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  12704/125000: episode: 20, duration: 4.974s, episode steps: 667, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  13694/125000: episode: 21, duration: 5.113s, episode steps: 990, steps per second: 194, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  14360/125000: episode: 22, duration: 3.719s, episode steps: 666, steps per second: 179, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  14805/125000: episode: 23, duration: 3.403s, episode steps: 445, steps per second: 131, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  15864/125000: episode: 24, duration: 7.916s, episode steps: 1059, steps per second: 134, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  16365/125000: episode: 25, duration: 2.612s, episode steps: 501, steps per second: 192, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  16888/125000: episode: 26, duration: 3.116s, episode steps: 523, steps per second: 168, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  17667/125000: episode: 27, duration: 5.018s, episode steps: 779, steps per second: 155, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  18695/125000: episode: 28, duration: 5.049s, episode steps: 1028, steps per second: 204, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  19242/125000: episode: 29, duration: 2.801s, episode steps: 547, steps per second: 195, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  19735/125000: episode: 30, duration: 3.994s, episode steps: 493, steps per second: 123, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20135/125000: episode: 31, duration: 2.067s, episode steps: 400, steps per second: 194, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  20796/125000: episode: 32, duration: 3.323s, episode steps: 661, steps per second: 199, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  21343/125000: episode: 33, duration: 2.694s, episode steps: 547, steps per second: 203, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  22167/125000: episode: 34, duration: 5.674s, episode steps: 824, steps per second: 145, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23243/125000: episode: 35, duration: 5.464s, episode steps: 1076, steps per second: 197, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  23644/125000: episode: 36, duration: 1.976s, episode steps: 401, steps per second: 203, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24133/125000: episode: 37, duration: 2.597s, episode steps: 489, steps per second: 188, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  24641/125000: episode: 38, duration: 4.115s, episode steps: 508, steps per second: 123, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  25134/125000: episode: 39, duration: 3.046s, episode steps: 493, steps per second: 162, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  25622/125000: episode: 40, duration: 2.386s, episode steps: 488, steps per second: 205, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26286/125000: episode: 41, duration: 3.416s, episode steps: 664, steps per second: 194, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  26999/125000: episode: 42, duration: 5.210s, episode steps: 713, steps per second: 137, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  27646/125000: episode: 43, duration: 3.364s, episode steps: 647, steps per second: 192, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  28299/125000: episode: 44, duration: 3.311s, episode steps: 653, steps per second: 197, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  28711/125000: episode: 45, duration: 1.969s, episode steps: 412, steps per second: 209, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  29208/125000: episode: 46, duration: 3.506s, episode steps: 497, steps per second: 142, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30251/125000: episode: 47, duration: 5.798s, episode steps: 1043, steps per second: 180, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  30629/125000: episode: 48, duration: 1.758s, episode steps: 378, steps per second: 215, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31068/125000: episode: 49, duration: 2.198s, episode steps: 439, steps per second: 200, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  31789/125000: episode: 50, duration: 4.698s, episode steps: 721, steps per second: 153, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  32222/125000: episode: 51, duration: 2.572s, episode steps: 433, steps per second: 168, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  32802/125000: episode: 52, duration: 2.846s, episode steps: 580, steps per second: 204, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  33315/125000: episode: 53, duration: 2.393s, episode steps: 513, steps per second: 214, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34026/125000: episode: 54, duration: 4.018s, episode steps: 711, steps per second: 177, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  34747/125000: episode: 55, duration: 4.688s, episode steps: 721, steps per second: 154, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  35234/125000: episode: 56, duration: 2.512s, episode steps: 487, steps per second: 194, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36016/125000: episode: 57, duration: 4.012s, episode steps: 782, steps per second: 195, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  36654/125000: episode: 58, duration: 4.246s, episode steps: 638, steps per second: 150, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  37171/125000: episode: 59, duration: 3.193s, episode steps: 517, steps per second: 162, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  37716/125000: episode: 60, duration: 2.633s, episode steps: 545, steps per second: 207, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  38454/125000: episode: 61, duration: 3.578s, episode steps: 738, steps per second: 206, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  39708/125000: episode: 62, duration: 7.918s, episode steps: 1254, steps per second: 158, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40080/125000: episode: 63, duration: 1.894s, episode steps: 372, steps per second: 196, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40474/125000: episode: 64, duration: 1.931s, episode steps: 394, steps per second: 204, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  40824/125000: episode: 65, duration: 1.790s, episode steps: 350, steps per second: 196, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41347/125000: episode: 66, duration: 2.928s, episode steps: 523, steps per second: 179, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  41851/125000: episode: 67, duration: 3.871s, episode steps: 504, steps per second: 130, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  42985/125000: episode: 68, duration: 5.588s, episode steps: 1134, steps per second: 203, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44189/125000: episode: 69, duration: 7.555s, episode steps: 1204, steps per second: 159, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  44966/125000: episode: 70, duration: 4.052s, episode steps: 777, steps per second: 192, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  45711/125000: episode: 71, duration: 4.348s, episode steps: 745, steps per second: 171, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  46397/125000: episode: 72, duration: 6.319s, episode steps: 686, steps per second: 109, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  47448/125000: episode: 73, duration: 5.592s, episode steps: 1051, steps per second: 188, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48091/125000: episode: 74, duration: 3.368s, episode steps: 643, steps per second: 191, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48450/125000: episode: 75, duration: 2.274s, episode steps: 359, steps per second: 158, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  48970/125000: episode: 76, duration: 3.856s, episode steps: 520, steps per second: 135, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  49364/125000: episode: 77, duration: 1.980s, episode steps: 394, steps per second: 199, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  49844/125000: episode: 78, duration: 2.594s, episode steps: 480, steps per second: 185, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  50326/125000: episode: 79, duration: 81.127s, episode steps: 482, steps per second:   6, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.011988, mae: 0.480977, mean_q: 0.591011, mean_eps: 0.774266\n",
            "  50836/125000: episode: 80, duration: 122.197s, episode steps: 510, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011484, mae: 0.479145, mean_q: 0.585658, mean_eps: 0.772388\n",
            "  51476/125000: episode: 81, duration: 156.931s, episode steps: 640, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010772, mae: 0.475709, mean_q: 0.580060, mean_eps: 0.769800\n",
            "  51971/125000: episode: 82, duration: 119.482s, episode steps: 495, steps per second:   4, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.009741, mae: 0.476042, mean_q: 0.580918, mean_eps: 0.767246\n",
            "  52571/125000: episode: 83, duration: 143.439s, episode steps: 600, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.009943, mae: 0.471340, mean_q: 0.574531, mean_eps: 0.764783\n",
            "  53172/125000: episode: 84, duration: 142.251s, episode steps: 601, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009456, mae: 0.472407, mean_q: 0.578133, mean_eps: 0.762081\n",
            "  53522/125000: episode: 85, duration: 84.129s, episode steps: 350, steps per second:   4, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.009044, mae: 0.472888, mean_q: 0.577837, mean_eps: 0.759941\n",
            "  54416/125000: episode: 86, duration: 210.955s, episode steps: 894, steps per second:   4, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.009258, mae: 0.472055, mean_q: 0.577535, mean_eps: 0.757142\n",
            "  55067/125000: episode: 87, duration: 153.907s, episode steps: 651, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008847, mae: 0.470176, mean_q: 0.576097, mean_eps: 0.753665\n",
            "  56299/125000: episode: 88, duration: 298.046s, episode steps: 1232, steps per second:   4, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007948, mae: 0.467888, mean_q: 0.573750, mean_eps: 0.749429\n",
            "  57138/125000: episode: 89, duration: 203.244s, episode steps: 839, steps per second:   4, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.007476, mae: 0.471152, mean_q: 0.579060, mean_eps: 0.744769\n",
            "  57750/125000: episode: 90, duration: 148.132s, episode steps: 612, steps per second:   4, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.007409, mae: 0.469741, mean_q: 0.578037, mean_eps: 0.741504\n",
            "  58268/125000: episode: 91, duration: 121.757s, episode steps: 518, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.006943, mae: 0.467585, mean_q: 0.574831, mean_eps: 0.738962\n",
            "  58731/125000: episode: 92, duration: 110.882s, episode steps: 463, steps per second:   4, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.006642, mae: 0.465818, mean_q: 0.572870, mean_eps: 0.736754\n",
            "  59118/125000: episode: 93, duration: 92.218s, episode steps: 387, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.006418, mae: 0.463832, mean_q: 0.571791, mean_eps: 0.734842\n",
            "  59959/125000: episode: 94, duration: 200.285s, episode steps: 841, steps per second:   4, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006708, mae: 0.462724, mean_q: 0.570988, mean_eps: 0.732079\n",
            "  60483/125000: episode: 95, duration: 123.408s, episode steps: 524, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.008201, mae: 0.490624, mean_q: 0.604158, mean_eps: 0.729008\n",
            "  61471/125000: episode: 96, duration: 235.448s, episode steps: 988, steps per second:   4, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.007944, mae: 0.493279, mean_q: 0.607539, mean_eps: 0.725606\n",
            "  61988/125000: episode: 97, duration: 123.289s, episode steps: 517, steps per second:   4, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.007317, mae: 0.496141, mean_q: 0.611961, mean_eps: 0.722219\n",
            "  62323/125000: episode: 98, duration: 79.244s, episode steps: 335, steps per second:   4, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.007284, mae: 0.495897, mean_q: 0.612368, mean_eps: 0.720302\n",
            "  62968/125000: episode: 99, duration: 153.939s, episode steps: 645, steps per second:   4, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.006773, mae: 0.491007, mean_q: 0.606372, mean_eps: 0.718098\n",
            "  63567/125000: episode: 100, duration: 141.227s, episode steps: 599, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.006545, mae: 0.490937, mean_q: 0.606002, mean_eps: 0.715299\n",
            "  64224/125000: episode: 101, duration: 155.688s, episode steps: 657, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.006263, mae: 0.491224, mean_q: 0.605351, mean_eps: 0.712472\n",
            "  64737/125000: episode: 102, duration: 124.993s, episode steps: 513, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.006309, mae: 0.495133, mean_q: 0.611349, mean_eps: 0.709840\n",
            "  65304/125000: episode: 103, duration: 133.926s, episode steps: 567, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006387, mae: 0.489166, mean_q: 0.603210, mean_eps: 0.707410\n",
            "  66120/125000: episode: 104, duration: 196.143s, episode steps: 816, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.005739, mae: 0.489083, mean_q: 0.604608, mean_eps: 0.704298\n",
            "  66680/125000: episode: 105, duration: 134.394s, episode steps: 560, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.006092, mae: 0.490236, mean_q: 0.605086, mean_eps: 0.701202\n",
            "  67203/125000: episode: 106, duration: 125.859s, episode steps: 523, steps per second:   4, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.006080, mae: 0.494678, mean_q: 0.611210, mean_eps: 0.698766\n",
            "  67864/125000: episode: 107, duration: 161.214s, episode steps: 661, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.006133, mae: 0.487727, mean_q: 0.602894, mean_eps: 0.696101\n",
            "  68607/125000: episode: 108, duration: 184.274s, episode steps: 743, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.005735, mae: 0.489630, mean_q: 0.605837, mean_eps: 0.692943\n",
            "  69514/125000: episode: 109, duration: 223.947s, episode steps: 907, steps per second:   4, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.005835, mae: 0.487494, mean_q: 0.602982, mean_eps: 0.689230\n",
            "  70025/125000: episode: 110, duration: 125.750s, episode steps: 511, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.005731, mae: 0.488064, mean_q: 0.604163, mean_eps: 0.686039\n",
            "  71091/125000: episode: 111, duration: 264.181s, episode steps: 1066, steps per second:   4, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.007558, mae: 0.521328, mean_q: 0.643369, mean_eps: 0.682491\n",
            "  71837/125000: episode: 112, duration: 182.904s, episode steps: 746, steps per second:   4, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.006518, mae: 0.519342, mean_q: 0.640099, mean_eps: 0.678414\n",
            "  72270/125000: episode: 113, duration: 103.632s, episode steps: 433, steps per second:   4, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.007048, mae: 0.524887, mean_q: 0.646578, mean_eps: 0.675762\n",
            "  72933/125000: episode: 114, duration: 160.426s, episode steps: 663, steps per second:   4, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.006731, mae: 0.519790, mean_q: 0.642088, mean_eps: 0.673296\n",
            "  73323/125000: episode: 115, duration: 93.591s, episode steps: 390, steps per second:   4, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.006362, mae: 0.519267, mean_q: 0.640569, mean_eps: 0.670926\n",
            "  74267/125000: episode: 116, duration: 228.242s, episode steps: 944, steps per second:   4, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.006425, mae: 0.518701, mean_q: 0.639885, mean_eps: 0.667925\n",
            "  74819/125000: episode: 117, duration: 136.507s, episode steps: 552, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.006325, mae: 0.519381, mean_q: 0.641006, mean_eps: 0.664559\n",
            "  75209/125000: episode: 118, duration: 95.982s, episode steps: 390, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.005928, mae: 0.514596, mean_q: 0.636071, mean_eps: 0.662439\n",
            "  76073/125000: episode: 119, duration: 212.981s, episode steps: 864, steps per second:   4, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.006001, mae: 0.522086, mean_q: 0.644906, mean_eps: 0.659618\n",
            "  76424/125000: episode: 120, duration: 84.661s, episode steps: 351, steps per second:   4, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.006246, mae: 0.521851, mean_q: 0.644763, mean_eps: 0.656884\n",
            "  77160/125000: episode: 121, duration: 182.030s, episode steps: 736, steps per second:   4, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.005710, mae: 0.519249, mean_q: 0.641293, mean_eps: 0.654438\n",
            "  77796/125000: episode: 122, duration: 156.454s, episode steps: 636, steps per second:   4, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.005322, mae: 0.521345, mean_q: 0.645274, mean_eps: 0.651351\n",
            "  78172/125000: episode: 123, duration: 89.706s, episode steps: 376, steps per second:   4, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.005320, mae: 0.522984, mean_q: 0.646439, mean_eps: 0.649074\n",
            "  79336/125000: episode: 124, duration: 282.285s, episode steps: 1164, steps per second:   4, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.005783, mae: 0.525156, mean_q: 0.648266, mean_eps: 0.645609\n",
            "  80317/125000: episode: 125, duration: 238.947s, episode steps: 981, steps per second:   4, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.006021, mae: 0.527036, mean_q: 0.650446, mean_eps: 0.640783\n",
            "  81148/125000: episode: 126, duration: 203.600s, episode steps: 831, steps per second:   4, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.007346, mae: 0.551695, mean_q: 0.680611, mean_eps: 0.636706\n",
            "  82026/125000: episode: 127, duration: 212.264s, episode steps: 878, steps per second:   4, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006772, mae: 0.546298, mean_q: 0.673571, mean_eps: 0.632861\n",
            "  83040/125000: episode: 128, duration: 241.188s, episode steps: 1014, steps per second:   4, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.006473, mae: 0.548580, mean_q: 0.675958, mean_eps: 0.628604\n",
            "  83465/125000: episode: 129, duration: 96.814s, episode steps: 425, steps per second:   4, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.006174, mae: 0.549155, mean_q: 0.676607, mean_eps: 0.625366\n",
            "  84209/125000: episode: 130, duration: 168.770s, episode steps: 744, steps per second:   4, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.006215, mae: 0.545550, mean_q: 0.672346, mean_eps: 0.622736\n",
            "  84667/125000: episode: 131, duration: 106.266s, episode steps: 458, steps per second:   4, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.006375, mae: 0.550588, mean_q: 0.678593, mean_eps: 0.620031\n",
            "  85249/125000: episode: 132, duration: 134.883s, episode steps: 582, steps per second:   4, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.006289, mae: 0.546209, mean_q: 0.673647, mean_eps: 0.617691\n",
            "  85792/125000: episode: 133, duration: 124.167s, episode steps: 543, steps per second:   4, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.006537, mae: 0.548321, mean_q: 0.675473, mean_eps: 0.615160\n",
            "  86306/125000: episode: 134, duration: 118.418s, episode steps: 514, steps per second:   4, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.006075, mae: 0.546155, mean_q: 0.675178, mean_eps: 0.612782\n",
            "  87428/125000: episode: 135, duration: 254.715s, episode steps: 1122, steps per second:   4, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.006290, mae: 0.550166, mean_q: 0.679400, mean_eps: 0.609101\n",
            "  88261/125000: episode: 136, duration: 188.514s, episode steps: 833, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.005719, mae: 0.546940, mean_q: 0.674266, mean_eps: 0.604702\n",
            "  89138/125000: episode: 137, duration: 200.432s, episode steps: 877, steps per second:   4, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.005840, mae: 0.542837, mean_q: 0.669812, mean_eps: 0.600854\n",
            "  89632/125000: episode: 138, duration: 112.760s, episode steps: 494, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.005842, mae: 0.550935, mean_q: 0.678914, mean_eps: 0.597770\n",
            "  90214/125000: episode: 139, duration: 133.362s, episode steps: 582, steps per second:   4, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.006667, mae: 0.555621, mean_q: 0.685314, mean_eps: 0.595349\n",
            "  90711/125000: episode: 140, duration: 113.897s, episode steps: 497, steps per second:   4, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.008217, mae: 0.576812, mean_q: 0.711340, mean_eps: 0.592921\n",
            "  91185/125000: episode: 141, duration: 106.637s, episode steps: 474, steps per second:   4, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.007941, mae: 0.573514, mean_q: 0.707156, mean_eps: 0.590736\n",
            "  91965/125000: episode: 142, duration: 178.063s, episode steps: 780, steps per second:   4, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.007871, mae: 0.574353, mean_q: 0.708143, mean_eps: 0.587915\n",
            "  92618/125000: episode: 143, duration: 148.427s, episode steps: 653, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007007, mae: 0.564857, mean_q: 0.696021, mean_eps: 0.584691\n",
            "  93123/125000: episode: 144, duration: 114.228s, episode steps: 505, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.007135, mae: 0.576522, mean_q: 0.710722, mean_eps: 0.582085\n",
            "  93651/125000: episode: 145, duration: 120.606s, episode steps: 528, steps per second:   4, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.007223, mae: 0.577290, mean_q: 0.711869, mean_eps: 0.579761\n",
            "  94159/125000: episode: 146, duration: 115.026s, episode steps: 508, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.007019, mae: 0.578353, mean_q: 0.713439, mean_eps: 0.577430\n",
            "  94885/125000: episode: 147, duration: 164.014s, episode steps: 726, steps per second:   4, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.006614, mae: 0.570351, mean_q: 0.702536, mean_eps: 0.574653\n",
            "  95507/125000: episode: 148, duration: 142.551s, episode steps: 622, steps per second:   4, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.006228, mae: 0.566955, mean_q: 0.698606, mean_eps: 0.571620\n",
            "  96576/125000: episode: 149, duration: 243.821s, episode steps: 1069, steps per second:   4, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.006156, mae: 0.568608, mean_q: 0.701282, mean_eps: 0.567816\n",
            "  96941/125000: episode: 150, duration: 84.555s, episode steps: 365, steps per second:   4, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.006312, mae: 0.568959, mean_q: 0.701591, mean_eps: 0.564589\n",
            "  97580/125000: episode: 151, duration: 145.220s, episode steps: 639, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.006507, mae: 0.570505, mean_q: 0.702743, mean_eps: 0.562330\n",
            "  98050/125000: episode: 152, duration: 107.816s, episode steps: 470, steps per second:   4, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.006360, mae: 0.571420, mean_q: 0.704288, mean_eps: 0.559835\n",
            "  98749/125000: episode: 153, duration: 158.461s, episode steps: 699, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.005988, mae: 0.566286, mean_q: 0.698506, mean_eps: 0.557204\n",
            "  99304/125000: episode: 154, duration: 126.042s, episode steps: 555, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006123, mae: 0.569917, mean_q: 0.702977, mean_eps: 0.554383\n",
            "  99921/125000: episode: 155, duration: 139.916s, episode steps: 617, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.005923, mae: 0.566077, mean_q: 0.698083, mean_eps: 0.551746\n",
            " 100478/125000: episode: 156, duration: 129.140s, episode steps: 557, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.007984, mae: 0.582972, mean_q: 0.717914, mean_eps: 0.549104\n",
            " 101053/125000: episode: 157, duration: 131.487s, episode steps: 575, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.008028, mae: 0.588658, mean_q: 0.725016, mean_eps: 0.546557\n",
            " 102127/125000: episode: 158, duration: 246.751s, episode steps: 1074, steps per second:   4, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.008063, mae: 0.585983, mean_q: 0.721780, mean_eps: 0.542847\n",
            " 102769/125000: episode: 159, duration: 150.171s, episode steps: 642, steps per second:   4, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.007298, mae: 0.584425, mean_q: 0.719058, mean_eps: 0.538986\n",
            " 103115/125000: episode: 160, duration: 78.789s, episode steps: 346, steps per second:   4, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.006655, mae: 0.590728, mean_q: 0.725533, mean_eps: 0.536763\n",
            " 103700/125000: episode: 161, duration: 134.600s, episode steps: 585, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.007228, mae: 0.586391, mean_q: 0.719702, mean_eps: 0.534668\n",
            " 104092/125000: episode: 162, duration: 92.029s, episode steps: 392, steps per second:   4, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.006664, mae: 0.581991, mean_q: 0.716289, mean_eps: 0.532470\n",
            " 104924/125000: episode: 163, duration: 191.780s, episode steps: 832, steps per second:   4, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.006431, mae: 0.583789, mean_q: 0.718357, mean_eps: 0.529716\n",
            " 105549/125000: episode: 164, duration: 145.511s, episode steps: 625, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.006437, mae: 0.582361, mean_q: 0.715844, mean_eps: 0.526438\n",
            " 106127/125000: episode: 165, duration: 132.234s, episode steps: 578, steps per second:   4, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.006433, mae: 0.580848, mean_q: 0.714414, mean_eps: 0.523731\n",
            " 106815/125000: episode: 166, duration: 156.981s, episode steps: 688, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.006430, mae: 0.587616, mean_q: 0.723060, mean_eps: 0.520883\n",
            " 107363/125000: episode: 167, duration: 126.233s, episode steps: 548, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.006592, mae: 0.583908, mean_q: 0.718302, mean_eps: 0.518102\n",
            " 108051/125000: episode: 168, duration: 154.999s, episode steps: 688, steps per second:   4, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.006620, mae: 0.587186, mean_q: 0.722711, mean_eps: 0.515321\n",
            " 108560/125000: episode: 169, duration: 115.243s, episode steps: 509, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.006379, mae: 0.584494, mean_q: 0.719770, mean_eps: 0.512628\n",
            " 109057/125000: episode: 170, duration: 113.001s, episode steps: 497, steps per second:   4, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.006422, mae: 0.584553, mean_q: 0.719652, mean_eps: 0.510364\n",
            " 109687/125000: episode: 171, duration: 143.009s, episode steps: 630, steps per second:   4, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.006351, mae: 0.587421, mean_q: 0.723644, mean_eps: 0.507828\n",
            " 110317/125000: episode: 172, duration: 142.594s, episode steps: 630, steps per second:   4, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.007091, mae: 0.600028, mean_q: 0.738072, mean_eps: 0.504993\n",
            " 110894/125000: episode: 173, duration: 131.333s, episode steps: 577, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.008223, mae: 0.607929, mean_q: 0.747032, mean_eps: 0.502278\n",
            " 111261/125000: episode: 174, duration: 83.408s, episode steps: 367, steps per second:   4, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.008376, mae: 0.605272, mean_q: 0.742902, mean_eps: 0.500154\n",
            " 111947/125000: episode: 175, duration: 155.969s, episode steps: 686, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.007758, mae: 0.604332, mean_q: 0.742622, mean_eps: 0.497784\n",
            " 112437/125000: episode: 176, duration: 108.914s, episode steps: 490, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.007623, mae: 0.609725, mean_q: 0.749552, mean_eps: 0.495138\n",
            " 113605/125000: episode: 177, duration: 265.072s, episode steps: 1168, steps per second:   4, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.007527, mae: 0.610238, mean_q: 0.750962, mean_eps: 0.491408\n",
            " 114004/125000: episode: 178, duration: 90.044s, episode steps: 399, steps per second:   4, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.007595, mae: 0.616648, mean_q: 0.757804, mean_eps: 0.487882\n",
            " 114534/125000: episode: 179, duration: 118.615s, episode steps: 530, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.007122, mae: 0.612388, mean_q: 0.753439, mean_eps: 0.485792\n",
            " 115169/125000: episode: 180, duration: 143.102s, episode steps: 635, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.006972, mae: 0.611215, mean_q: 0.750895, mean_eps: 0.483170\n",
            " 115847/125000: episode: 181, duration: 153.624s, episode steps: 678, steps per second:   4, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.006727, mae: 0.608314, mean_q: 0.748370, mean_eps: 0.480216\n",
            " 116331/125000: episode: 182, duration: 110.585s, episode steps: 484, steps per second:   4, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.007091, mae: 0.604318, mean_q: 0.743606, mean_eps: 0.477602\n",
            " 117167/125000: episode: 183, duration: 189.501s, episode steps: 836, steps per second:   4, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.007006, mae: 0.608525, mean_q: 0.748469, mean_eps: 0.474632\n",
            " 117670/125000: episode: 184, duration: 113.381s, episode steps: 503, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.006875, mae: 0.607395, mean_q: 0.746698, mean_eps: 0.471619\n",
            " 118352/125000: episode: 185, duration: 153.573s, episode steps: 682, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.006831, mae: 0.608621, mean_q: 0.749161, mean_eps: 0.468953\n",
            " 119301/125000: episode: 186, duration: 214.396s, episode steps: 949, steps per second:   4, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.006338, mae: 0.610669, mean_q: 0.751742, mean_eps: 0.465283\n",
            " 119812/125000: episode: 187, duration: 116.115s, episode steps: 511, steps per second:   4, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.881 [0.000, 5.000],  loss: 0.007054, mae: 0.607993, mean_q: 0.747585, mean_eps: 0.461998\n",
            " 120657/125000: episode: 188, duration: 190.993s, episode steps: 845, steps per second:   4, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.008684, mae: 0.632111, mean_q: 0.777301, mean_eps: 0.458947\n",
            " 121198/125000: episode: 189, duration: 121.723s, episode steps: 541, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008513, mae: 0.645262, mean_q: 0.792039, mean_eps: 0.455828\n",
            " 121728/125000: episode: 190, duration: 119.497s, episode steps: 530, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.008614, mae: 0.641960, mean_q: 0.788651, mean_eps: 0.453419\n",
            " 122500/125000: episode: 191, duration: 172.655s, episode steps: 772, steps per second:   4, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.008344, mae: 0.633270, mean_q: 0.777704, mean_eps: 0.450489\n",
            " 123242/125000: episode: 192, duration: 168.691s, episode steps: 742, steps per second:   4, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.007810, mae: 0.635274, mean_q: 0.780082, mean_eps: 0.447083\n",
            " 123965/125000: episode: 193, duration: 163.738s, episode steps: 723, steps per second:   4, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.007422, mae: 0.634824, mean_q: 0.781268, mean_eps: 0.443787\n",
            " 124380/125000: episode: 194, duration: 94.127s, episode steps: 415, steps per second:   4, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.007180, mae: 0.635163, mean_q: 0.781220, mean_eps: 0.441226\n",
            "done, took 17828.386 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a92eb93da90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "weights_filename = 'dqn_weights_125000.h5f'.format(env)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-P1haTn_Goo",
        "outputId": "0b71a628-8cc2-4cab-defe-bff7ab8d5f4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 9.000, steps: 520\n",
            "Episode 2: reward: 22.000, steps: 980\n",
            "Episode 3: reward: 10.000, steps: 601\n",
            "Episode 4: reward: 22.000, steps: 953\n",
            "Episode 5: reward: 21.000, steps: 1026\n",
            "Episode 6: reward: 10.000, steps: 638\n",
            "Episode 7: reward: 6.000, steps: 477\n",
            "Episode 8: reward: 9.000, steps: 522\n",
            "Episode 9: reward: 8.000, steps: 688\n",
            "Episode 10: reward: 7.000, steps: 598\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a92e8fe7990>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_weights_100000.h5f\n",
        "#Continuación al 325000 + 100000 + 100000+75000+100000+125000 = 925000\n",
        "\n",
        "dqn.load_weights(\"dqn_weights_125000.h5f\")\n",
        "# 🔢 Paso de partida 5\n",
        "starting_step = 125000 #ultimo weight actualizado\n",
        "remaining_steps = 225000 - starting_step\n",
        "\n",
        "\n",
        "# 🧩 Callback personalizado para continuar numeración de checkpoints\n",
        "class OffsetModelCheckpoint(ModelIntervalCheckpoint):\n",
        "    def __init__(self, filepath, interval, offset):\n",
        "        super().__init__(filepath, interval)\n",
        "        self.offset = offset\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        # Ajusta el número de paso en el nombre del archivo\n",
        "        self.step = step + self.offset\n",
        "        super().on_step_end(step, logs)\n",
        "\n",
        "# 🚀 Entrenamiento con pasos continuados desde 100000\n",
        "dqn.fit(env, nb_steps=remaining_steps, visualize=False, verbose=2,\n",
        "        callbacks=[\n",
        "            FileLogger(\"dqn_log_continuacion.json\", interval=10000),\n",
        "            OffsetModelCheckpoint(\"dqn_weights_{step}.h5f\", interval=25000, offset=starting_step)\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V1QLfy5DmHX",
        "outputId": "c6259ee5-010c-4afc-b9bf-543bb1509a46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "   609/100000: episode: 1, duration: 3.928s, episode steps: 609, steps per second: 155, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1483/100000: episode: 2, duration: 5.362s, episode steps: 874, steps per second: 163, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2082/100000: episode: 3, duration: 3.132s, episode steps: 599, steps per second: 191, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2745/100000: episode: 4, duration: 3.480s, episode steps: 663, steps per second: 191, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3492/100000: episode: 5, duration: 5.292s, episode steps: 747, steps per second: 141, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4121/100000: episode: 6, duration: 3.309s, episode steps: 629, steps per second: 190, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4733/100000: episode: 7, duration: 3.208s, episode steps: 612, steps per second: 191, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5884/100000: episode: 8, duration: 7.265s, episode steps: 1151, steps per second: 158, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6385/100000: episode: 9, duration: 2.642s, episode steps: 501, steps per second: 190, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7159/100000: episode: 10, duration: 4.088s, episode steps: 774, steps per second: 189, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7766/100000: episode: 11, duration: 4.013s, episode steps: 607, steps per second: 151, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9137/100000: episode: 12, duration: 7.835s, episode steps: 1371, steps per second: 175, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9530/100000: episode: 13, duration: 3.236s, episode steps: 393, steps per second: 121, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9960/100000: episode: 14, duration: 3.260s, episode steps: 430, steps per second: 132, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10750/100000: episode: 15, duration: 4.512s, episode steps: 790, steps per second: 175, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11374/100000: episode: 16, duration: 3.255s, episode steps: 624, steps per second: 192, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12455/100000: episode: 17, duration: 7.084s, episode steps: 1081, steps per second: 153, episode reward:  7.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13086/100000: episode: 18, duration: 3.308s, episode steps: 631, steps per second: 191, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13878/100000: episode: 19, duration: 4.203s, episode steps: 792, steps per second: 188, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14617/100000: episode: 20, duration: 4.732s, episode steps: 739, steps per second: 156, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15293/100000: episode: 21, duration: 4.717s, episode steps: 676, steps per second: 143, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15689/100000: episode: 22, duration: 2.113s, episode steps: 396, steps per second: 187, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16385/100000: episode: 23, duration: 3.698s, episode steps: 696, steps per second: 188, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16922/100000: episode: 24, duration: 3.939s, episode steps: 537, steps per second: 136, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17470/100000: episode: 25, duration: 3.095s, episode steps: 548, steps per second: 177, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18361/100000: episode: 26, duration: 4.535s, episode steps: 891, steps per second: 196, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19638/100000: episode: 27, duration: 7.941s, episode steps: 1277, steps per second: 161, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20263/100000: episode: 28, duration: 3.236s, episode steps: 625, steps per second: 193, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20909/100000: episode: 29, duration: 3.346s, episode steps: 646, steps per second: 193, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21420/100000: episode: 30, duration: 2.850s, episode steps: 511, steps per second: 179, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22361/100000: episode: 31, duration: 5.920s, episode steps: 941, steps per second: 159, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22963/100000: episode: 32, duration: 3.237s, episode steps: 602, steps per second: 186, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23369/100000: episode: 33, duration: 2.084s, episode steps: 406, steps per second: 195, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23916/100000: episode: 34, duration: 3.380s, episode steps: 547, steps per second: 162, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24314/100000: episode: 35, duration: 2.884s, episode steps: 398, steps per second: 138, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25271/100000: episode: 36, duration: 4.959s, episode steps: 957, steps per second: 193, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25810/100000: episode: 37, duration: 2.732s, episode steps: 539, steps per second: 197, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26419/100000: episode: 38, duration: 3.949s, episode steps: 609, steps per second: 154, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27078/100000: episode: 39, duration: 3.918s, episode steps: 659, steps per second: 168, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27683/100000: episode: 40, duration: 3.104s, episode steps: 605, steps per second: 195, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28090/100000: episode: 41, duration: 2.452s, episode steps: 407, steps per second: 166, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28801/100000: episode: 42, duration: 6.302s, episode steps: 711, steps per second: 113, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29434/100000: episode: 43, duration: 3.465s, episode steps: 633, steps per second: 183, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30208/100000: episode: 44, duration: 4.055s, episode steps: 774, steps per second: 191, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30846/100000: episode: 45, duration: 3.737s, episode steps: 638, steps per second: 171, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31221/100000: episode: 46, duration: 3.020s, episode steps: 375, steps per second: 124, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31778/100000: episode: 47, duration: 2.967s, episode steps: 557, steps per second: 188, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32174/100000: episode: 48, duration: 2.075s, episode steps: 396, steps per second: 191, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32724/100000: episode: 49, duration: 2.881s, episode steps: 550, steps per second: 191, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33743/100000: episode: 50, duration: 6.749s, episode steps: 1019, steps per second: 151, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34410/100000: episode: 51, duration: 3.541s, episode steps: 667, steps per second: 188, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35404/100000: episode: 52, duration: 5.224s, episode steps: 994, steps per second: 190, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35933/100000: episode: 53, duration: 4.087s, episode steps: 529, steps per second: 129, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36419/100000: episode: 54, duration: 2.506s, episode steps: 486, steps per second: 194, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37124/100000: episode: 55, duration: 3.717s, episode steps: 705, steps per second: 190, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37752/100000: episode: 56, duration: 3.236s, episode steps: 628, steps per second: 194, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38477/100000: episode: 57, duration: 5.122s, episode steps: 725, steps per second: 142, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39822/100000: episode: 58, duration: 6.832s, episode steps: 1345, steps per second: 197, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40479/100000: episode: 59, duration: 4.306s, episode steps: 657, steps per second: 153, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41268/100000: episode: 60, duration: 4.549s, episode steps: 789, steps per second: 173, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41868/100000: episode: 61, duration: 3.150s, episode steps: 600, steps per second: 190, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42566/100000: episode: 62, duration: 3.689s, episode steps: 698, steps per second: 189, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43749/100000: episode: 63, duration: 7.463s, episode steps: 1183, steps per second: 159, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44293/100000: episode: 64, duration: 2.922s, episode steps: 544, steps per second: 186, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44982/100000: episode: 65, duration: 3.953s, episode steps: 689, steps per second: 174, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45461/100000: episode: 66, duration: 3.656s, episode steps: 479, steps per second: 131, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46117/100000: episode: 67, duration: 3.403s, episode steps: 656, steps per second: 193, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46731/100000: episode: 68, duration: 3.257s, episode steps: 614, steps per second: 189, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47413/100000: episode: 69, duration: 5.516s, episode steps: 682, steps per second: 124, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48038/100000: episode: 70, duration: 4.312s, episode steps: 625, steps per second: 145, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48562/100000: episode: 71, duration: 2.792s, episode steps: 524, steps per second: 188, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49337/100000: episode: 72, duration: 4.004s, episode steps: 775, steps per second: 194, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49956/100000: episode: 73, duration: 4.320s, episode steps: 619, steps per second: 143, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 50341/100000: episode: 74, duration: 76.488s, episode steps: 385, steps per second:   5, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013361, mae: 0.662432, mean_q: 0.806568, mean_eps: 0.774233\n",
            " 51003/100000: episode: 75, duration: 152.694s, episode steps: 662, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.012641, mae: 0.657403, mean_q: 0.800065, mean_eps: 0.771978\n",
            " 51402/100000: episode: 76, duration: 92.628s, episode steps: 399, steps per second:   4, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012201, mae: 0.655780, mean_q: 0.796986, mean_eps: 0.769591\n",
            " 52281/100000: episode: 77, duration: 200.358s, episode steps: 879, steps per second:   4, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011730, mae: 0.656134, mean_q: 0.797709, mean_eps: 0.766715\n",
            " 53186/100000: episode: 78, duration: 207.799s, episode steps: 905, steps per second:   4, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011902, mae: 0.655521, mean_q: 0.795499, mean_eps: 0.762701\n",
            " 53870/100000: episode: 79, duration: 156.700s, episode steps: 684, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.011395, mae: 0.653766, mean_q: 0.794115, mean_eps: 0.759126\n",
            " 54381/100000: episode: 80, duration: 116.178s, episode steps: 511, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.010680, mae: 0.653807, mean_q: 0.794245, mean_eps: 0.756437\n",
            " 54761/100000: episode: 81, duration: 86.436s, episode steps: 380, steps per second:   4, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.009920, mae: 0.655847, mean_q: 0.797842, mean_eps: 0.754433\n",
            " 55768/100000: episode: 82, duration: 231.033s, episode steps: 1007, steps per second:   4, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010559, mae: 0.661325, mean_q: 0.804394, mean_eps: 0.751312\n",
            " 56381/100000: episode: 83, duration: 141.287s, episode steps: 613, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.010370, mae: 0.654094, mean_q: 0.795290, mean_eps: 0.747667\n",
            " 56860/100000: episode: 84, duration: 108.606s, episode steps: 479, steps per second:   4, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.010032, mae: 0.652448, mean_q: 0.793614, mean_eps: 0.745210\n",
            " 57443/100000: episode: 85, duration: 133.762s, episode steps: 583, steps per second:   4, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.010380, mae: 0.656662, mean_q: 0.799725, mean_eps: 0.742821\n",
            " 58265/100000: episode: 86, duration: 186.023s, episode steps: 822, steps per second:   4, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009161, mae: 0.652045, mean_q: 0.793651, mean_eps: 0.739659\n",
            " 58989/100000: episode: 87, duration: 168.761s, episode steps: 724, steps per second:   4, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.009786, mae: 0.655323, mean_q: 0.797580, mean_eps: 0.736181\n",
            " 59372/100000: episode: 88, duration: 88.372s, episode steps: 383, steps per second:   4, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.009960, mae: 0.653730, mean_q: 0.794994, mean_eps: 0.733690\n",
            " 60019/100000: episode: 89, duration: 150.531s, episode steps: 647, steps per second:   4, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.009207, mae: 0.651513, mean_q: 0.792324, mean_eps: 0.731372\n",
            " 60647/100000: episode: 90, duration: 142.606s, episode steps: 628, steps per second:   4, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.010254, mae: 0.663774, mean_q: 0.807915, mean_eps: 0.728504\n",
            " 61171/100000: episode: 91, duration: 120.147s, episode steps: 524, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010214, mae: 0.672072, mean_q: 0.818206, mean_eps: 0.725912\n",
            " 62222/100000: episode: 92, duration: 239.832s, episode steps: 1051, steps per second:   4, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.009678, mae: 0.670177, mean_q: 0.815473, mean_eps: 0.722368\n",
            " 63051/100000: episode: 93, duration: 190.478s, episode steps: 829, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.009377, mae: 0.664070, mean_q: 0.809074, mean_eps: 0.718138\n",
            " 63673/100000: episode: 94, duration: 141.228s, episode steps: 622, steps per second:   4, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.009935, mae: 0.663345, mean_q: 0.807098, mean_eps: 0.714873\n",
            " 64527/100000: episode: 95, duration: 194.788s, episode steps: 854, steps per second:   4, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.009324, mae: 0.664882, mean_q: 0.809689, mean_eps: 0.711552\n",
            " 65132/100000: episode: 96, duration: 136.804s, episode steps: 605, steps per second:   4, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.008970, mae: 0.669804, mean_q: 0.814512, mean_eps: 0.708269\n",
            " 65996/100000: episode: 97, duration: 198.429s, episode steps: 864, steps per second:   4, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.008515, mae: 0.666445, mean_q: 0.811626, mean_eps: 0.704964\n",
            " 66327/100000: episode: 98, duration: 74.209s, episode steps: 331, steps per second:   4, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.008020, mae: 0.668618, mean_q: 0.815056, mean_eps: 0.702276\n",
            " 66994/100000: episode: 99, duration: 154.056s, episode steps: 667, steps per second:   4, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.008851, mae: 0.669649, mean_q: 0.815065, mean_eps: 0.700030\n",
            " 67729/100000: episode: 100, duration: 168.394s, episode steps: 735, steps per second:   4, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.008496, mae: 0.665397, mean_q: 0.810619, mean_eps: 0.696876\n",
            " 68519/100000: episode: 101, duration: 182.269s, episode steps: 790, steps per second:   4, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.008873, mae: 0.661751, mean_q: 0.806199, mean_eps: 0.693444\n",
            " 69271/100000: episode: 102, duration: 171.093s, episode steps: 752, steps per second:   4, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.008187, mae: 0.667556, mean_q: 0.812955, mean_eps: 0.689975\n",
            " 69794/100000: episode: 103, duration: 120.484s, episode steps: 523, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.007807, mae: 0.666690, mean_q: 0.811783, mean_eps: 0.687106\n",
            " 70595/100000: episode: 104, duration: 183.813s, episode steps: 801, steps per second:   4, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.009863, mae: 0.688575, mean_q: 0.836138, mean_eps: 0.684127\n",
            " 71077/100000: episode: 105, duration: 110.031s, episode steps: 482, steps per second:   4, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.009550, mae: 0.705166, mean_q: 0.859058, mean_eps: 0.681240\n",
            " 71731/100000: episode: 106, duration: 148.880s, episode steps: 654, steps per second:   4, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.009133, mae: 0.704490, mean_q: 0.857857, mean_eps: 0.678684\n",
            " 72290/100000: episode: 107, duration: 126.886s, episode steps: 559, steps per second:   4, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.009200, mae: 0.701551, mean_q: 0.854939, mean_eps: 0.675955\n",
            " 73625/100000: episode: 108, duration: 302.349s, episode steps: 1335, steps per second:   4, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009064, mae: 0.700156, mean_q: 0.852694, mean_eps: 0.671693\n",
            " 74525/100000: episode: 109, duration: 206.231s, episode steps: 900, steps per second:   4, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.008305, mae: 0.704312, mean_q: 0.857946, mean_eps: 0.666665\n",
            " 75075/100000: episode: 110, duration: 125.676s, episode steps: 550, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.008744, mae: 0.702916, mean_q: 0.855984, mean_eps: 0.663402\n",
            " 75781/100000: episode: 111, duration: 163.231s, episode steps: 706, steps per second:   4, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.008623, mae: 0.701764, mean_q: 0.855469, mean_eps: 0.660576\n",
            " 76462/100000: episode: 112, duration: 155.328s, episode steps: 681, steps per second:   4, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.007576, mae: 0.700129, mean_q: 0.854498, mean_eps: 0.657455\n",
            " 77044/100000: episode: 113, duration: 133.862s, episode steps: 582, steps per second:   4, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.008077, mae: 0.695848, mean_q: 0.848985, mean_eps: 0.654614\n",
            " 77737/100000: episode: 114, duration: 158.650s, episode steps: 693, steps per second:   4, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.008324, mae: 0.701643, mean_q: 0.855277, mean_eps: 0.651745\n",
            " 78567/100000: episode: 115, duration: 191.119s, episode steps: 830, steps per second:   4, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.008179, mae: 0.706596, mean_q: 0.860484, mean_eps: 0.648318\n",
            " 79049/100000: episode: 116, duration: 110.600s, episode steps: 482, steps per second:   4, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007898, mae: 0.700411, mean_q: 0.854017, mean_eps: 0.645366\n",
            " 79967/100000: episode: 117, duration: 211.457s, episode steps: 918, steps per second:   4, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.008003, mae: 0.703691, mean_q: 0.857077, mean_eps: 0.642216\n",
            " 80561/100000: episode: 118, duration: 135.731s, episode steps: 594, steps per second:   4, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.009290, mae: 0.728211, mean_q: 0.887101, mean_eps: 0.638814\n",
            " 80939/100000: episode: 119, duration: 86.861s, episode steps: 378, steps per second:   4, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.009410, mae: 0.730318, mean_q: 0.887888, mean_eps: 0.636627\n",
            " 81875/100000: episode: 120, duration: 214.971s, episode steps: 936, steps per second:   4, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.008836, mae: 0.728754, mean_q: 0.887608, mean_eps: 0.633671\n",
            " 82386/100000: episode: 121, duration: 115.957s, episode steps: 511, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.008492, mae: 0.730198, mean_q: 0.889849, mean_eps: 0.630415\n",
            " 83087/100000: episode: 122, duration: 160.651s, episode steps: 701, steps per second:   4, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.008477, mae: 0.721025, mean_q: 0.879294, mean_eps: 0.627688\n",
            " 83567/100000: episode: 123, duration: 110.400s, episode steps: 480, steps per second:   4, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.007859, mae: 0.716851, mean_q: 0.874518, mean_eps: 0.625031\n",
            " 84081/100000: episode: 124, duration: 117.009s, episode steps: 514, steps per second:   4, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.007913, mae: 0.722349, mean_q: 0.880657, mean_eps: 0.622794\n",
            " 84510/100000: episode: 125, duration: 98.420s, episode steps: 429, steps per second:   4, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.007653, mae: 0.727515, mean_q: 0.886814, mean_eps: 0.620673\n",
            " 85382/100000: episode: 126, duration: 198.987s, episode steps: 872, steps per second:   4, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.007893, mae: 0.723503, mean_q: 0.882226, mean_eps: 0.617745\n",
            " 86180/100000: episode: 127, duration: 181.588s, episode steps: 798, steps per second:   4, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.007943, mae: 0.727482, mean_q: 0.887458, mean_eps: 0.613988\n",
            " 86669/100000: episode: 128, duration: 111.842s, episode steps: 489, steps per second:   4, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.007732, mae: 0.728767, mean_q: 0.889250, mean_eps: 0.611092\n",
            " 87773/100000: episode: 129, duration: 276.674s, episode steps: 1104, steps per second:   4, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.007534, mae: 0.724499, mean_q: 0.884180, mean_eps: 0.607508\n",
            " 88298/100000: episode: 130, duration: 131.323s, episode steps: 525, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.007849, mae: 0.724470, mean_q: 0.883613, mean_eps: 0.603843\n",
            " 88709/100000: episode: 131, duration: 103.388s, episode steps: 411, steps per second:   4, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.007327, mae: 0.729124, mean_q: 0.889535, mean_eps: 0.601737\n",
            " 89666/100000: episode: 132, duration: 220.784s, episode steps: 957, steps per second:   4, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.007395, mae: 0.724961, mean_q: 0.884873, mean_eps: 0.598659\n",
            " 90408/100000: episode: 133, duration: 168.179s, episode steps: 742, steps per second:   4, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008179, mae: 0.737477, mean_q: 0.898488, mean_eps: 0.594836\n",
            " 91263/100000: episode: 134, duration: 196.377s, episode steps: 855, steps per second:   4, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.009034, mae: 0.749617, mean_q: 0.914567, mean_eps: 0.591243\n",
            " 91938/100000: episode: 135, duration: 153.962s, episode steps: 675, steps per second:   4, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.008438, mae: 0.746996, mean_q: 0.909326, mean_eps: 0.587800\n",
            " 92467/100000: episode: 136, duration: 119.985s, episode steps: 529, steps per second:   4, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.008182, mae: 0.748307, mean_q: 0.912738, mean_eps: 0.585091\n",
            " 93164/100000: episode: 137, duration: 159.366s, episode steps: 697, steps per second:   4, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.008387, mae: 0.750935, mean_q: 0.914824, mean_eps: 0.582332\n",
            " 93685/100000: episode: 138, duration: 117.966s, episode steps: 521, steps per second:   4, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.008622, mae: 0.752607, mean_q: 0.916613, mean_eps: 0.579592\n",
            " 94331/100000: episode: 139, duration: 147.807s, episode steps: 646, steps per second:   4, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.008525, mae: 0.749853, mean_q: 0.912899, mean_eps: 0.576966\n",
            " 94880/100000: episode: 140, duration: 125.274s, episode steps: 549, steps per second:   4, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.007737, mae: 0.744465, mean_q: 0.908098, mean_eps: 0.574277\n",
            " 95682/100000: episode: 141, duration: 183.726s, episode steps: 802, steps per second:   4, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.007642, mae: 0.740028, mean_q: 0.901892, mean_eps: 0.571238\n",
            " 96596/100000: episode: 142, duration: 206.977s, episode steps: 914, steps per second:   4, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.007399, mae: 0.747788, mean_q: 0.912139, mean_eps: 0.567377\n",
            " 97542/100000: episode: 143, duration: 215.335s, episode steps: 946, steps per second:   4, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.007386, mae: 0.750390, mean_q: 0.915116, mean_eps: 0.563192\n",
            " 98235/100000: episode: 144, duration: 156.800s, episode steps: 693, steps per second:   4, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.007496, mae: 0.748873, mean_q: 0.912993, mean_eps: 0.559504\n",
            " 98844/100000: episode: 145, duration: 138.583s, episode steps: 609, steps per second:   4, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.007039, mae: 0.749196, mean_q: 0.913747, mean_eps: 0.556574\n",
            " 99898/100000: episode: 146, duration: 239.840s, episode steps: 1054, steps per second:   4, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.007919, mae: 0.753272, mean_q: 0.918388, mean_eps: 0.552833\n",
            "done, took 11775.277 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a92e3a2c750>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}